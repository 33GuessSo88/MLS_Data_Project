{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/33GuessSo88/MLS_Data_Project/blob/main/Pillar9_Date_Loading_and_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRzUYj7o_MQc"
      },
      "source": [
        "# TO DO\n",
        "\n",
        "1.   Commission needs some work, there are still obvious commission structres in the 'other' category that need to be dealt with.\n",
        "2.   Coordinate cleaning, unable to clean 6, why?\n",
        "3. RMS why does it only fix one data point, I thought there were 4. also stops at 97%, why. need to observe more examples of suspicious values.\n",
        "4. Style analysis needs work, way too much reporting.\n",
        "5. fix the restart thing when changing runtime.\n",
        "6. there are 4 rows with no close price. need to address this.\n",
        "7. why are there so many null commissions?\n",
        "8. does the style code replace original style text? we should create a new standardized style column\n",
        "\n",
        "9. review the parking code, still looks verbose. create standardized parking column?\n",
        "10. after the random forest runs for condo name it shows a bunch of prediction mistakes, but some of them aren't mistakes. The sequential ones? Should I make them all one? Maybe not. There was one condo that the only difference was a THE.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NF2zRQaddYh"
      },
      "source": [
        "\n",
        "# Data Loading and Combination Script"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# GOOGLE DRIVE MOUNTING\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell handles mounting your Google Drive to access files.\n",
        "Run this cell first and follow the authorization prompts.\n",
        "If it fails, restart the runtime and try again.\n",
        "\"\"\"\n",
        "\n",
        "def mount_google_drive():\n",
        "    \"\"\"Attempt to mount Google Drive and verify the connection\"\"\"\n",
        "    try:\n",
        "        # Import the drive module from google.colab\n",
        "        from google.colab import drive\n",
        "\n",
        "        # Attempt to mount the drive\n",
        "        print(\"⏳ Mounting Google Drive... (Check for the authorization prompt)\")\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        # Verify the mount was successful\n",
        "        from pathlib import Path\n",
        "        if Path('/content/drive').exists():\n",
        "            print(\"\\n✓ Google Drive mounted successfully!\")\n",
        "            print(\"📁 Your files are accessible at '/content/drive/My Drive/'\")\n",
        "        else:\n",
        "            print(\"\\n⚠️ Mount point exists but may not be working properly\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n❌ Error mounting Google Drive:\")\n",
        "        print(f\"Error details: {str(e)}\")\n",
        "        print(\"\\nTroubleshooting steps:\")\n",
        "        print(\"1. Click 'Runtime' -> 'Restart runtime'\")\n",
        "        print(\"2. Run this cell again\")\n",
        "        print(\"3. Complete the authorization process when prompted\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Execute the mounting function\n",
        "print(\"🔄 Starting Google Drive mount process...\")\n",
        "mount_google_drive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo7I28DdrCM8",
        "outputId": "a91a5818-9a93-41ba-d48b-c8d0ffad440d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Starting Google Drive mount process...\n",
            "⏳ Mounting Google Drive... (Check for the authorization prompt)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "✓ Google Drive mounted successfully!\n",
            "📁 Your files are accessible at '/content/drive/My Drive/'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lHY4w8Bh-f6M",
        "outputId": "80fc8f85-67af-43fe-f180-e4483f53850f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Installing required packages...\n",
            "Requirement already satisfied: tqdm==4.65.0 in /usr/local/lib/python3.11/dist-packages (4.65.0)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: matplotlib==3.7.1 in /usr/local/lib/python3.11/dist-packages (3.7.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.1) (1.17.0)\n",
            "✓ Packages installed successfully\n",
            "\n",
            "📚 Importing libraries...\n",
            "  Loading core Python libraries...\n",
            "  Loading data manipulation libraries...\n",
            "  Loading machine learning libraries...\n",
            "  Loading visualization libraries...\n",
            "  Loading system utilities...\n",
            "✓ All libraries imported successfully\n",
            "\n",
            "📝 Setting up logging...\n",
            "=== Starting New Notebook Run ===\n",
            "✓ Logger initialized successfully\n",
            "\n",
            "🎨 Configuring display settings...\n",
            "\n",
            "💾 Setting up memory monitoring...\n",
            "\n",
            "🔍 Verifying setup...\n",
            "\n",
            "🚀 Executing setup functions...\n",
            "Step 1 completed: Display settings configuration\n",
            "✓ Display settings configured successfully\n",
            "\n",
            "Library versions:\n",
            "pandas version: 2.2.2\n",
            "numpy version: 1.26.4\n",
            "scikit-learn version: 1.2.2\n",
            "matplotlib version: 3.7.1\n",
            "seaborn version: 0.13.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initial memory usage:\n",
            "--------------------\n",
            "Current memory usage: 250.53 MB\n",
            "\n",
            "Detailed Memory Info:\n",
            "--------------------\n",
            "Virtual Memory: 6370.38 MB\n",
            "RAM Usage: 1.5%\n",
            "Step 2 completed: Setup verification\n",
            "✓ Setup verified successfully\n",
            "\n",
            "================================================================================\n",
            "NOTEBOOK RUN REPORT - 2025-02-05 14:32\n",
            "Duration: 0:00:00.307740\n",
            "================================================================================\n",
            "\n",
            "INFO: === Starting New Notebook Run ===\n",
            "INFO: Step 1 completed: Display settings configuration\n",
            "INFO: \n",
            "Library versions:\n",
            "INFO: pandas version: 2.2.2\n",
            "INFO: numpy version: 1.26.4\n",
            "INFO: scikit-learn version: 1.2.2\n",
            "INFO: matplotlib version: 3.7.1\n",
            "INFO: seaborn version: 0.13.2\n",
            "INFO: \n",
            "Initial memory usage:\n",
            "INFO: --------------------\n",
            "INFO: Current memory usage: 250.53 MB\n",
            "INFO: \n",
            "Detailed Memory Info:\n",
            "INFO: --------------------\n",
            "INFO: Virtual Memory: 6370.38 MB\n",
            "INFO: RAM Usage: 1.5%\n",
            "INFO: Step 2 completed: Setup verification\n",
            "\n",
            "================================================================================\n",
            "Log file location: /content/notebook_run_20250205_143235.log\n",
            "================================================================================\n",
            "\n",
            "✨ All setup steps completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# SETUP AND IMPORTS\n",
        "# =================================================================================\n",
        "\n",
        "\"\"\"\n",
        "This notebook cell handles the initial setup for our data analysis:\n",
        "1. Installs required packages with specific versions\n",
        "2. Imports all necessary libraries\n",
        "3. Sets up logging functionality\n",
        "4. Configures display settings\n",
        "5. Implements memory monitoring\n",
        "6. Verifies the setup\n",
        "\"\"\"\n",
        "\n",
        "# SECTION 1: Package Installation\n",
        "# =================================================================================\n",
        "print(\"📦 Installing required packages...\")\n",
        "try:\n",
        "    !pip install tqdm==4.65.0 tabulate==0.9.0 scikit-learn==1.2.2 matplotlib==3.7.1 lightgbm seaborn\n",
        "    print(\"✓ Packages installed successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error installing packages: {e}\")\n",
        "\n",
        "# SECTION 2: Library Imports\n",
        "# =================================================================================\n",
        "print(\"\\n📚 Importing libraries...\")\n",
        "try:\n",
        "    # Core Python libraries for basic operations\n",
        "    print(\"  Loading core Python libraries...\")\n",
        "    import logging\n",
        "    from datetime import datetime, timedelta\n",
        "    from pathlib import Path\n",
        "    import os\n",
        "    import gc\n",
        "    import re\n",
        "    import time\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Data manipulation libraries\n",
        "    print(\"  Loading data manipulation libraries...\")\n",
        "    import pandas as pd  # For DataFrame operations\n",
        "    import numpy as np   # For numerical operations\n",
        "\n",
        "    # Machine learning libraries\n",
        "    print(\"  Loading machine learning libraries...\")\n",
        "    import sklearn  # Main scikit-learn module (needed for version checking)\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import classification_report\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "    # Visualization libraries\n",
        "    print(\"  Loading visualization libraries...\")\n",
        "    import matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    # System monitoring and notebook utilities\n",
        "    print(\"  Loading system utilities...\")\n",
        "    import psutil\n",
        "    import threading\n",
        "    from IPython.display import clear_output, display\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "    print(\"✓ All libraries imported successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error importing libraries: {e}\")\n",
        "    print(f\"  Error details: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# SECTION 3: Logging Setup\n",
        "# =================================================================================\n",
        "print(\"\\n📝 Setting up logging...\")\n",
        "\n",
        "class NotebookLogger:\n",
        "    \"\"\"\n",
        "    A custom logger class for tracking notebook execution and errors.\n",
        "    Provides both file-based and in-memory logging capabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create timestamp for unique log identification\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        self.log_file = f'notebook_run_{self.timestamp}.log'\n",
        "        self.start_time = datetime.now()\n",
        "        self.step_counter = 1\n",
        "        self.message_history = []\n",
        "\n",
        "        # Configure logging settings\n",
        "        logging.basicConfig(\n",
        "            filename=self.log_file,\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "\n",
        "        self.log_info(\"=== Starting New Notebook Run ===\")\n",
        "\n",
        "    def log_info(self, message):\n",
        "        \"\"\"Log informational messages\"\"\"\n",
        "        print(message)\n",
        "        logging.info(message)\n",
        "        self.message_history.append(f\"INFO: {message}\")\n",
        "\n",
        "    def log_error(self, message):\n",
        "        \"\"\"Log error messages\"\"\"\n",
        "        error_msg = f\"ERROR: {message}\"\n",
        "        print(error_msg)\n",
        "        logging.error(message)\n",
        "        self.message_history.append(error_msg)\n",
        "\n",
        "    def log_step_complete(self, step_name):\n",
        "        \"\"\"Track completion of notebook steps\"\"\"\n",
        "        message = f\"Step {self.step_counter} completed: {step_name}\"\n",
        "        self.log_info(message)\n",
        "        self.step_counter += 1\n",
        "\n",
        "    def display_log(self):\n",
        "        \"\"\"Display complete log contents\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"NOTEBOOK RUN REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
        "        print(f\"Duration: {datetime.now() - self.start_time}\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        try:\n",
        "            if os.path.exists(self.log_file):\n",
        "                with open(self.log_file, 'r') as f:\n",
        "                    print(f.read())\n",
        "            else:\n",
        "                print(\"\\n\".join(self.message_history))\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(f\"Log file location: {os.path.abspath(self.log_file)}\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"\\nFalling back to memory-stored messages due to file access error:\")\n",
        "            print(\"\\n\".join(self.message_history))\n",
        "            print(f\"\\nNote: Could not access log file due to: {str(e)}\")\n",
        "\n",
        "# Initialize our logger\n",
        "logger = NotebookLogger()\n",
        "print(\"✓ Logger initialized successfully\")\n",
        "\n",
        "# SECTION 4: Display Settings\n",
        "# =================================================================================\n",
        "print(\"\\n🎨 Configuring display settings...\")\n",
        "\n",
        "def configure_display_settings():\n",
        "    \"\"\"Configure pandas and visualization settings for better readability\"\"\"\n",
        "    try:\n",
        "        # Make pandas output more readable\n",
        "        pd.set_option('display.max_columns', None)  # Show all columns\n",
        "        pd.set_option('display.max_rows', 100)      # Show up to 100 rows\n",
        "        pd.set_option('display.width', None)        # Auto-adjust display width\n",
        "        pd.set_option('display.float_format', lambda x: '%.3f' % x)  # Format decimals\n",
        "\n",
        "        # Set up visualization defaults\n",
        "        sns.set_theme()  # Use seaborn's default theme\n",
        "        plt.rcParams['figure.figsize'] = [10, 6]  # Set default figure size\n",
        "        plt.rcParams['font.size'] = 12            # Set default font size\n",
        "\n",
        "        logger.log_step_complete(\"Display settings configuration\")\n",
        "        print(\"✓ Display settings configured successfully\")\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error configuring display settings: {e}\")\n",
        "        raise\n",
        "\n",
        "# SECTION 5: Memory Monitoring\n",
        "# =================================================================================\n",
        "print(\"\\n💾 Setting up memory monitoring...\")\n",
        "\n",
        "def check_memory_usage(show_details=False, warning_threshold_mb=1000):\n",
        "    \"\"\"\n",
        "    Monitor notebook memory usage and provide warnings if threshold is exceeded\n",
        "\n",
        "    Args:\n",
        "        show_details (bool): Display detailed memory statistics\n",
        "        warning_threshold_mb (float): Memory threshold for warnings in MB\n",
        "\n",
        "    Returns:\n",
        "        float: Current memory usage in MB\n",
        "    \"\"\"\n",
        "    try:\n",
        "        gc.collect()  # Clean up unused memory\n",
        "        process = psutil.Process()\n",
        "        memory_usage = process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "        logger.log_info(f\"Current memory usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "        if memory_usage > warning_threshold_mb:\n",
        "            logger.log_error(f\"WARNING: Memory usage exceeds {warning_threshold_mb} MB\")\n",
        "\n",
        "        if show_details:\n",
        "            logger.log_info(\"\\nDetailed Memory Info:\")\n",
        "            logger.log_info(\"-\" * 20)\n",
        "            logger.log_info(f\"Virtual Memory: {process.memory_info().vms / 1024 / 1024:.2f} MB\")\n",
        "            if hasattr(process.memory_info(), 'peak_wset'):\n",
        "                logger.log_info(f\"Peak Memory: {process.memory_info().peak_wset / 1024 / 1024:.2f} MB\")\n",
        "            logger.log_info(f\"RAM Usage: {psutil.virtual_memory().percent}%\")\n",
        "\n",
        "        return memory_usage\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error checking memory usage: {e}\")\n",
        "        return None\n",
        "\n",
        "# SECTION 6: Setup Verification\n",
        "# =================================================================================\n",
        "print(\"\\n🔍 Verifying setup...\")\n",
        "\n",
        "def verify_setup():\n",
        "    \"\"\"Verify library versions and test visualization capabilities\"\"\"\n",
        "    try:\n",
        "        # Check library versions\n",
        "        logger.log_info(\"\\nLibrary versions:\")\n",
        "        logger.log_info(f\"pandas version: {pd.__version__}\")\n",
        "        logger.log_info(f\"numpy version: {np.__version__}\")\n",
        "        logger.log_info(f\"scikit-learn version: {sklearn.__version__}\")  # Now this will work\n",
        "        logger.log_info(f\"matplotlib version: {matplotlib.__version__}\")\n",
        "        logger.log_info(f\"seaborn version: {sns.__version__}\")\n",
        "\n",
        "        # Create test visualization\n",
        "        plt.figure()\n",
        "        sns.lineplot(x=[1, 2, 3], y=[1, 2, 3])\n",
        "        plt.title(\"Test Plot\")\n",
        "        plt.show()\n",
        "\n",
        "        # Check memory usage\n",
        "        logger.log_info(\"\\nInitial memory usage:\")\n",
        "        logger.log_info(\"-\" * 20)\n",
        "        check_memory_usage(show_details=True)\n",
        "\n",
        "        logger.log_step_complete(\"Setup verification\")\n",
        "        print(\"✓ Setup verified successfully\")\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error during setup verification: {e}\")\n",
        "        raise\n",
        "\n",
        "# Execute setup functions\n",
        "# =================================================================================\n",
        "print(\"\\n🚀 Executing setup functions...\")\n",
        "configure_display_settings()\n",
        "verify_setup()\n",
        "logger.display_log()\n",
        "\n",
        "print(\"\\n✨ All setup steps completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96cc0Xov5GHo"
      },
      "source": [
        "# MLS CSV File Validation and Merging System\n",
        "\n",
        "## Overview\n",
        "This code block builds a robust data loading system that combines multiple CSV files while ensuring data consistency. It's specifically designed to handle monthly MLS data updates and catch potential column name changes or loading issues before they cause problems downstream.\n",
        "\n",
        "## Technical Details\n",
        "### Input\n",
        "* Source: MLS CSV files from Pillar9 system\n",
        "* Location: Google Drive folder containing multiple CSV export files\n",
        "* Path: '/content/drive/My Drive/Realtor/Data Project/Pillar9_RawCSV_Files'\n",
        "* Current Scale: ~400,000 rows × 28 columns\n",
        "\n",
        "### Output\n",
        "* Consolidated DataFrame: `combined_data`\n",
        "* Metadata Columns Added:\n",
        "  * `source_file`: Tracks which CSV file each record came from\n",
        "  * `data_load_date`: Records when the data was loaded\n",
        "  * `data_load_timestamp`: Precise timestamp for tracking\n",
        "\n",
        "### Key Operations\n",
        "1. Column Validation System\n",
        "   * Creates reference column list from first file\n",
        "   * Compares all subsequent files against reference\n",
        "   * Identifies and reports:\n",
        "     * Missing columns\n",
        "     * Extra columns\n",
        "     * Column name mismatches\n",
        "\n",
        "2. File Processing\n",
        "   * Discovers all CSV files in specified directory\n",
        "   * Loads each file with error handling\n",
        "   * Tracks successful and failed loads\n",
        "   * Combines validated files into single DataFrame\n",
        "\n",
        "3. Comprehensive Error Handling\n",
        "   * Validates folder existence\n",
        "   * Tracks failed file loads with detailed error messages\n",
        "   * Prevents merging if column validation fails\n",
        "   * Provides clear error messages for troubleshooting\n",
        "\n",
        "### Quality Checks\n",
        "The code automatically verifies and reports:\n",
        "* Number of files found and processed\n",
        "* Success/failure count for file loading\n",
        "* Detailed column validation results\n",
        "* File-by-file row and column counts\n",
        "* Final dataset dimensions\n",
        "* Memory usage throughout processing\n",
        "\n",
        "## Data Pipeline Note\n",
        "This enhanced loading system acts as a guardian for our data pipeline, ensuring that column inconsistencies are caught early. This is crucial for maintaining data integrity, especially with monthly MLS data updates.\n",
        "\n",
        "## Console Output Guide\n",
        "Watch for these key messages:\n",
        "* Initial file count discovery\n",
        "* Column validation results\n",
        "* Any column mismatches (crucial for MLS updates)\n",
        "* Processing summary with success/failure counts\n",
        "* Final dataset statistics\n",
        "\n",
        "## Error Resolution Guide\n",
        "If column mismatch errors occur:\n",
        "1. Check the validation output for specific mismatched columns\n",
        "2. Compare with previous month's data\n",
        "3. Review recent MLS system changes\n",
        "4. Update column mapping if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bbde107e9b9642e690f56ecac4f21700",
            "28ebf00879fd4666898ea2ebafa6aebf",
            "fe185bfe809e4e14bd8e62b5e3dfc3a3",
            "d0d9d971fc184a8ca40d7b671a6134d5",
            "d61fca44a2f546a98d531d9a73999207",
            "bf271a0012754a88b614d2030dafb14d",
            "187c45d946884361a5c794373bc9cefb",
            "b63c1a7695c74bc4bac227ffbaa667d2",
            "4c1cb0ab25384d898f02bfca3ac47017",
            "14d9429878fa456e90e10ff8b525187e",
            "87135e7e865d429b89b17f445bf6d6fe"
          ]
        },
        "id": "iBilOq-9jeO6",
        "outputId": "4bac00d4-c148-4175-c8a5-9bc3c844151b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data merge process...\n",
            "Source folder: /content/drive/My Drive/Realtor/Data Project/Pillar9_RawCSV_Files\n",
            "Found 83 CSV files\n",
            "\n",
            "Initial memory state before processing:\n",
            "Current memory usage: 244.98 MB\n",
            "\n",
            "Detailed Memory Info:\n",
            "--------------------\n",
            "Virtual Memory: 6364.67 MB\n",
            "RAM Usage: 1.4%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing CSV files:   0%|          | 0/83 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bbde107e9b9642e690f56ecac4f21700"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded: PBI - Export1.csv\n",
            "  Rows: 4,791\n",
            "  Columns: 43\n",
            "  Memory usage: 1.57 MB\n",
            "\n",
            "Loaded: PBI - Export1(1).csv\n",
            "  Rows: 4,633\n",
            "  Columns: 43\n",
            "  Memory usage: 1.52 MB\n",
            "\n",
            "Loaded: PBI - Export1(2).csv\n",
            "  Rows: 4,818\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(3).csv\n",
            "  Rows: 4,724\n",
            "  Columns: 43\n",
            "  Memory usage: 1.55 MB\n",
            "\n",
            "Loaded: PBI - Export1(5).csv\n",
            "  Rows: 4,723\n",
            "  Columns: 43\n",
            "  Memory usage: 1.55 MB\n",
            "\n",
            "Loaded: PBI - Export1(6).csv\n",
            "  Rows: 4,623\n",
            "  Columns: 43\n",
            "  Memory usage: 1.52 MB\n",
            "\n",
            "Loaded: PBI - Export1(7).csv\n",
            "  Rows: 4,484\n",
            "  Columns: 43\n",
            "  Memory usage: 1.47 MB\n",
            "\n",
            "Loaded: PBI - Export1(8).csv\n",
            "  Rows: 4,736\n",
            "  Columns: 43\n",
            "  Memory usage: 1.55 MB\n",
            "\n",
            "Loaded: PBI - Export1(9).csv\n",
            "  Rows: 4,963\n",
            "  Columns: 43\n",
            "  Memory usage: 1.63 MB\n",
            "\n",
            "Loaded: PBI - Export1(10).csv\n",
            "  Rows: 4,798\n",
            "  Columns: 43\n",
            "  Memory usage: 1.57 MB\n",
            "\n",
            "Loaded: PBI - Export1(11).csv\n",
            "  Rows: 4,951\n",
            "  Columns: 43\n",
            "  Memory usage: 1.62 MB\n",
            "\n",
            "Loaded: PBI - Export1(12).csv\n",
            "  Rows: 4,993\n",
            "  Columns: 43\n",
            "  Memory usage: 1.64 MB\n",
            "\n",
            "Loaded: PBI - Export1(13).csv\n",
            "  Rows: 4,781\n",
            "  Columns: 43\n",
            "  Memory usage: 1.57 MB\n",
            "\n",
            "Loaded: PBI - Export1(14).csv\n",
            "  Rows: 4,990\n",
            "  Columns: 43\n",
            "  Memory usage: 1.64 MB\n",
            "\n",
            "Loaded: PBI - Export1(15).csv\n",
            "  Rows: 4,877\n",
            "  Columns: 43\n",
            "  Memory usage: 1.60 MB\n",
            "\n",
            "Loaded: PBI - Export1(16).csv\n",
            "  Rows: 4,816\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(17).csv\n",
            "  Rows: 4,760\n",
            "  Columns: 43\n",
            "  Memory usage: 1.56 MB\n",
            "\n",
            "Loaded: PBI - Export1(18).csv\n",
            "  Rows: 4,992\n",
            "  Columns: 43\n",
            "  Memory usage: 1.64 MB\n",
            "\n",
            "Loaded: PBI - Export1(19).csv\n",
            "  Rows: 4,864\n",
            "  Columns: 43\n",
            "  Memory usage: 1.60 MB\n",
            "\n",
            "Loaded: PBI - Export1(20).csv\n",
            "  Rows: 4,809\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(21).csv\n",
            "  Rows: 4,840\n",
            "  Columns: 43\n",
            "  Memory usage: 1.59 MB\n",
            "\n",
            "Loaded: PBI - Export1(22).csv\n",
            "  Rows: 4,909\n",
            "  Columns: 43\n",
            "  Memory usage: 1.61 MB\n",
            "\n",
            "Loaded: PBI - Export1(23).csv\n",
            "  Rows: 4,975\n",
            "  Columns: 43\n",
            "  Memory usage: 1.63 MB\n",
            "\n",
            "Loaded: PBI - Export1(24).csv\n",
            "  Rows: 4,906\n",
            "  Columns: 43\n",
            "  Memory usage: 1.61 MB\n",
            "\n",
            "Loaded: PBI - Export1(25).csv\n",
            "  Rows: 4,819\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(26).csv\n",
            "  Rows: 4,965\n",
            "  Columns: 43\n",
            "  Memory usage: 1.63 MB\n",
            "\n",
            "Loaded: PBI - Export1(27).csv\n",
            "  Rows: 4,823\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(28).csv\n",
            "  Rows: 4,844\n",
            "  Columns: 43\n",
            "  Memory usage: 1.59 MB\n",
            "\n",
            "Loaded: PBI - Export1(29).csv\n",
            "  Rows: 4,611\n",
            "  Columns: 43\n",
            "  Memory usage: 1.51 MB\n",
            "\n",
            "Loaded: PBI - Export1(30).csv\n",
            "  Rows: 4,881\n",
            "  Columns: 43\n",
            "  Memory usage: 1.60 MB\n",
            "\n",
            "Loaded: PBI - Export1(31).csv\n",
            "  Rows: 4,837\n",
            "  Columns: 43\n",
            "  Memory usage: 1.59 MB\n",
            "\n",
            "Loaded: PBI - Export1(32).csv\n",
            "  Rows: 4,991\n",
            "  Columns: 43\n",
            "  Memory usage: 1.64 MB\n",
            "\n",
            "Loaded: PBI - Export1(33).csv\n",
            "  Rows: 4,801\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(34).csv\n",
            "  Rows: 4,867\n",
            "  Columns: 43\n",
            "  Memory usage: 1.60 MB\n",
            "\n",
            "Loaded: PBI - Export1(35).csv\n",
            "  Rows: 4,927\n",
            "  Columns: 43\n",
            "  Memory usage: 1.62 MB\n",
            "\n",
            "Loaded: PBI - Export1(43).csv\n",
            "  Rows: 4,889\n",
            "  Columns: 43\n",
            "  Memory usage: 1.60 MB\n",
            "\n",
            "Loaded: PBI - Export1(44).csv\n",
            "  Rows: 4,833\n",
            "  Columns: 43\n",
            "  Memory usage: 1.59 MB\n",
            "\n",
            "Loaded: PBI - Export1(45).csv\n",
            "  Rows: 4,845\n",
            "  Columns: 43\n",
            "  Memory usage: 1.59 MB\n",
            "\n",
            "Loaded: PBI - Export1(46).csv\n",
            "  Rows: 4,805\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(47).csv\n",
            "  Rows: 4,932\n",
            "  Columns: 43\n",
            "  Memory usage: 1.62 MB\n",
            "\n",
            "Loaded: PBI - Export1(48).csv\n",
            "  Rows: 4,876\n",
            "  Columns: 43\n",
            "  Memory usage: 1.60 MB\n",
            "\n",
            "Loaded: PBI - Export1(49).csv\n",
            "  Rows: 4,928\n",
            "  Columns: 43\n",
            "  Memory usage: 1.62 MB\n",
            "\n",
            "Loaded: PBI - Export1(50).csv\n",
            "  Rows: 4,823\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(51).csv\n",
            "  Rows: 4,849\n",
            "  Columns: 43\n",
            "  Memory usage: 1.59 MB\n",
            "\n",
            "Loaded: PBI - Export1(52).csv\n",
            "  Rows: 4,959\n",
            "  Columns: 43\n",
            "  Memory usage: 1.63 MB\n",
            "\n",
            "Loaded: PBI - Export1(53).csv\n",
            "  Rows: 4,840\n",
            "  Columns: 43\n",
            "  Memory usage: 1.59 MB\n",
            "\n",
            "Loaded: PBI - Export1(54).csv\n",
            "  Rows: 4,953\n",
            "  Columns: 43\n",
            "  Memory usage: 1.63 MB\n",
            "\n",
            "Loaded: PBI - Export1(55).csv\n",
            "  Rows: 4,907\n",
            "  Columns: 43\n",
            "  Memory usage: 1.61 MB\n",
            "\n",
            "Loaded: PBI - Export1(56).csv\n",
            "  Rows: 4,757\n",
            "  Columns: 43\n",
            "  Memory usage: 1.56 MB\n",
            "\n",
            "Loaded: PBI - Export1(57).csv\n",
            "  Rows: 4,950\n",
            "  Columns: 43\n",
            "  Memory usage: 1.62 MB\n",
            "\n",
            "Loaded: PBI - Export1(58).csv\n",
            "  Rows: 4,519\n",
            "  Columns: 43\n",
            "  Memory usage: 1.48 MB\n",
            "\n",
            "Loaded: PBI - Export1(59).csv\n",
            "  Rows: 4,829\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(60).csv\n",
            "  Rows: 4,895\n",
            "  Columns: 43\n",
            "  Memory usage: 1.61 MB\n",
            "\n",
            "Loaded: PBI - Export1(61).csv\n",
            "  Rows: 4,840\n",
            "  Columns: 43\n",
            "  Memory usage: 1.59 MB\n",
            "\n",
            "Loaded: PBI - Export1(62).csv\n",
            "  Rows: 4,873\n",
            "  Columns: 43\n",
            "  Memory usage: 1.60 MB\n",
            "\n",
            "Loaded: PBI - Export1(63).csv\n",
            "  Rows: 4,951\n",
            "  Columns: 43\n",
            "  Memory usage: 1.62 MB\n",
            "\n",
            "Loaded: PBI - Export1(64).csv\n",
            "  Rows: 4,818\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(65).csv\n",
            "  Rows: 4,810\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(66).csv\n",
            "  Rows: 4,858\n",
            "  Columns: 43\n",
            "  Memory usage: 1.59 MB\n",
            "\n",
            "Loaded: PBI - Export1(67).csv\n",
            "  Rows: 4,843\n",
            "  Columns: 43\n",
            "  Memory usage: 1.59 MB\n",
            "\n",
            "Loaded: PBI - Export1(68).csv\n",
            "  Rows: 4,878\n",
            "  Columns: 43\n",
            "  Memory usage: 1.60 MB\n",
            "\n",
            "Loaded: PBI - Export1(69).csv\n",
            "  Rows: 4,972\n",
            "  Columns: 43\n",
            "  Memory usage: 1.63 MB\n",
            "\n",
            "Loaded: PBI - Export1(70).csv\n",
            "  Rows: 4,934\n",
            "  Columns: 43\n",
            "  Memory usage: 1.62 MB\n",
            "\n",
            "Loaded: PBI - Export1(71).csv\n",
            "  Rows: 4,899\n",
            "  Columns: 43\n",
            "  Memory usage: 1.61 MB\n",
            "\n",
            "Loaded: PBI - Export1(72).csv\n",
            "  Rows: 4,819\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(73).csv\n",
            "  Rows: 4,934\n",
            "  Columns: 43\n",
            "  Memory usage: 1.62 MB\n",
            "\n",
            "Loaded: PBI - Export1(74).csv\n",
            "  Rows: 4,903\n",
            "  Columns: 43\n",
            "  Memory usage: 1.61 MB\n",
            "\n",
            "Loaded: PBI - Export1(75).csv\n",
            "  Rows: 4,904\n",
            "  Columns: 43\n",
            "  Memory usage: 1.61 MB\n",
            "\n",
            "Loaded: PBI - Export1(76).csv\n",
            "  Rows: 4,828\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(77).csv\n",
            "  Rows: 4,910\n",
            "  Columns: 43\n",
            "  Memory usage: 1.61 MB\n",
            "\n",
            "Loaded: PBI - Export1(78).csv\n",
            "  Rows: 4,953\n",
            "  Columns: 43\n",
            "  Memory usage: 1.63 MB\n",
            "\n",
            "Loaded: PBI - Export1(79).csv\n",
            "  Rows: 4,917\n",
            "  Columns: 43\n",
            "  Memory usage: 1.61 MB\n",
            "\n",
            "Loaded: PBI - Export1(80).csv\n",
            "  Rows: 3,772\n",
            "  Columns: 43\n",
            "  Memory usage: 1.24 MB\n",
            "\n",
            "Loaded: PBI - Export1(36).csv\n",
            "  Rows: 4,823\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(38).csv\n",
            "  Rows: 4,860\n",
            "  Columns: 43\n",
            "  Memory usage: 1.59 MB\n",
            "\n",
            "Loaded: PBI - Export1(40).csv\n",
            "  Rows: 4,865\n",
            "  Columns: 43\n",
            "  Memory usage: 1.60 MB\n",
            "\n",
            "Loaded: PBI - Export1(39).csv\n",
            "  Rows: 4,863\n",
            "  Columns: 43\n",
            "  Memory usage: 1.60 MB\n",
            "\n",
            "Loaded: PBI - Export1(4).csv\n",
            "  Rows: 4,774\n",
            "  Columns: 43\n",
            "  Memory usage: 1.57 MB\n",
            "\n",
            "Loaded: PBI - Export1(37).csv\n",
            "  Rows: 4,842\n",
            "  Columns: 43\n",
            "  Memory usage: 1.59 MB\n",
            "\n",
            "Loaded: PBI - Export1(41).csv\n",
            "  Rows: 4,813\n",
            "  Columns: 43\n",
            "  Memory usage: 1.58 MB\n",
            "\n",
            "Loaded: PBI - Export1(42).csv\n",
            "  Rows: 4,897\n",
            "  Columns: 43\n",
            "  Memory usage: 1.61 MB\n",
            "\n",
            "Loaded: PBI export oct 2024.csv\n",
            "  Rows: 4,866\n",
            "  Columns: 43\n",
            "  Memory usage: 1.60 MB\n",
            "\n",
            "Loaded: PBI export dec 2024.csv\n",
            "  Rows: 4,177\n",
            "  Columns: 43\n",
            "  Memory usage: 1.37 MB\n",
            "\n",
            "==================================================\n",
            "Processing Summary:\n",
            "==================================================\n",
            "Total files found: 83\n",
            "Successfully loaded: 83\n",
            "Failed to load: 0\n",
            "Total rows processed: 400,907\n",
            "\n",
            "Column Validation Report:\n",
            "--------------------------------------------------\n",
            "Number of dataframes to validate: 83\n",
            "Number of reference columns: 43\n",
            "\n",
            "Reference columns:\n",
            "  - # Garage Sp\n",
            "  - BG Fin Area\n",
            "  - Basement\n",
            "  - Bedrms Above Grade\n",
            "  - Bedrooms Below Grade\n",
            "  - CDOM\n",
            "  - City\n",
            "  - Close Date\n",
            "  - Close Price\n",
            "  - Commission\n",
            "  - Condo Fee\n",
            "  - Condo Name\n",
            "  - Condo Type\n",
            "  - DOM\n",
            "  - Full Baths\n",
            "  - Garage YN\n",
            "  - LINC #\n",
            "  - Latitude\n",
            "  - Listing Contract Date\n",
            "  - Longitude\n",
            "  - Lot Size SF\n",
            "  - MLS® #\n",
            "  - Occupant Type\n",
            "  - Original List Price\n",
            "  - Parking \n",
            "  - Postal Code\n",
            "  - Previous List Price\n",
            "  - Previous Status\n",
            "  - Property Sub Type\n",
            "  - Purchase Contract Date\n",
            "  - RMS Total\n",
            "  - Street Dir Suffix\n",
            "  - Structure Type\n",
            "  - Style\n",
            "  - Subdivision Name\n",
            "  - Suite\n",
            "  - Tax Assessed Value\n",
            "  - Total Baths\n",
            "  - Year Built\n",
            "  - Zoning\n",
            "  - data_load_date\n",
            "  - data_load_timestamp\n",
            "  - source_file\n",
            "\n",
            "Validation Summary:\n",
            "------------------------------\n",
            "✓ All files have matching columns!\n",
            "\n",
            "Merging validated dataframes...\n",
            "\n",
            "Final Dataset Summary:\n",
            "------------------------------\n",
            "Total rows: 400,907\n",
            "Total columns: 43\n",
            "Memory usage: 131.52 MB\n",
            "Step 3 completed: CSV file merging\n",
            "\n",
            "Data loading completed successfully! 🎉\n",
            "\n",
            "Column Overview:\n",
            "------------------------------\n",
            "  1. # Garage Sp\n",
            "  2. BG Fin Area\n",
            "  3. Basement\n",
            "  4. Bedrms Above Grade\n",
            "  5. Bedrooms Below Grade\n",
            "  6. CDOM\n",
            "  7. City\n",
            "  8. Close Date\n",
            "  9. Close Price\n",
            " 10. Commission\n",
            " 11. Condo Fee\n",
            " 12. Condo Name\n",
            " 13. Condo Type\n",
            " 14. DOM\n",
            " 15. Full Baths\n",
            " 16. Garage YN\n",
            " 17. LINC #\n",
            " 18. Latitude\n",
            " 19. Listing Contract Date\n",
            " 20. Longitude\n",
            " 21. Lot Size SF\n",
            " 22. MLS® #\n",
            " 23. Occupant Type\n",
            " 24. Original List Price\n",
            " 25. Parking \n",
            " 26. Postal Code\n",
            " 27. Previous List Price\n",
            " 28. Previous Status\n",
            " 29. Property Sub Type\n",
            " 30. Purchase Contract Date\n",
            " 31. RMS Total\n",
            " 32. Street Dir Suffix\n",
            " 33. Structure Type\n",
            " 34. Style\n",
            " 35. Subdivision Name\n",
            " 36. Suite\n",
            " 37. Tax Assessed Value\n",
            " 38. Total Baths\n",
            " 39. Year Built\n",
            " 40. Zoning\n",
            " 41. data_load_date\n",
            " 42. data_load_timestamp\n",
            " 43. source_file\n",
            "\n",
            "Final Memory Usage:\n",
            "------------------------------\n",
            "Current memory usage: 650.21 MB\n",
            "\n",
            "Detailed Memory Info:\n",
            "--------------------\n",
            "Virtual Memory: 6779.84 MB\n",
            "RAM Usage: 1.6%\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# CSV FILE MERGING AND VALIDATION\n",
        "# =================================================================================\n",
        "\n",
        "def validate_columns(dataframes, verbose=True):\n",
        "    \"\"\"\n",
        "    Check if all dataframes have the same columns.\n",
        "\n",
        "    Args:\n",
        "        dataframes (list): List of pandas dataframes to validate\n",
        "        verbose (bool): Whether to print detailed information\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all dataframes have matching columns, False otherwise\n",
        "    \"\"\"\n",
        "    # First, check if we received any dataframes\n",
        "    if not dataframes:\n",
        "        logger.log_error(\"No dataframes provided for validation!\")\n",
        "        return False\n",
        "\n",
        "    # Get columns from first dataframe as reference\n",
        "    reference_columns = set(dataframes[0].columns)\n",
        "\n",
        "    # Add more detailed initial reporting\n",
        "    if verbose:\n",
        "        logger.log_info(\"\\nColumn Validation Report:\")\n",
        "        logger.log_info(\"-\" * 50)\n",
        "        logger.log_info(f\"Number of dataframes to validate: {len(dataframes)}\")\n",
        "        logger.log_info(f\"Number of reference columns: {len(reference_columns)}\")\n",
        "        logger.log_info(\"\\nReference columns:\")\n",
        "        for col in sorted(reference_columns):\n",
        "            logger.log_info(f\"  - {col}\")\n",
        "\n",
        "    # Check each dataframe against the reference\n",
        "    all_valid = True  # Track if all dataframes are valid\n",
        "    for i, df in enumerate(dataframes[1:], start=1):\n",
        "        current_columns = set(df.columns)\n",
        "\n",
        "        # Check for mismatches\n",
        "        missing_cols = reference_columns - current_columns\n",
        "        extra_cols = current_columns - reference_columns\n",
        "\n",
        "        if missing_cols or extra_cols:\n",
        "            all_valid = False\n",
        "            if verbose:\n",
        "                logger.log_error(f\"\\nMismatch found in dataframe {i}:\")\n",
        "                logger.log_error(\"-\" * 30)\n",
        "                if missing_cols:\n",
        "                    logger.log_error(\"Missing columns:\")\n",
        "                    for col in sorted(missing_cols):\n",
        "                        logger.log_error(f\"  - {col}\")\n",
        "                if extra_cols:\n",
        "                    logger.log_error(\"Extra columns:\")\n",
        "                    for col in sorted(extra_cols):\n",
        "                        logger.log_error(f\"  - {col}\")\n",
        "\n",
        "    if verbose:\n",
        "        logger.log_info(\"\\nValidation Summary:\")\n",
        "        logger.log_info(\"-\" * 30)\n",
        "        if all_valid:\n",
        "            logger.log_info(\"✓ All files have matching columns!\")\n",
        "        else:\n",
        "            logger.log_error(\"✗ Column mismatches detected!\")\n",
        "\n",
        "    return all_valid\n",
        "\n",
        "def merge_real_estate_files(folder_path):\n",
        "    \"\"\"\n",
        "    Merge all CSV files from a specified folder into a single DataFrame.\n",
        "    Includes validation for column consistency across files.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str or Path): Path to the folder containing CSV files\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Combined dataset from all CSV files\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If no files are found or if column validation fails\n",
        "    \"\"\"\n",
        "    # Convert to Path object and verify folder exists\n",
        "    folder_path = Path(folder_path)\n",
        "    if not folder_path.exists():\n",
        "        logger.log_error(f\"Folder not found: {folder_path}\")\n",
        "        raise ValueError(f\"Folder not found: {folder_path}\")\n",
        "\n",
        "    # Get list of CSV files\n",
        "    csv_files = list(folder_path.glob('*.csv'))\n",
        "    if not csv_files:\n",
        "        logger.log_error(f\"No CSV files found in {folder_path}\")\n",
        "        raise ValueError(f\"No CSV files found in {folder_path}\")\n",
        "\n",
        "    logger.log_info(f\"Found {len(csv_files)} CSV files\")\n",
        "\n",
        "    # Track processing statistics\n",
        "    all_dataframes = []\n",
        "    failed_files = []\n",
        "    total_rows = 0\n",
        "\n",
        "    # Log memory usage before processing\n",
        "    logger.log_info(\"\\nInitial memory state before processing:\")\n",
        "    check_memory_usage(show_details=True)\n",
        "\n",
        "    # Read each CSV file with detailed tracking\n",
        "    for file_path in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
        "        try:\n",
        "            # Read the CSV with error handling for common issues\n",
        "            df = pd.read_csv(\n",
        "                file_path,\n",
        "                encoding='utf-8',  # Explicit encoding\n",
        "                low_memory=False,  # Prevent mixed type inference warnings\n",
        "                on_bad_lines='warn'  # Don't fail on problematic lines\n",
        "            )\n",
        "\n",
        "            # Enhanced validation\n",
        "            if len(df.columns) == 0:\n",
        "                raise ValueError(\"File contains no columns\")\n",
        "            if len(df) == 0:\n",
        "                raise ValueError(\"File contains no data rows\")\n",
        "\n",
        "            # Add metadata columns\n",
        "            df['source_file'] = file_path.name\n",
        "            df['data_load_date'] = datetime.now().date()\n",
        "            df['data_load_timestamp'] = datetime.now()\n",
        "\n",
        "            # Update statistics\n",
        "            total_rows += len(df)\n",
        "            all_dataframes.append(df)\n",
        "\n",
        "            # Log file processing success\n",
        "            logger.log_info(f\"\\nLoaded: {file_path.name}\")\n",
        "            logger.log_info(f\"  Rows: {len(df):,}\")\n",
        "            logger.log_info(f\"  Columns: {len(df.columns)}\")\n",
        "            logger.log_info(f\"  Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_files.append((file_path.name, str(e)))\n",
        "            logger.log_error(f\"\\nError reading {file_path.name}:\")\n",
        "            logger.log_error(f\"Error details: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Enhanced processing summary\n",
        "    logger.log_info(\"\\n\" + \"=\"*50)\n",
        "    logger.log_info(\"Processing Summary:\")\n",
        "    logger.log_info(\"=\"*50)\n",
        "    logger.log_info(f\"Total files found: {len(csv_files):,}\")\n",
        "    logger.log_info(f\"Successfully loaded: {len(all_dataframes):,}\")\n",
        "    logger.log_info(f\"Failed to load: {len(failed_files):,}\")\n",
        "    logger.log_info(f\"Total rows processed: {total_rows:,}\")\n",
        "\n",
        "    # Show failed files with more detail\n",
        "    if failed_files:\n",
        "        logger.log_error(\"\\nFailed Files Report:\")\n",
        "        logger.log_error(\"-\" * 30)\n",
        "        for i, (file_name, error) in enumerate(failed_files, 1):\n",
        "            logger.log_error(f\"{i}. {file_name}\")\n",
        "            logger.log_error(f\"   Error: {error}\")\n",
        "\n",
        "    # Validate column consistency\n",
        "    if not validate_columns(all_dataframes):\n",
        "        error_msg = (\"Column mismatch detected between files. \"\n",
        "                    \"Please review the validation output above and ensure all files have matching columns.\")\n",
        "        logger.log_error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    # Combine dataframes if we have any\n",
        "    if all_dataframes:\n",
        "        logger.log_info(\"\\nMerging validated dataframes...\")\n",
        "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "        # Memory cleanup\n",
        "        del all_dataframes\n",
        "        gc.collect()\n",
        "\n",
        "        # Final dataset summary\n",
        "        logger.log_info(\"\\nFinal Dataset Summary:\")\n",
        "        logger.log_info(\"-\" * 30)\n",
        "        logger.log_info(f\"Total rows: {len(combined_df):,}\")\n",
        "        logger.log_info(f\"Total columns: {len(combined_df.columns)}\")\n",
        "        logger.log_info(f\"Memory usage: {combined_df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "        # Log step completion\n",
        "        logger.log_step_complete(\"CSV file merging\")\n",
        "\n",
        "        return combined_df\n",
        "    else:\n",
        "        logger.log_error(\"No CSV files were successfully processed\")\n",
        "        raise ValueError(\"No CSV files were successfully processed\")\n",
        "\n",
        "# Main execution block\n",
        "try:\n",
        "    # Define folder path and ensure it exists\n",
        "    folder_path = Path('/content/drive/My Drive/Realtor/Data Project/Pillar9_RawCSV_Files')\n",
        "\n",
        "    logger.log_info(\"Starting data merge process...\")\n",
        "    logger.log_info(f\"Source folder: {folder_path}\")\n",
        "\n",
        "    # Process and merge files\n",
        "    combined_data = merge_real_estate_files(folder_path)\n",
        "\n",
        "    # Success message and column overview\n",
        "    logger.log_info(\"\\nData loading completed successfully! 🎉\")\n",
        "    logger.log_info(\"\\nColumn Overview:\")\n",
        "    logger.log_info(\"-\" * 30)\n",
        "    for i, col in enumerate(sorted(combined_data.columns), 1):\n",
        "        logger.log_info(f\"{i:3d}. {col}\")\n",
        "\n",
        "    # Save memory usage information\n",
        "    logger.log_info(\"\\nFinal Memory Usage:\")\n",
        "    logger.log_info(\"-\" * 30)\n",
        "    check_memory_usage(show_details=True)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(\"\\nError during execution:\")\n",
        "    logger.log_error(\"-\" * 30)\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve6Cvozwmv4M"
      },
      "source": [
        "# Data Cleaning: Column Name Standardization\n",
        "\n",
        "## Overview\n",
        "This code block implements a systematic approach to clean and standardize DataFrame column names. It ensures consistent naming conventions across our dataset, making it easier to work with and reference columns in subsequent analysis.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input\n",
        "* DataFrame: `combined_data` from previous merging step\n",
        "* Column Names: Mix of raw MLS column names with various formats\n",
        "* Special Characters: Handles ®, #, spaces, and other special characters\n",
        "\n",
        "### Output\n",
        "* DataFrame with standardized column names\n",
        "* Comparison report showing original vs. cleaned names\n",
        "* Detailed log of changes made\n",
        "\n",
        "### Cleaning Rules\n",
        "1. Specific Name Mappings\n",
        "   * MLS® # → MLS_Num\n",
        "   * LINC # → LINC_Num\n",
        "   * DOM → Days_On_Market\n",
        "   * CDOM → Cumulative_Days_On_Market\n",
        "   * Other domain-specific transformations\n",
        "\n",
        "2. General Cleaning Rules\n",
        "   * Remove leading/trailing spaces\n",
        "   * Replace spaces with underscores\n",
        "   * Replace # with Num\n",
        "   * Remove ® symbol\n",
        "   * Replace & with 'and'\n",
        "   * Replace slashes with underscores\n",
        "   * Remove parentheses\n",
        "   * Replace periods with underscores\n",
        "   * Replace hyphens with underscores\n",
        "\n",
        "### Quality Checks\n",
        "The code verifies and reports:\n",
        "* Number of columns processed\n",
        "* Which columns were modified\n",
        "* Total number of changes made\n",
        "* Sample of data with new column names\n",
        "\n",
        "## Process Monitoring\n",
        "Watch for these console outputs:\n",
        "* Initial column count\n",
        "* List of modified column names\n",
        "* Confirmation of successful cleaning\n",
        "* Any error messages if issues occur\n",
        "\n",
        "## Error Handling\n",
        "The code includes comprehensive error checking:\n",
        "* Creates safe copy of DataFrame\n",
        "* Tracks all column name changes\n",
        "* Provides detailed error messages if problems occur\n",
        "* Maintains original error handling behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds1JQ_7IoCZ4",
        "outputId": "712a2632-4de9-46bd-f32c-ef85baf4a9bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting column name cleaning process...\n",
            "Starting column name cleaning process...\n",
            "Created copy of DataFrame with 43 columns\n",
            "Applied specific column renaming rules\n",
            "Applied general column cleaning rules\n",
            "\n",
            "Column Name Changes:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Modified Columns:\n",
            "              Original                       New\n",
            "                MLS® #                   MLS_Num\n",
            "                LINC #                  LINC_Num\n",
            " Listing Contract Date     Listing_Contract_Date\n",
            "Purchase Contract Date    Purchase_Contract_Date\n",
            "            Close Date                Close_Date\n",
            "       Previous Status           Previous_Status\n",
            "                  CDOM Cumulative_Days_On_Market\n",
            "                   DOM            Days_On_Market\n",
            "   Original List Price       Original_List_Price\n",
            "   Previous List Price       Previous_List_Price\n",
            "           Close Price               Close_Price\n",
            "             RMS Total                 RMS_Total\n",
            "     Street Dir Suffix         Street_Dir_Suffix\n",
            "        Structure Type            Structure_Type\n",
            "    Tax Assessed Value        Tax_Assessed_Value\n",
            "      Subdivision Name          Subdivision_Name\n",
            "     Property Sub Type         Property_Sub_Type\n",
            "            Year Built                Year_Built\n",
            "    Bedrms Above Grade      Bedrooms_Above_Grade\n",
            "  Bedrooms Below Grade      Bedrooms_Below_Grade\n",
            "           Total Baths               Total_Baths\n",
            "            Full Baths                Full_Baths\n",
            "           BG Fin Area Below_Grade_Finished_Area\n",
            "             Condo Fee                 Condo_Fee\n",
            "            Condo Type                Condo_Type\n",
            "           Lot Size SF               Lot_Size_SF\n",
            "           Postal Code               Postal_Code\n",
            "             Garage YN                 Garage_YN\n",
            "           # Garage Sp         Num_Garage_Spaces\n",
            "              Parking                    Parking\n",
            "            Condo Name                Condo_Name\n",
            "         Occupant Type             Occupant_Type\n",
            "\n",
            "Total columns modified: 32\n",
            "Total columns: 43\n",
            "Column cleaning completed successfully!\n",
            "\n",
            "First few rows with cleaned column names:\n",
            "    MLS_Num    LINC_Num     City Listing_Contract_Date Purchase_Contract_Date  Close_Date Previous_Status Cumulative_Days_On_Market Days_On_Market Original_List_Price Previous_List_Price  Close_Price RMS_Total Street_Dir_Suffix Structure_Type Tax_Assessed_Value                                          Commission Suite                           Basement     Style Subdivision_Name Property_Sub_Type  Year_Built  Bedrooms_Above_Grade  Bedrooms_Below_Grade  Total_Baths  Full_Baths Below_Grade_Finished_Area Condo_Fee    Condo_Type Lot_Size_SF Postal_Code Garage_YN Num_Garage_Spaces                     Parking Condo_Name  Latitude  Longitude Zoning Occupant_Type        source_file data_load_date        data_load_timestamp\n",
            "0  E4400087  0029570265  Calgary            08/01/2024                    NaN  08/07/2024             NaN                         6              6                 NaN                 NaN  $838,000.00     2,252               NaN            NaN                NaN          3.5% First 100K and 1.5% Remainder of Sale   NaN  Finished, Full, Walk-Out To Grade  2 Storey   Panorama Hills          Detached    2003.000                 3.000                   NaN        4.000       2.000                       NaN       NaN           NaN         NaN     T3K 5R6       Yes               NaN      Double Garage Attached        NaN    51.162   -114.078    NaN           NaN  PBI - Export1.csv     2025-02-05 2025-02-05 14:32:38.481520\n",
            "1  E4396127  0020250577  Calgary            07/05/2024                    NaN  08/28/2024             NaN                        54             54                 NaN                 NaN  $625,000.00       949               NaN            NaN                NaN    3.5% on first $100,000 and 1.5% on the remaining   NaN                     Finished, Full  Bi-Level      Thorncliffe          Detached    1956.000                 2.000                   NaN        2.000       2.000                       NaN       NaN           NaN       5,995     T2K 3C3       Yes               NaN  220 Volt Wiring, Oversized        NaN    51.103   -114.069    NaN           NaN  PBI - Export1.csv     2025-02-05 2025-02-05 14:32:38.481520\n",
            "2  E4393149  0039738109  Calgary            06/17/2024                    NaN  09/04/2024             NaN                        79             79                 NaN                 NaN  $475,000.00     1,627               NaN            NaN                NaN  3.5% on the first $100,000 & 1.5% on balance of SP   NaN                                NaN  3 Storey              NaN     Row/Townhouse    2024.000                 3.000                   NaN        3.000       2.000                       NaN       NaN  Conventional         NaN     T2X 5L1       Yes               NaN      Double Garage Attached        NaN    50.859   -114.064    NaN           NaN  PBI - Export1.csv     2025-02-05 2025-02-05 14:32:38.481520\n",
            "Step 4 completed: Column name cleaning\n"
          ]
        }
      ],
      "source": [
        "def clean_column_names(df, verbose=True):\n",
        "    \"\"\"\n",
        "    Clean and standardize DataFrame column names for consistency and readability.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): Input DataFrame with columns to be cleaned\n",
        "        verbose (bool): Whether to print the comparison table (default: True)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with cleaned column names\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting column name cleaning process...\")\n",
        "\n",
        "        # Create a copy to avoid modifying the original DataFrame\n",
        "        df = df.copy()\n",
        "        logger.log_info(f\"Created copy of DataFrame with {len(df.columns)} columns\")\n",
        "\n",
        "        # Store original columns for comparison\n",
        "        original_cols = df.columns.tolist()\n",
        "\n",
        "        # Define specific column name mappings\n",
        "        # Dictionary maps original column names to their cleaned versions\n",
        "        rename_dict = {\n",
        "            'MLS® #': 'MLS_Num',\n",
        "            'LINC #': 'LINC_Num',\n",
        "            'DOM': 'Days_On_Market',\n",
        "            'CDOM': 'Cumulative_Days_On_Market',\n",
        "            'Bedrms Above Grade': 'Bedrooms_Above_Grade',\n",
        "            'BG Fin Area': 'Below_Grade_Finished_Area',\n",
        "            '# Garage Sp': 'Num_Garage_Spaces'\n",
        "        }\n",
        "\n",
        "        # STEP 1: Apply specific renaming rules\n",
        "        df.columns = df.columns.map(lambda x: rename_dict.get(x, x))\n",
        "        logger.log_info(\"Applied specific column renaming rules\")\n",
        "\n",
        "        # STEP 2: Apply general cleaning rules to all columns\n",
        "        df.columns = df.columns.map(lambda x: x\n",
        "            .strip()                    # Remove leading/trailing spaces\n",
        "            .replace(' ', '_')          # Replace spaces with underscores\n",
        "            .replace('#', 'Num')        # Replace # with Num\n",
        "            .replace('®', '')           # Remove registered trademark symbol\n",
        "            .replace('&', 'and')        # Replace & with 'and'\n",
        "            .replace('/', '_')          # Replace slashes with underscore\n",
        "            .replace('(', '')           # Remove parentheses\n",
        "            .replace(')', '')           # Remove parentheses\n",
        "            .replace('.', '_')          # Replace periods with underscore\n",
        "            .replace('-', '_')          # Replace hyphens with underscore\n",
        "        )\n",
        "        logger.log_info(\"Applied general column cleaning rules\")\n",
        "\n",
        "        # STEP 3: Create comparison table if verbose is True\n",
        "        if verbose:\n",
        "            # Create comparison DataFrame\n",
        "            comparison = pd.DataFrame({\n",
        "                'Original': original_cols,\n",
        "                'New': df.columns.tolist()\n",
        "            })\n",
        "\n",
        "            # Add a \"Changed\" column to highlight modifications\n",
        "            comparison['Changed'] = comparison['Original'] != comparison['New']\n",
        "\n",
        "            # Display settings for better readability\n",
        "            pd.set_option('display.max_rows', None)  # Show all rows\n",
        "            pd.set_option('display.max_colwidth', None)  # Show full column content\n",
        "\n",
        "            logger.log_info(\"\\nColumn Name Changes:\")\n",
        "            logger.log_info(\"-\" * 80)\n",
        "\n",
        "            # Display only changed columns if there are any\n",
        "            changed_cols = comparison[comparison['Changed']]\n",
        "            if not changed_cols.empty:\n",
        "                logger.log_info(\"\\nModified Columns:\")\n",
        "                # Convert the comparison to a string format for logging\n",
        "                comparison_str = changed_cols[['Original', 'New']].to_string(index=False)\n",
        "                for line in comparison_str.split('\\n'):\n",
        "                    logger.log_info(line)\n",
        "                logger.log_info(f\"\\nTotal columns modified: {len(changed_cols)}\")\n",
        "            else:\n",
        "                logger.log_info(\"No columns were modified.\")\n",
        "\n",
        "            logger.log_info(f\"Total columns: {len(df.columns)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in clean_column_names:\")\n",
        "        logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "        logger.log_error(f\"Details: {str(e)}\")\n",
        "        raise  # Re-raise the exception after logging\n",
        "\n",
        "# Execute the column cleaning\n",
        "try:\n",
        "    logger.log_info(\"Starting column name cleaning process...\")\n",
        "    combined_data = clean_column_names(combined_data)\n",
        "    logger.log_info(\"Column cleaning completed successfully!\")\n",
        "\n",
        "    # Display a sample of the cleaned DataFrame\n",
        "    logger.log_info(\"\\nFirst few rows with cleaned column names:\")\n",
        "    # Convert the head output to a string format for logging\n",
        "    sample_data = combined_data.head(3).to_string()\n",
        "    for line in sample_data.split('\\n'):\n",
        "        logger.log_info(line)\n",
        "\n",
        "    # Mark step completion\n",
        "    logger.log_step_complete(\"Column name cleaning\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during column cleaning:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")\n",
        "    raise  # Re-raise the exception to maintain original error handling behavior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXM7zXjbxopw"
      },
      "source": [
        "# Data Analysis: Temporal Coverage and Quality Assessment\n",
        "\n",
        "## Overview\n",
        "This code block examines the quality and completeness of dates in our real estate dataset. It identifies gaps in our temporal data, analyzes listing patterns over time, and provides detailed statistics about our date coverage. This analysis is crucial for understanding any potential data collection issues or seasonal patterns in our MLS listings.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input\n",
        "* DataFrame: Cleaned dataset with transaction dates\n",
        "* Primary Date Column: 'Close_Date'\n",
        "* Date Format: Handles multiple input formats automatically\n",
        "\n",
        "### Key Analysis Components\n",
        "1. Date Validation and Cleaning\n",
        "   * Checks for invalid date formats\n",
        "   * Converts strings to datetime objects\n",
        "   * Reports problematic date entries\n",
        "   * Creates backup of original dates\n",
        "\n",
        "2. Coverage Analysis\n",
        "   * Calculates total date range\n",
        "   * Identifies missing dates\n",
        "   * Groups consecutive missing dates into ranges\n",
        "   * Analyzes patterns in missing data\n",
        "\n",
        "3. Temporal Distribution Analysis\n",
        "   * Monthly listing counts\n",
        "   * Year-over-year comparisons\n",
        "   * Growth rate calculations\n",
        "   * Seasonal pattern identification\n",
        "\n",
        "### Quality Metrics Reported\n",
        "* Total date range covered\n",
        "* Number of unique dates\n",
        "* Average listings per date\n",
        "* Invalid date entries\n",
        "* Gaps in coverage\n",
        "* Longest periods without data\n",
        "\n",
        "## Process Output\n",
        "The analysis provides:\n",
        "* Date range summary\n",
        "* Missing date patterns\n",
        "* Monthly distribution tables\n",
        "* Year-over-year growth rates\n",
        "* Optional CSV export of missing dates\n",
        "* Comprehensive error logging\n",
        "\n",
        "## Error Protection\n",
        "* Creates date column backup\n",
        "* Validates date formats\n",
        "* Restores original data if errors occur\n",
        "* Provides detailed error messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz13XAo2o5p-",
        "outputId": "e32312cf-4cb4-4dae-b11f-00719bc0b155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting comprehensive date coverage analysis...\n",
            "Created backup of date column\n",
            "\n",
            "Starting Date Coverage Analysis...\n",
            "==================================================\n",
            "\n",
            "Processing date column: Close_Date\n",
            "\n",
            "Sample of raw dates before conversion:\n",
            "0    08/07/2024\n",
            "1    08/28/2024\n",
            "2    09/04/2024\n",
            "3    08/02/2024\n",
            "4    08/16/2024\n",
            "\n",
            "Date Range Summary:\n",
            "------------------------------\n",
            "Earliest date: 2010-01-01\n",
            "Latest date: 2025-01-17\n",
            "Total date range: 5,495 days (15.0 years)\n",
            "Number of unique dates: 5,496\n",
            "Average listings per date: 72.9\n",
            "\n",
            "Analyzing missing dates...\n",
            "\n",
            "✓ No missing dates found! The date coverage is complete.\n",
            "\n",
            "Temporal Distribution Analysis:\n",
            "------------------------------\n",
            "\n",
            "Monthly Listings by Year:\n",
            "Close_Date    1     2     3     4     5     6     7     8     9     10    11    12\n",
            "Close_Date                                                                        \n",
            "2010        1346  1871  2401  2340  2230  1796  1566  1532  1569  1390  1471  1252\n",
            "2011        1285  1908  2243  2059  2231  2374  2018  1889  1759  1625  1632  1263\n",
            "2012        1276  2043  2644  2608  2918  2697  2407  2046  1995  1989  1759  1303\n",
            "2013        1479  1998  2588  2896  3044  2882  2731  2704  2299  2385  2050  1432\n",
            "2014        1693  2201  3046  3204  3743  3249  2976  2722  2627  2624  2208  1317\n",
            "2015        1093  1536  2207  2503  2684  2794  2476  2162  1816  1770  1594  1075\n",
            "2016         980  1440  1920  2191  2478  2518  2196  1927  1925  2004  1503  1155\n",
            "2017        1120  1625  2353  2311  2592  2593  2061  2000  1824  1837  1704  1252\n",
            "2018        1232  1397  1678  1948  2152  2384  1929  1844  1643  1628  1396   987\n",
            "2019         992  1232  1643  2029  2352  2257  2057  2019  1705  1757  1437  1051\n",
            "2020        1094  1445  1542   750  1378  2215  2399  2026  2271  2195  1831  1510\n",
            "2021        1516  2400  3675  4038  3858  3769  2982  2691  2793  2671  2677  2113\n",
            "2022        2528  4174  5122  4417  3864  3686  2896  2630  2440  2281  2060  1457\n",
            "2023        1465  2169  2905  3372  4018  3925  3226  3394  3064  2565  2258  1666\n",
            "2024        2069  2522  3334  3730  3771  3555  3101  2748  2538  2624  2261  1584\n",
            "2025         848     0     0     0     0     0     0     0     0     0     0     0\n",
            "\n",
            "Yearly Totals:\n",
            "2010: 20,764 listings\n",
            "2011: 22,286 listings\n",
            "2012: 25,685 listings\n",
            "2013: 28,488 listings\n",
            "2014: 31,610 listings\n",
            "2015: 23,710 listings\n",
            "2016: 22,237 listings\n",
            "2017: 23,272 listings\n",
            "2018: 20,218 listings\n",
            "2019: 20,531 listings\n",
            "2020: 20,656 listings\n",
            "2021: 35,183 listings\n",
            "2022: 37,555 listings\n",
            "2023: 34,027 listings\n",
            "2024: 33,837 listings\n",
            "2025: 848 listings\n",
            "\n",
            "Year-over-Year Growth:\n",
            "2011: +7.3%\n",
            "2012: +15.3%\n",
            "2013: +10.9%\n",
            "2014: +11.0%\n",
            "2015: -25.0%\n",
            "2016: -6.2%\n",
            "2017: +4.7%\n",
            "2018: -13.1%\n",
            "2019: +1.5%\n",
            "2020: +0.6%\n",
            "2021: +70.3%\n",
            "2022: +6.7%\n",
            "2023: -9.4%\n",
            "2024: -0.6%\n",
            "2025: -97.5%\n",
            "\n",
            "Analysis Summary:\n",
            "==================================================\n",
            "Total Listings Analyzed: 400,907\n",
            "Date Range: 5,495 days\n",
            "Missing Dates: 0\n",
            "\n",
            "Analysis completed successfully! 🎉\n",
            "Step 5 completed: Date coverage analysis\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# DATE COVERAGE ANALYSIS\n",
        "# =================================================================================\n",
        "\n",
        "def analyze_date_coverage(df, date_column='Close_Date', save_missing_dates=False):\n",
        "    \"\"\"\n",
        "    Analyze date coverage in a DataFrame to identify gaps and patterns in temporal data.\n",
        "    This function helps identify missing dates and understand the temporal distribution\n",
        "    of real estate listings.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): Input DataFrame containing real estate listings\n",
        "        date_column (str): Name of the column containing dates (default: 'Close_Date')\n",
        "        save_missing_dates (bool): Whether to save missing dates to CSV (default: False)\n",
        "\n",
        "    Returns:\n",
        "        dict: Summary statistics about the date coverage\n",
        "    \"\"\"\n",
        "    logger.log_info(\"\\nStarting Date Coverage Analysis...\")\n",
        "    logger.log_info(\"=\" * 50)\n",
        "\n",
        "    # STEP 1: Prepare and validate date column\n",
        "    try:\n",
        "        # First, check if the column exists\n",
        "        if date_column not in df.columns:\n",
        "            error_msg = f\"Column '{date_column}' not found in DataFrame\"\n",
        "            logger.log_error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        logger.log_info(f\"\\nProcessing date column: {date_column}\")\n",
        "\n",
        "        # Show sample of raw dates before conversion\n",
        "        logger.log_info(\"\\nSample of raw dates before conversion:\")\n",
        "        sample_dates = df[date_column].head().to_string()\n",
        "        for line in sample_dates.split('\\n'):\n",
        "            logger.log_info(line)\n",
        "\n",
        "        # Convert date column to datetime, handling various date formats\n",
        "        df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
        "\n",
        "        # Check for and report any dates that couldn't be parsed\n",
        "        invalid_dates = df[date_column].isnull().sum()\n",
        "        if invalid_dates > 0:\n",
        "            logger.log_error(f\"\\n⚠️ Warning: Found {invalid_dates:,} invalid dates ({invalid_dates/len(df):.1%} of total)\")\n",
        "\n",
        "            # Show examples of problematic dates\n",
        "            logger.log_error(\"\\nSample of rows with invalid dates:\")\n",
        "            invalid_mask = df[date_column].isnull()\n",
        "            invalid_samples = df[invalid_mask].head().to_string()\n",
        "            for line in invalid_samples.split('\\n'):\n",
        "                logger.log_error(line)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error processing date column: {str(e)}\"\n",
        "        logger.log_error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    # STEP 2: Analyze date range and basic statistics\n",
        "    earliest_date = df[date_column].min()\n",
        "    latest_date = df[date_column].max()\n",
        "    date_range_days = (latest_date - earliest_date).days\n",
        "    total_dates = df[date_column].nunique()\n",
        "\n",
        "    logger.log_info(f\"\\nDate Range Summary:\")\n",
        "    logger.log_info(\"-\" * 30)\n",
        "    logger.log_info(f\"Earliest date: {earliest_date.strftime('%Y-%m-%d')}\")\n",
        "    logger.log_info(f\"Latest date: {latest_date.strftime('%Y-%m-%d')}\")\n",
        "    logger.log_info(f\"Total date range: {date_range_days:,} days ({date_range_days/365.25:.1f} years)\")\n",
        "    logger.log_info(f\"Number of unique dates: {total_dates:,}\")\n",
        "    logger.log_info(f\"Average listings per date: {len(df)/total_dates:.1f}\")\n",
        "\n",
        "    # STEP 3: Find missing dates\n",
        "    logger.log_info(\"\\nAnalyzing missing dates...\")\n",
        "    full_date_range = pd.date_range(start=earliest_date, end=latest_date, freq='D')\n",
        "    expected_dates = set(full_date_range.date)\n",
        "    actual_dates = set(df[date_column].dt.date.unique())\n",
        "    missing_dates = sorted(expected_dates - actual_dates)\n",
        "\n",
        "    # STEP 4: Report missing dates and identify patterns\n",
        "    if missing_dates:\n",
        "        logger.log_info(f\"\\nFound {len(missing_dates):,} missing dates\")\n",
        "\n",
        "        # Group consecutive missing dates into ranges\n",
        "        ranges = []\n",
        "        range_start = missing_dates[0]\n",
        "        prev_date = missing_dates[0]\n",
        "\n",
        "        for date in missing_dates[1:]:\n",
        "            if (date - prev_date).days > 1:\n",
        "                ranges.append((range_start, prev_date))\n",
        "                range_start = date\n",
        "            prev_date = date\n",
        "        ranges.append((range_start, prev_date))\n",
        "\n",
        "        # Analyze and report missing date patterns\n",
        "        logger.log_info(\"\\nMissing Date Patterns:\")\n",
        "        logger.log_info(\"-\" * 30)\n",
        "\n",
        "        # Show the longest gaps\n",
        "        ranges.sort(key=lambda x: (x[1] - x[0]).days, reverse=True)\n",
        "        logger.log_info(\"\\nLongest gaps (top 5):\")\n",
        "        for start, end in ranges[:5]:\n",
        "            days_missing = (end - start).days + 1\n",
        "            logger.log_info(f\"- {start} to {end} ({days_missing:,} days)\")\n",
        "\n",
        "        # Save missing dates if requested\n",
        "        if save_missing_dates:\n",
        "            try:\n",
        "                missing_dates_df = pd.DataFrame({\n",
        "                    'Missing_Date': missing_dates,\n",
        "                    'Day_of_Week': [d.strftime('%A') for d in missing_dates],\n",
        "                    'Month': [d.strftime('%B') for d in missing_dates]\n",
        "                })\n",
        "                save_path = 'missing_dates.csv'\n",
        "                missing_dates_df.to_csv(save_path, index=False)\n",
        "                logger.log_info(f\"\\n✓ Saved missing dates to: {save_path}\")\n",
        "            except Exception as e:\n",
        "                logger.log_error(f\"\\n⚠️ Error saving missing dates: {str(e)}\")\n",
        "    else:\n",
        "        logger.log_info(\"\\n✓ No missing dates found! The date coverage is complete.\")\n",
        "\n",
        "    # STEP 5: Temporal distribution analysis\n",
        "    logger.log_info(\"\\nTemporal Distribution Analysis:\")\n",
        "    logger.log_info(\"-\" * 30)\n",
        "\n",
        "    # Monthly distribution with year-over-year comparison\n",
        "    monthly_counts = df.groupby([\n",
        "        df[date_column].dt.year,\n",
        "        df[date_column].dt.month\n",
        "    ]).size().unstack()\n",
        "\n",
        "    logger.log_info(\"\\nMonthly Listings by Year:\")\n",
        "    monthly_str = monthly_counts.fillna(0).astype(int).to_string()\n",
        "    for line in monthly_str.split('\\n'):\n",
        "        logger.log_info(line)\n",
        "\n",
        "    # Yearly totals\n",
        "    yearly_counts = df[date_column].dt.year.value_counts().sort_index()\n",
        "    logger.log_info(\"\\nYearly Totals:\")\n",
        "    for year, count in yearly_counts.items():\n",
        "        logger.log_info(f\"{year}: {count:,} listings\")\n",
        "\n",
        "    # Calculate year-over-year growth\n",
        "    logger.log_info(\"\\nYear-over-Year Growth:\")\n",
        "    for i in range(1, len(yearly_counts)):\n",
        "        current_year = yearly_counts.index[i]\n",
        "        previous_year = yearly_counts.index[i-1]\n",
        "        growth = (yearly_counts[current_year] - yearly_counts[previous_year]) / yearly_counts[previous_year] * 100\n",
        "        logger.log_info(f\"{current_year}: {growth:+.1f}%\")\n",
        "\n",
        "    # Return summary statistics\n",
        "    return {\n",
        "        'earliest_date': earliest_date,\n",
        "        'latest_date': latest_date,\n",
        "        'total_days': date_range_days,\n",
        "        'missing_dates': len(missing_dates),\n",
        "        'total_listings': len(df)\n",
        "    }\n",
        "\n",
        "# Main execution block\n",
        "try:\n",
        "    logger.log_info(\"Starting comprehensive date coverage analysis...\")\n",
        "\n",
        "    # Create a backup of the date column before analysis\n",
        "    backup_dates = combined_data['Close_Date'].copy()\n",
        "    logger.log_info(\"Created backup of date column\")\n",
        "\n",
        "    # Run the analysis\n",
        "    results = analyze_date_coverage(combined_data, save_missing_dates=True)\n",
        "\n",
        "    # Print final summary\n",
        "    logger.log_info(\"\\nAnalysis Summary:\")\n",
        "    logger.log_info(\"=\" * 50)\n",
        "    logger.log_info(f\"Total Listings Analyzed: {results['total_listings']:,}\")\n",
        "    logger.log_info(f\"Date Range: {results['total_days']:,} days\")\n",
        "    logger.log_info(f\"Missing Dates: {results['missing_dates']:,}\")\n",
        "\n",
        "    logger.log_info(\"\\nAnalysis completed successfully! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"Date coverage analysis\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\n❌ Error during analysis:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")\n",
        "\n",
        "    # Restore the original dates if there was an error\n",
        "    combined_data['Close_Date'] = backup_dates\n",
        "    logger.log_info(\"\\nRestored original dates due to error.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k-2M16ZXhBin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79cb2d17-5252-42bd-b88b-88273d60e174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['MLS_Num', 'LINC_Num', 'City', 'Listing_Contract_Date', 'Purchase_Contract_Date', 'Close_Date', 'Previous_Status', 'Cumulative_Days_On_Market', 'Days_On_Market', 'Original_List_Price', 'Previous_List_Price', 'Close_Price', 'RMS_Total', 'Street_Dir_Suffix', 'Structure_Type', 'Tax_Assessed_Value', 'Commission', 'Suite', 'Basement', 'Style', 'Subdivision_Name', 'Property_Sub_Type', 'Year_Built', 'Bedrooms_Above_Grade', 'Bedrooms_Below_Grade', 'Total_Baths', 'Full_Baths', 'Below_Grade_Finished_Area', 'Condo_Fee', 'Condo_Type', 'Lot_Size_SF', 'Postal_Code', 'Garage_YN', 'Num_Garage_Spaces', 'Parking', 'Condo_Name', 'Latitude', 'Longitude', 'Zoning', 'Occupant_Type', 'source_file', 'data_load_date', 'data_load_timestamp']\n"
          ]
        }
      ],
      "source": [
        "# prompt: show me a list of column names\n",
        "\n",
        "print(combined_data.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwWk5CmYduVw"
      },
      "source": [
        "# Data Cleaning: Duplicate Record Analysis and Removal\n",
        "\n",
        "## Overview\n",
        "This code block implements a systematic approach to identify, analyze, and remove duplicate listings from our MLS dataset. It provides detailed insights about duplicate patterns and ensures data integrity by removing redundant entries while preserving the most relevant records.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input\n",
        "* DataFrame: Current state of combined_data\n",
        "* Key Identifier: MLS_Num (unique listing identifier)\n",
        "* Date Column: Close_Date (for temporal analysis)\n",
        "* Source Tracking: source_file (original file tracking)\n",
        "\n",
        "### Key Operations\n",
        "1. Initial Analysis\n",
        "   * Count total rows vs unique identifiers\n",
        "   * Calculate duplication rate\n",
        "   * Verify column existence\n",
        "\n",
        "2. Detailed Duplicate Investigation\n",
        "   * Identify all duplicated records\n",
        "   * Analyze frequency of duplications\n",
        "   * Track source files containing duplicates\n",
        "   * Check for price discrepancies in duplicates\n",
        "\n",
        "3. Data Cleaning\n",
        "   * Remove duplicate entries\n",
        "   * Keep first occurrence of each MLS number\n",
        "   * Verify successful duplicate removal\n",
        "   * Return cleaned dataset with statistics\n",
        "\n",
        "### Quality Checks\n",
        "The code verifies and reports:\n",
        "* Total number of records analyzed\n",
        "* Number of unique MLS numbers\n",
        "* Duplication rate percentage\n",
        "* Source files containing duplicates\n",
        "* Price discrepancies between duplicates\n",
        "* Verification of complete duplicate removal\n",
        "\n",
        "## Process Monitoring\n",
        "Key metrics tracked:\n",
        "* Initial row count\n",
        "* Number of unique identifiers\n",
        "* Duplicates removed\n",
        "* Remaining record count\n",
        "* Source file distribution\n",
        "* Data discrepancies\n",
        "\n",
        "## Error Protection\n",
        "* Creates safe copy of DataFrame\n",
        "* Validates input data types\n",
        "* Checks column existence\n",
        "* Provides detailed error logging\n",
        "* Returns both cleaned data and summary statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znE5Vr0NdsuN",
        "outputId": "ad1daf1d-72dc-4ba8-d4db-7bc1a27d273e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting duplicate analysis and removal process...\n",
            "Created copy of input DataFrame for safe processing\n",
            "\n",
            "Initial Duplicate Analysis:\n",
            "--------------------------------------------------\n",
            "Total rows in dataset: 400,907\n",
            "Unique MLS_Nums: 393,664\n",
            "Number of duplicates: 7,243\n",
            "Duplication rate: 1.81%\n",
            "\n",
            "Duplicate Records Analysis:\n",
            "--------------------------------------------------\n",
            "\n",
            "Example of duplicated records (first 5 cases):\n",
            "         MLS_Num Close_Date            source_file  Close_Price\n",
            "133316  A1002970 2020-09-03  PBI - Export1(28).csv  $276,250.00\n",
            "138359  A1002970 2020-09-03  PBI - Export1(29).csv  $276,250.00\n",
            "124199  A1004059 2021-02-08  PBI - Export1(26).csv  $145,000.00\n",
            "129114  A1004059 2021-02-08  PBI - Export1(27).csv  $145,000.00\n",
            "132106  A1007755 2020-09-03  PBI - Export1(28).csv  $415,000.00\n",
            "\n",
            "Frequency of duplications:\n",
            "Maximum occurrences of any MLS_Num: 2\n",
            "\n",
            "Top 5 most duplicated records:\n",
            "MLS #A2115676: 2 occurrences\n",
            "MLS #A2164970: 2 occurrences\n",
            "MLS #A2144012: 2 occurrences\n",
            "MLS #A2163422: 2 occurrences\n",
            "MLS #A2147110: 2 occurrences\n",
            "\n",
            "Duplicates by source file:\n",
            "PBI - Export1(2).csv: 458 duplicates\n",
            "PBI - Export1(3).csv: 381 duplicates\n",
            "PBI - Export1(1).csv: 364 duplicates\n",
            "PBI - Export1(9).csv: 361 duplicates\n",
            "PBI - Export1(10).csv: 345 duplicates\n",
            "PBI - Export1(16).csv: 322 duplicates\n",
            "PBI - Export1(18).csv: 282 duplicates\n",
            "PBI - Export1(7).csv: 276 duplicates\n",
            "PBI - Export1(24).csv: 271 duplicates\n",
            "PBI - Export1(58).csv: 268 duplicates\n",
            "PBI - Export1(17).csv: 265 duplicates\n",
            "PBI - Export1(6).csv: 265 duplicates\n",
            "PBI - Export1.csv: 264 duplicates\n",
            "PBI - Export1(57).csv: 262 duplicates\n",
            "PBI - Export1(23).csv: 262 duplicates\n",
            "PBI - Export1(19).csv: 254 duplicates\n",
            "PBI - Export1(15).csv: 252 duplicates\n",
            "PBI - Export1(21).csv: 246 duplicates\n",
            "PBI - Export1(8).csv: 245 duplicates\n",
            "PBI - Export1(20).csv: 227 duplicates\n",
            "PBI - Export1(11).csv: 225 duplicates\n",
            "PBI - Export1(22).csv: 222 duplicates\n",
            "PBI - Export1(63).csv: 217 duplicates\n",
            "PBI - Export1(4).csv: 210 duplicates\n",
            "PBI - Export1(65).csv: 203 duplicates\n",
            "PBI - Export1(59).csv: 203 duplicates\n",
            "PBI - Export1(51).csv: 203 duplicates\n",
            "PBI - Export1(64).csv: 203 duplicates\n",
            "PBI export oct 2024.csv: 194 duplicates\n",
            "PBI - Export1(25).csv: 193 duplicates\n",
            "PBI - Export1(52).csv: 186 duplicates\n",
            "PBI - Export1(12).csv: 184 duplicates\n",
            "PBI - Export1(13).csv: 183 duplicates\n",
            "PBI - Export1(34).csv: 179 duplicates\n",
            "PBI - Export1(5).csv: 178 duplicates\n",
            "PBI - Export1(14).csv: 174 duplicates\n",
            "PBI - Export1(79).csv: 169 duplicates\n",
            "PBI - Export1(78).csv: 160 duplicates\n",
            "PBI - Export1(60).csv: 158 duplicates\n",
            "PBI - Export1(70).csv: 154 duplicates\n",
            "PBI - Export1(26).csv: 151 duplicates\n",
            "PBI - Export1(62).csv: 149 duplicates\n",
            "PBI - Export1(38).csv: 148 duplicates\n",
            "PBI - Export1(37).csv: 148 duplicates\n",
            "PBI - Export1(66).csv: 146 duplicates\n",
            "PBI - Export1(35).csv: 146 duplicates\n",
            "PBI - Export1(33).csv: 145 duplicates\n",
            "PBI - Export1(53).csv: 143 duplicates\n",
            "PBI - Export1(47).csv: 132 duplicates\n",
            "PBI - Export1(54).csv: 129 duplicates\n",
            "PBI - Export1(50).csv: 128 duplicates\n",
            "PBI - Export1(56).csv: 127 duplicates\n",
            "PBI - Export1(29).csv: 127 duplicates\n",
            "PBI - Export1(32).csv: 123 duplicates\n",
            "PBI - Export1(73).csv: 122 duplicates\n",
            "PBI - Export1(46).csv: 121 duplicates\n",
            "PBI - Export1(44).csv: 121 duplicates\n",
            "PBI - Export1(28).csv: 119 duplicates\n",
            "PBI - Export1(69).csv: 119 duplicates\n",
            "PBI - Export1(71).csv: 118 duplicates\n",
            "PBI - Export1(39).csv: 118 duplicates\n",
            "PBI - Export1(48).csv: 117 duplicates\n",
            "PBI - Export1(36).csv: 116 duplicates\n",
            "PBI - Export1(27).csv: 114 duplicates\n",
            "PBI - Export1(74).csv: 113 duplicates\n",
            "PBI - Export1(31).csv: 110 duplicates\n",
            "PBI - Export1(68).csv: 109 duplicates\n",
            "PBI - Export1(45).csv: 108 duplicates\n",
            "PBI - Export1(43).csv: 106 duplicates\n",
            "PBI - Export1(30).csv: 104 duplicates\n",
            "PBI - Export1(55).csv: 103 duplicates\n",
            "PBI - Export1(77).csv: 103 duplicates\n",
            "PBI - Export1(61).csv: 103 duplicates\n",
            "PBI - Export1(67).csv: 92 duplicates\n",
            "PBI export dec 2024.csv: 91 duplicates\n",
            "PBI - Export1(41).csv: 86 duplicates\n",
            "PBI - Export1(42).csv: 85 duplicates\n",
            "PBI - Export1(76).csv: 74 duplicates\n",
            "PBI - Export1(40).csv: 67 duplicates\n",
            "PBI - Export1(72).csv: 66 duplicates\n",
            "PBI - Export1(80).csv: 62 duplicates\n",
            "PBI - Export1(49).csv: 60 duplicates\n",
            "PBI - Export1(75).csv: 49 duplicates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:\n",
            "⚠️ Warning: Found records with different prices for same MLS number!\n",
            "ERROR:root:Number of records with price discrepancies: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: \n",
            "⚠️ Warning: Found records with different prices for same MLS number!\n",
            "ERROR: Number of records with price discrepancies: 1\n",
            "\n",
            "Duplicate Removal Results:\n",
            "--------------------------------------------------\n",
            "Original row count: 400,907\n",
            "Rows after deduplication: 393,664\n",
            "Rows removed: 7,243\n",
            "\n",
            "Verification: 0 duplicates remain\n",
            "✓ Duplicate removal successful!\n",
            "\n",
            "Final Summary:\n",
            "--------------------------------------------------\n",
            "Initial rows: 400,907\n",
            "Unique IDs: 393,664\n",
            "Duplicates removed: 7,243\n",
            "Duplication rate: 1.81%\n",
            "\n",
            "Process completed successfully! 🎉\n",
            "Step 6 completed: Duplicate analysis and removal\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# DUPLICATE ANALYSIS AND REMOVAL\n",
        "# =================================================================================\n",
        "\n",
        "def analyze_duplicates(df, id_column='MLS_Num', date_column='Close_Date', verbose=True):\n",
        "    \"\"\"\n",
        "    Analyze and remove duplicates from real estate dataset.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): Input DataFrame\n",
        "        id_column (str): Column containing unique identifiers (default: 'MLS_Num')\n",
        "        date_column (str): Column containing date information (default: 'Close_Date')\n",
        "        verbose (bool): Whether to print detailed analysis (default: True)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with duplicates removed\n",
        "        dict: Summary statistics about the duplicate analysis\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Input validation\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            error_msg = \"Input must be a pandas DataFrame\"\n",
        "            logger.log_error(error_msg)\n",
        "            raise TypeError(error_msg)\n",
        "\n",
        "        for col in [id_column, date_column]:\n",
        "            if col not in df.columns:\n",
        "                error_msg = f\"Column '{col}' not found in DataFrame\"\n",
        "                logger.log_error(error_msg)\n",
        "                raise ValueError(error_msg)\n",
        "\n",
        "        # Create a copy to preserve original data\n",
        "        df = df.copy()\n",
        "        logger.log_info(\"Created copy of input DataFrame for safe processing\")\n",
        "\n",
        "        # STEP 1: Initial Analysis\n",
        "        total_rows = len(df)\n",
        "        unique_ids = df[id_column].nunique()\n",
        "        duplicate_count = total_rows - unique_ids\n",
        "\n",
        "        if verbose:\n",
        "            logger.log_info(\"\\nInitial Duplicate Analysis:\")\n",
        "            logger.log_info(\"-\" * 50)\n",
        "            logger.log_info(f\"Total rows in dataset: {total_rows:,}\")\n",
        "            logger.log_info(f\"Unique {id_column}s: {unique_ids:,}\")\n",
        "            logger.log_info(f\"Number of duplicates: {duplicate_count:,}\")\n",
        "\n",
        "            if duplicate_count > 0:\n",
        "                duplication_rate = (duplicate_count / total_rows) * 100\n",
        "                logger.log_info(f\"Duplication rate: {duplication_rate:.2f}%\")\n",
        "\n",
        "        # STEP 2: Detailed Duplicate Analysis\n",
        "        # Find all duplicated records (keeping all instances)\n",
        "        duplicated_records = df[df.duplicated(subset=[id_column], keep=False)]\n",
        "\n",
        "        if not duplicated_records.empty and verbose:\n",
        "            logger.log_info(\"\\nDuplicate Records Analysis:\")\n",
        "            logger.log_info(\"-\" * 50)\n",
        "\n",
        "            # Sort duplicates by ID and date for better analysis\n",
        "            sorted_duplicates = duplicated_records.sort_values([id_column, date_column])\n",
        "\n",
        "            # Show example duplicates with key information\n",
        "            display_columns = [id_column, date_column, 'source_file', 'Close_Price']\n",
        "            logger.log_info(\"\\nExample of duplicated records (first 5 cases):\")\n",
        "            example_duplicates = sorted_duplicates[display_columns].head().to_string()\n",
        "            for line in example_duplicates.split('\\n'):\n",
        "                logger.log_info(line)\n",
        "\n",
        "            # Analyze how many times each ID appears\n",
        "            duplicate_counts = duplicated_records[id_column].value_counts()\n",
        "            logger.log_info(\"\\nFrequency of duplications:\")\n",
        "            logger.log_info(f\"Maximum occurrences of any {id_column}: {duplicate_counts.max()}\")\n",
        "            logger.log_info(\"\\nTop 5 most duplicated records:\")\n",
        "            for id_val, count in duplicate_counts.head().items():\n",
        "                logger.log_info(f\"MLS #{id_val}: {count} occurrences\")\n",
        "\n",
        "            # Analyze which source files contain duplicates\n",
        "            logger.log_info(\"\\nDuplicates by source file:\")\n",
        "            source_counts = duplicated_records['source_file'].value_counts()\n",
        "            for source, count in source_counts.items():\n",
        "                logger.log_info(f\"{source}: {count:,} duplicates\")\n",
        "\n",
        "            # Check for potential data discrepancies in duplicates\n",
        "            if 'Close_Price' in df.columns:\n",
        "                price_discrepancies = duplicated_records.groupby(id_column)['Close_Price'].nunique() > 1\n",
        "                discrepant_prices = price_discrepancies[price_discrepancies].index\n",
        "                if len(discrepant_prices) > 0:\n",
        "                    logger.log_error(\"\\n⚠️ Warning: Found records with different prices for same MLS number!\")\n",
        "                    logger.log_error(f\"Number of records with price discrepancies: {len(discrepant_prices)}\")\n",
        "\n",
        "        # STEP 3: Remove Duplicates\n",
        "        # Keep the first occurrence of each MLS number\n",
        "        df_clean = df.drop_duplicates(subset=[id_column], keep='first')\n",
        "        rows_removed = len(df) - len(df_clean)\n",
        "\n",
        "        if verbose:\n",
        "            logger.log_info(\"\\nDuplicate Removal Results:\")\n",
        "            logger.log_info(\"-\" * 50)\n",
        "            logger.log_info(f\"Original row count: {len(df):,}\")\n",
        "            logger.log_info(f\"Rows after deduplication: {len(df_clean):,}\")\n",
        "            logger.log_info(f\"Rows removed: {rows_removed:,}\")\n",
        "\n",
        "            # Verify no duplicates remain\n",
        "            remaining_duplicates = df_clean[id_column].duplicated().sum()\n",
        "            logger.log_info(f\"\\nVerification: {remaining_duplicates} duplicates remain\")\n",
        "\n",
        "            if remaining_duplicates == 0:\n",
        "                logger.log_info(\"✓ Duplicate removal successful!\")\n",
        "            else:\n",
        "                logger.log_error(\"⚠️ Warning: Some duplicates remain!\")\n",
        "\n",
        "        # Return summary statistics along with cleaned DataFrame\n",
        "        summary_stats = {\n",
        "            'total_rows': total_rows,\n",
        "            'unique_ids': unique_ids,\n",
        "            'duplicates_removed': rows_removed,\n",
        "            'duplication_rate': (duplicate_count / total_rows) * 100 if total_rows > 0 else 0\n",
        "        }\n",
        "\n",
        "        return df_clean, summary_stats\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"\\nError in analyze_duplicates function:\"\n",
        "        logger.log_error(error_msg)\n",
        "        logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "        logger.log_error(f\"Details: {str(e)}\")\n",
        "        raise  # Re-raise the exception after logging details\n",
        "\n",
        "# Execute duplicate analysis and removal\n",
        "try:\n",
        "    logger.log_info(\"Starting duplicate analysis and removal process...\")\n",
        "\n",
        "    # Note: We're now capturing both the cleaned DataFrame and summary statistics\n",
        "    combined_data, duplicate_stats = analyze_duplicates(combined_data)\n",
        "\n",
        "    logger.log_info(\"\\nFinal Summary:\")\n",
        "    logger.log_info(\"-\" * 50)\n",
        "    logger.log_info(f\"Initial rows: {duplicate_stats['total_rows']:,}\")\n",
        "    logger.log_info(f\"Unique IDs: {duplicate_stats['unique_ids']:,}\")\n",
        "    logger.log_info(f\"Duplicates removed: {duplicate_stats['duplicates_removed']:,}\")\n",
        "    logger.log_info(f\"Duplication rate: {duplicate_stats['duplication_rate']:.2f}%\")\n",
        "\n",
        "    logger.log_info(\"\\nProcess completed successfully! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"Duplicate analysis and removal\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during duplicate analysis:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Quality: Null Value Analysis and Reporting\n",
        "\n",
        "## Overview\n",
        "This code block performs a comprehensive analysis of missing (null) values across all columns in our dataset. It identifies patterns in missing data, categorizes columns by their null percentages, and provides detailed reporting to help understand where data quality can be improved.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input\n",
        "* DataFrame: Current state of combined_data\n",
        "* Parameters:\n",
        "  * verbose: Controls detail level of reporting\n",
        "  * high_null_threshold: Percentage to flag concerning null levels (default: 50%)\n",
        "\n",
        "### Analysis Components\n",
        "1. Basic Statistics\n",
        "   * Total null count per column\n",
        "   * Percentage of nulls per column\n",
        "   * Total rows and columns analyzed\n",
        "\n",
        "2. Categorization of Nulls\n",
        "   * Severe (>75% nulls)\n",
        "   * High (50-75% nulls)\n",
        "   * Moderate (25-50% nulls)\n",
        "   * Low (<25% nulls)\n",
        "\n",
        "3. Detailed Reporting\n",
        "   * Lists columns exceeding threshold\n",
        "   * Sorts columns by null frequency\n",
        "   * Saves analysis to timestamped CSV\n",
        "\n",
        "### Output Analysis\n",
        "The code reports:\n",
        "* Overall dataset dimensions\n",
        "* Column-by-column null statistics\n",
        "* Categorized summary of null patterns\n",
        "* Detailed lists of problematic columns\n",
        "* CSV export of complete analysis\n",
        "\n",
        "## Data Quality Indicators\n",
        "Monitors and reports:\n",
        "* Number of empty columns\n",
        "* Fields with critical missing data\n",
        "* Patterns in data completeness\n",
        "* Potential data collection issues\n",
        "\n",
        "## Error Protection\n",
        "* Validates input DataFrame\n",
        "* Checks for empty datasets\n",
        "* Provides detailed error logging\n",
        "* Creates timestamped analysis file"
      ],
      "metadata": {
        "id": "BujJiwanyI4D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LuWV3STHsxo",
        "outputId": "d5260c37-0470-42d1-b04f-e430eb126475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting null value analysis...\n",
            "Calculating null statistics...\n",
            "\n",
            "Null Value Analysis Overview:\n",
            "--------------------------------------------------\n",
            "Total rows in dataset: 393,664\n",
            "Total columns analyzed: 43\n",
            "\n",
            "Null Value Summary:\n",
            "Severe (>75%): 1 columns\n",
            "High (50-75%): 7 columns\n",
            "Moderate (25-50%): 2 columns\n",
            "Low (<25%): 33 columns\n",
            "\n",
            "Columns with more than 50% null values:\n",
            "--------------------------------------------------\n",
            "Tax_Assessed_Value             (78.98% nulls)\n",
            "Num_Garage_Spaces              (74.89% nulls)\n",
            "Purchase_Contract_Date         (73.92% nulls)\n",
            "Condo_Name                     (72.47% nulls)\n",
            "Condo_Fee                      (67.72% nulls)\n",
            "Below_Grade_Finished_Area      (65.67% nulls)\n",
            "Bedrooms_Below_Grade           (60.44% nulls)\n",
            "Commission                     (57.97% nulls)\n",
            "Analysis saved to: null_analysis_20250205_143329.csv\n",
            "Step 7 completed: Null value analysis\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# NULL VALUE ANALYSIS\n",
        "# =================================================================================\n",
        "\n",
        "def analyze_column_nulls(df, verbose=True, high_null_threshold=50):\n",
        "    \"\"\"\n",
        "    Analyze null values in each column of the DataFrame.\n",
        "    Shows basic statistics about null values in each column.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame to analyze\n",
        "        verbose (bool): Whether to print additional analysis details (default: True)\n",
        "        high_null_threshold (int): Percentage threshold to flag high null columns (default: 50)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame containing null value analysis results\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Input validation\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            logger.log_error(\"Input must be a pandas DataFrame\")\n",
        "            raise TypeError(\"Input must be a pandas DataFrame\")\n",
        "\n",
        "        if len(df) == 0:\n",
        "            logger.log_error(\"DataFrame is empty\")\n",
        "            raise ValueError(\"DataFrame is empty\")\n",
        "\n",
        "        # Calculate null statistics\n",
        "        logger.log_info(\"Calculating null statistics...\")\n",
        "\n",
        "        # Create the analysis DataFrame\n",
        "        null_analysis = pd.DataFrame({\n",
        "            'null_count': df.isnull().sum(),\n",
        "            'total_rows': len(df),\n",
        "            'null_percentage': (df.isnull().sum() / len(df) * 100)\n",
        "        }).reset_index()\n",
        "\n",
        "        # Rename columns and sort\n",
        "        null_analysis.columns = ['Column_Name', 'Null_Count', 'Total_Rows', 'Null_Percentage']\n",
        "        null_analysis = null_analysis.sort_values('Null_Count', ascending=False)\n",
        "\n",
        "        if verbose:\n",
        "            # Print basic overview\n",
        "            logger.log_info(\"\\nNull Value Analysis Overview:\")\n",
        "            logger.log_info(\"-\" * 50)\n",
        "            logger.log_info(f\"Total rows in dataset: {len(df):,}\")\n",
        "            logger.log_info(f\"Total columns analyzed: {len(df.columns):,}\")\n",
        "\n",
        "            # Categorize and summarize nulls\n",
        "            severe_nulls = len(null_analysis[null_analysis['Null_Percentage'] > 75])\n",
        "            high_nulls = len(null_analysis[null_analysis['Null_Percentage'].between(50, 75)])\n",
        "            moderate_nulls = len(null_analysis[null_analysis['Null_Percentage'].between(25, 50)])\n",
        "            low_nulls = len(null_analysis[null_analysis['Null_Percentage'] < 25])\n",
        "\n",
        "            logger.log_info(\"\\nNull Value Summary:\")\n",
        "            logger.log_info(f\"Severe (>75%): {severe_nulls} columns\")\n",
        "            logger.log_info(f\"High (50-75%): {high_nulls} columns\")\n",
        "            logger.log_info(f\"Moderate (25-50%): {moderate_nulls} columns\")\n",
        "            logger.log_info(f\"Low (<25%): {low_nulls} columns\")\n",
        "\n",
        "            # List columns with high null values\n",
        "            high_null_cols = null_analysis[null_analysis['Null_Percentage'] > high_null_threshold]\n",
        "            if not high_null_cols.empty:\n",
        "                logger.log_info(f\"\\nColumns with more than {high_null_threshold}% null values:\")\n",
        "                logger.log_info(\"-\" * 50)\n",
        "                for _, row in high_null_cols.iterrows():\n",
        "                    logger.log_info(f\"{row['Column_Name']:<30} ({row['Null_Percentage']:.2f}% nulls)\")\n",
        "\n",
        "        return null_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_column_nulls function: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the analysis\n",
        "try:\n",
        "    logger.log_info(\"Starting null value analysis...\")\n",
        "\n",
        "    # Run the analysis\n",
        "    null_stats = analyze_column_nulls(combined_data)\n",
        "\n",
        "    # Save the results with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_filename = f'null_analysis_{timestamp}.csv'\n",
        "    null_stats.to_csv(output_filename, index=False)\n",
        "    logger.log_info(f\"Analysis saved to: {output_filename}\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"Null value analysis\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"Error during analysis: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmcWixx1f_Jz"
      },
      "source": [
        "# Data Validation: Days on Market Logic Check\n",
        "\n",
        "## Overview\n",
        "This code block performs logical validation of Days on Market (DOM) and Cumulative Days on Market (CDOM) values. This validation is crucial because CDOM should always be greater than or equal to DOM - any violations indicate potential data quality issues or incorrect data entry.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input\n",
        "* DataFrame Columns:\n",
        "  * Days_On_Market (DOM)\n",
        "  * Cumulative_Days_On_Market (CDOM)\n",
        "  * Close_Date\n",
        "* Parameter:\n",
        "  * verbose: Controls detail level of reporting\n",
        "\n",
        "### Validation Steps\n",
        "1. Data Preparation\n",
        "   * Converts DOM to numeric values\n",
        "   * Converts CDOM to numeric values\n",
        "   * Validates Close_Date format\n",
        "   * Creates data backup for safety\n",
        "\n",
        "2. Logic Validation\n",
        "   * Identifies cases where CDOM < DOM\n",
        "   * Calculates the difference between DOM and CDOM\n",
        "   * Groups discrepancies by magnitude\n",
        "   * Excludes records with null values\n",
        "\n",
        "3. Analysis Reporting\n",
        "   * Total listings analyzed\n",
        "   * Number of invalid cases\n",
        "   * Error rate calculation\n",
        "   * Distribution of discrepancies\n",
        "   * CSV export of invalid records\n",
        "\n",
        "### Quality Metrics\n",
        "The code reports:\n",
        "* Total records analyzed\n",
        "* Invalid DOM/CDOM pairs found\n",
        "* Error rate percentage\n",
        "* Discrepancy ranges:\n",
        "  * 1-10 days\n",
        "  * 11-30 days\n",
        "  * 31-60 days\n",
        "  * 61-90 days\n",
        "  * 90+ days\n",
        "\n",
        "## Error Protection\n",
        "* Creates backup of DOM/CDOM data\n",
        "* Validates required columns\n",
        "* Handles invalid numeric values\n",
        "* Restores original data if errors occur\n",
        "* Saves invalid records for review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFcDwAHxf_XN",
        "outputId": "72ade9b8-79e3-44a5-987c-266ba73b2cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Days on Market timing analysis...\n",
            "Created backup of DOM/CDOM data\n",
            "Starting DOM timing analysis...\n",
            "\n",
            "Converting DOM and CDOM to numeric values...\n",
            "Found 5 invalid values in Days_On_Market\n",
            "Found 1,536 invalid values in Cumulative_Days_On_Market\n",
            "\n",
            "Days on Market (DOM) Analysis Results:\n",
            "============================================================\n",
            "\n",
            "Overview:\n",
            "----------------------------------------\n",
            "Total listings analyzed: 393,664\n",
            "Invalid DOM/CDOM cases: 7,145\n",
            "Error rate: 1.81%\n",
            "\n",
            "Largest Discrepancies Summary:\n",
            "----------------------------------------\n",
            "1-10 days: 6,589 cases\n",
            "11-30 days: 440 cases\n",
            "31-60 days: 68 cases\n",
            "61-90 days: 18 cases\n",
            "90+ days: 30 cases\n",
            "\n",
            "Invalid records saved to: invalid_dom_records_20250205_143329.csv\n",
            "\n",
            "Analysis completed successfully! 🎉\n",
            "Step 8 completed: DOM timing analysis\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# DAYS ON MARKET TIMING ANALYSIS\n",
        "# =================================================================================\n",
        "\n",
        "def analyze_dom_timing(df, verbose=True):\n",
        "    \"\"\"\n",
        "    Analyze cases where Cumulative Days on Market (CDOM) is less than Days on Market (DOM).\n",
        "    This should not happen as CDOM should always be >= DOM.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): Input DataFrame containing real estate listings\n",
        "        verbose (bool): Whether to print analysis details (default: True)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame containing the invalid records for review\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # STEP 1: Input validation and data preparation\n",
        "        required_columns = ['Days_On_Market', 'Cumulative_Days_On_Market', 'Close_Date']\n",
        "        if not all(col in df.columns for col in required_columns):\n",
        "            error_msg = \"Missing required columns for DOM analysis\"\n",
        "            logger.log_error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        # Create a copy for safety\n",
        "        df = df.copy()\n",
        "        logger.log_info(\"Starting DOM timing analysis...\")\n",
        "\n",
        "        # STEP 2: Convert DOM and CDOM to numeric values\n",
        "        logger.log_info(\"\\nConverting DOM and CDOM to numeric values...\")\n",
        "\n",
        "        # Convert Days_On_Market to numeric\n",
        "        df['Days_On_Market'] = pd.to_numeric(df['Days_On_Market'], errors='coerce')\n",
        "        dom_nulls = df['Days_On_Market'].isnull().sum()\n",
        "        if dom_nulls > 0:\n",
        "            logger.log_info(f\"Found {dom_nulls:,} invalid values in Days_On_Market\")\n",
        "\n",
        "        # Convert Cumulative_Days_On_Market to numeric\n",
        "        df['Cumulative_Days_On_Market'] = pd.to_numeric(df['Cumulative_Days_On_Market'], errors='coerce')\n",
        "        cdom_nulls = df['Cumulative_Days_On_Market'].isnull().sum()\n",
        "        if cdom_nulls > 0:\n",
        "            logger.log_info(f\"Found {cdom_nulls:,} invalid values in Cumulative_Days_On_Market\")\n",
        "\n",
        "        # STEP 3: Convert Close_Date to datetime\n",
        "        df['Close_Date'] = pd.to_datetime(df['Close_Date'], errors='coerce')\n",
        "        date_nulls = df['Close_Date'].isnull().sum()\n",
        "        if date_nulls > 0:\n",
        "            logger.log_info(f\"Found {date_nulls:,} invalid dates in Close_Date\")\n",
        "\n",
        "        # STEP 4: Find records where CDOM < DOM (invalid cases)\n",
        "        invalid_mask = (\n",
        "            (df['Cumulative_Days_On_Market'] < df['Days_On_Market']) &     # CDOM less than DOM\n",
        "            (df['Cumulative_Days_On_Market'].notna()) &                    # CDOM not null\n",
        "            (df['Days_On_Market'].notna()) &                              # DOM not null\n",
        "            (df['Close_Date'].notna())                                    # Date not null\n",
        "        )\n",
        "\n",
        "        invalid_records = df[invalid_mask].copy()\n",
        "\n",
        "        # Calculate the difference between DOM and CDOM\n",
        "        invalid_records['DOM_CDOM_Diff'] = (\n",
        "            invalid_records['Days_On_Market'] -\n",
        "            invalid_records['Cumulative_Days_On_Market']\n",
        "        )\n",
        "\n",
        "        # STEP 5: Log Analysis Results\n",
        "        logger.log_info(\"\\nDays on Market (DOM) Analysis Results:\")\n",
        "        logger.log_info(\"=\" * 60)\n",
        "\n",
        "        # Basic Statistics\n",
        "        total_listings = len(df)\n",
        "        invalid_count = len(invalid_records)\n",
        "        error_rate = (invalid_count / total_listings) * 100 if total_listings > 0 else 0\n",
        "\n",
        "        logger.log_info(f\"\\nOverview:\")\n",
        "        logger.log_info(\"-\" * 40)\n",
        "        logger.log_info(f\"Total listings analyzed: {total_listings:,}\")\n",
        "        logger.log_info(f\"Invalid DOM/CDOM cases: {invalid_count:,}\")\n",
        "        logger.log_info(f\"Error rate: {error_rate:.2f}%\")\n",
        "\n",
        "        if not invalid_records.empty:\n",
        "            # Display only the count of discrepancies\n",
        "            logger.log_info(\"\\nLargest Discrepancies Summary:\")\n",
        "            logger.log_info(\"-\" * 40)\n",
        "\n",
        "            # Group by difference ranges for a cleaner summary\n",
        "            ranges = [0, 10, 30, 60, 90, float('inf')]\n",
        "            labels = ['1-10 days', '11-30 days', '31-60 days', '61-90 days', '90+ days']\n",
        "            invalid_records['diff_range'] = pd.cut(\n",
        "                invalid_records['DOM_CDOM_Diff'],\n",
        "                bins=ranges,\n",
        "                labels=labels,\n",
        "                right=False\n",
        "            )\n",
        "            range_counts = invalid_records['diff_range'].value_counts().sort_index()\n",
        "\n",
        "            for range_name, count in range_counts.items():\n",
        "                logger.log_info(f\"{range_name}: {count:,} cases\")\n",
        "\n",
        "        return invalid_records\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"\\nError in analyze_dom_timing function:\")\n",
        "        logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "        logger.log_error(f\"Details: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the timing analysis\n",
        "try:\n",
        "    logger.log_info(\"Starting Days on Market timing analysis...\")\n",
        "\n",
        "    # Create backup before analysis\n",
        "    dom_backup = combined_data[['Days_On_Market', 'Cumulative_Days_On_Market']].copy()\n",
        "    logger.log_info(\"Created backup of DOM/CDOM data\")\n",
        "\n",
        "    # Perform the analysis\n",
        "    invalid_records = analyze_dom_timing(combined_data)\n",
        "\n",
        "    # Save results if there are any invalid records\n",
        "    if len(invalid_records) > 0:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        output_filename = f'invalid_dom_records_{timestamp}.csv'\n",
        "        invalid_records.to_csv(output_filename, index=False)\n",
        "        logger.log_info(f\"\\nInvalid records saved to: {output_filename}\")\n",
        "\n",
        "    logger.log_info(\"\\nAnalysis completed successfully! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"DOM timing analysis\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during DOM timing analysis:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")\n",
        "\n",
        "    # Restore original DOM values if there was an error\n",
        "    combined_data[['Days_On_Market', 'Cumulative_Days_On_Market']] = dom_backup\n",
        "    logger.log_info(\"\\nRestored original DOM values due to error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4_wj0_WFqjn"
      },
      "source": [
        "# Data Quality: RMS Value Detection and Correction Using Machine Learning\n",
        "\n",
        "## Overview\n",
        "This code block implements an intelligent system to identify and correct potentially erroneous RMS (Residential Measurement Standard) Total values in our real estate dataset. It uses machine learning clustering techniques combined with domain knowledge to identify outliers and apply corrections based on similar properties.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input Requirements\n",
        "* Primary Columns:\n",
        "  * RMS_Total: Property square footage\n",
        "  * Close_Price: Sale price for validation\n",
        "  * Close_Date: Transaction date for temporal analysis\n",
        "* Grouping Columns:\n",
        "  * Subdivision_Name: For neighborhood comparison\n",
        "  * Property_Sub_Type: Property classification\n",
        "  * Total_Baths: Additional validation metric\n",
        "\n",
        "### Machine Learning Approach\n",
        "1. Data Preparation\n",
        "   * Cleans numeric formatting (removes $, commas)\n",
        "   * Handles missing and invalid values\n",
        "   * Groups properties by subdivision and type\n",
        "   * Requires minimum 10 properties per group for analysis\n",
        "\n",
        "2. Clustering Analysis\n",
        "   * Uses K-means clustering (n_clusters=3)\n",
        "   * Features used:\n",
        "     * RMS_Total (square footage)\n",
        "     * Close_Price (sale price)\n",
        "     * Price_Per_SqFt (derived metric)\n",
        "   * Standardizes features for better clustering\n",
        "   * Calculates distance to cluster centers\n",
        "\n",
        "3. Suspicion Score Calculation\n",
        "   * Base factors:\n",
        "     * Size deviation from group median\n",
        "     * Price per square foot ratio\n",
        "     * Distance from cluster centers\n",
        "   * Adjustment factors:\n",
        "     * Increases score for size/price mismatches\n",
        "     * Reduces score for luxury properties\n",
        "\n",
        "### Correction Process\n",
        "1. Property Matching\n",
        "   * Finds similar properties within:\n",
        "     * Same subdivision\n",
        "     * Same property type\n",
        "     * Same bathroom count\n",
        "     * 12-month date range\n",
        "\n",
        "2. Value Replacement\n",
        "   * Requires minimum 3 similar properties\n",
        "   * Uses median RMS from similar properties\n",
        "   * Falls back to broader criteria if needed\n",
        "   * Only corrects highly suspicious cases (score > 1000)\n",
        "\n",
        "### Quality Controls\n",
        "* Creates data backup before processing\n",
        "* Tracks all changes made\n",
        "* Reports correction statistics:\n",
        "  * Total suspicious cases\n",
        "  * Number of fixes applied\n",
        "  * Size of corrections\n",
        "  * Impact statistics\n",
        "\n",
        "## Process Protection\n",
        "The code includes several safeguards:\n",
        "* Minimum sample sizes for analysis\n",
        "* Conservative correction threshold\n",
        "* Multiple validation criteria\n",
        "* Detailed logging of changes\n",
        "* Error recovery capabilities\n",
        "\n",
        "## Output Details\n",
        "The code provides:\n",
        "1. Initial Analysis:\n",
        "   * Total properties processed\n",
        "   * Valid RMS count\n",
        "   * Distribution statistics\n",
        "\n",
        "2. Processing Summary:\n",
        "   * Groups analyzed vs skipped\n",
        "   * Suspicious cases identified\n",
        "   * Corrections applied\n",
        "\n",
        "3. Detailed Fix Report:\n",
        "   * Location details\n",
        "   * Original and new values\n",
        "   * Percentage changes\n",
        "   * Supporting property counts\n",
        "\n",
        "## Usage Notes\n",
        "* Set suspicion_threshold higher for more conservative corrections\n",
        "* Adjust n_clusters based on property variety\n",
        "* Monitor correction sizes for reasonableness\n",
        "* Review skipped groups for potential issues"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =================================================================================\n",
        "# RMS TOTAL ANALYSIS AND CORRECTION\n",
        "# =================================================================================\n",
        "\n",
        "def analyze_and_fix_rms_values(df, n_clusters=3, suspicion_threshold=1000):\n",
        "    \"\"\"\n",
        "    Identify and fix suspicious RMS_Total values in real estate data.\n",
        "    Shows top suspicious cases before making changes and automatically fixes cases\n",
        "    above the suspicion threshold.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing real estate data\n",
        "        n_clusters: Number of clusters for analyzing property groups (default: 3)\n",
        "        suspicion_threshold: Score above which values will be fixed (default: 1000)\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with corrected RMS_Total values\n",
        "        dict with correction statistics\n",
        "    \"\"\"\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import numpy as np\n",
        "\n",
        "    logger.log_info(\"Starting RMS Total analysis and correction process...\")\n",
        "\n",
        "    try:\n",
        "        # Create a copy for safety\n",
        "        df_work = df.copy()\n",
        "        logger.log_info(\"Created working copy of DataFrame\")\n",
        "\n",
        "        # Clean numeric columns by removing formatting and converting to numbers\n",
        "        logger.log_info(\"\\nCleaning numeric data...\")\n",
        "        numeric_cols = ['RMS_Total', 'Close_Price']\n",
        "        for col in tqdm(numeric_cols, desc=\"Cleaning numeric columns\"):\n",
        "            df_work[col] = pd.to_numeric(\n",
        "                df_work[col].astype(str).str.replace(',', '').str.replace('$', ''),\n",
        "                errors='coerce'\n",
        "            )\n",
        "            # Remove clearly impossible values (negative or zero)\n",
        "            invalid_count = (df_work[col] <= 0).sum()\n",
        "            if invalid_count > 0:\n",
        "                logger.log_info(f\"Removed {invalid_count} invalid values from {col}\")\n",
        "            df_work.loc[df_work[col] <= 0, col] = np.nan\n",
        "\n",
        "        # Calculate price per square foot for analysis\n",
        "        df_work['Price_Per_SqFt'] = df_work['Close_Price'] / df_work['RMS_Total']\n",
        "        df_work['Close_Date'] = pd.to_datetime(df_work['Close_Date'])\n",
        "\n",
        "        # Log initial statistics\n",
        "        logger.log_info(\"\\nInitial Data Overview:\")\n",
        "        logger.log_info(f\"Total properties: {len(df_work):,}\")\n",
        "        logger.log_info(f\"Properties with valid RMS_Total: {df_work['RMS_Total'].notna().sum():,}\")\n",
        "        logger.log_info(f\"Largest RMS_Total value: {df_work['RMS_Total'].max():,.0f}\")\n",
        "        logger.log_info(f\"Properties over 10,000 sq ft: {len(df_work[df_work['RMS_Total'] > 10000]):,}\")\n",
        "\n",
        "        # Track skipped groups and their reasons\n",
        "        skipped_groups = []\n",
        "        suspicious_cases = []\n",
        "\n",
        "        # Analyze each subdivision/property type group separately\n",
        "        property_groups = df_work.groupby(['Subdivision_Name', 'Property_Sub_Type'])\n",
        "\n",
        "        logger.log_info(\"\\nAnalyzing property groups...\")\n",
        "        logger.log_info(f\"Total groups to process: {len(property_groups):,}\")\n",
        "\n",
        "        for (subdivision, prop_type), group in tqdm(property_groups, desc=\"Processing groups\"):\n",
        "            # Only analyze groups with enough valid data\n",
        "            valid_data = group[['RMS_Total', 'Close_Price', 'Price_Per_SqFt']].notna().all(axis=1)\n",
        "            valid_group = group[valid_data]\n",
        "\n",
        "            if len(valid_group) < 10:  # Not enough data\n",
        "                skipped_groups.append({\n",
        "                    'Subdivision': subdivision,\n",
        "                    'Property_Type': prop_type,\n",
        "                    'Count': len(valid_group),\n",
        "                    'Valid_Count': len(valid_group)\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Original clustering and analysis code remains the same\n",
        "            # Calculate statistics for this group\n",
        "            group_stats = {\n",
        "                'median_size': valid_group['RMS_Total'].median(),\n",
        "                'median_price': valid_group['Close_Price'].median(),\n",
        "                'median_ppsf': valid_group['Price_Per_SqFt'].median()\n",
        "            }\n",
        "\n",
        "            # Prepare features for clustering analysis\n",
        "            features = valid_group[['RMS_Total', 'Close_Price', 'Price_Per_SqFt']].copy()\n",
        "\n",
        "            # Scale features for better clustering\n",
        "            scaler = StandardScaler()\n",
        "            scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "            # Perform clustering to identify groups of similar properties\n",
        "            kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "            cluster_labels = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "            # Find outliers based on distance to nearest cluster center\n",
        "            distances = kmeans.transform(scaled_features)\n",
        "            min_distances = distances.min(axis=1)\n",
        "            distance_threshold = np.mean(min_distances) + (2 * np.std(min_distances))\n",
        "            potential_errors = min_distances > distance_threshold\n",
        "\n",
        "            if potential_errors.any():\n",
        "                suspicious_indices = features.index[potential_errors]\n",
        "                suspicious_group = valid_group.loc[suspicious_indices].copy()\n",
        "\n",
        "                # Calculate comparison metrics\n",
        "                suspicious_group['Distance_Score'] = min_distances[potential_errors]\n",
        "                suspicious_group['Group_Median_Size'] = group_stats['median_size']\n",
        "                suspicious_group['Group_Median_Price'] = group_stats['median_price']\n",
        "                suspicious_group['Group_Median_PPSF'] = group_stats['median_ppsf']\n",
        "\n",
        "                # Calculate comparison ratios\n",
        "                suspicious_group['Size_Ratio'] = (\n",
        "                    suspicious_group['RMS_Total'] / group_stats['median_size']\n",
        "                )\n",
        "                suspicious_group['Price_Ratio'] = (\n",
        "                    suspicious_group['Close_Price'] / group_stats['median_price']\n",
        "                )\n",
        "                suspicious_group['PPSF_Ratio'] = (\n",
        "                    suspicious_group['Price_Per_SqFt'] / group_stats['median_ppsf']\n",
        "                )\n",
        "\n",
        "                # Calculate suspicion score\n",
        "                size_deviation = abs(suspicious_group['Size_Ratio'] - 1)\n",
        "                price_factor = 1 / np.clip(suspicious_group['PPSF_Ratio'], 0.2, 5)\n",
        "\n",
        "                suspicious_group['Suspicion_Score'] = (\n",
        "                    size_deviation *\n",
        "                    price_factor *\n",
        "                    suspicious_group['Distance_Score']\n",
        "                )\n",
        "\n",
        "                # Adjust scores based on additional factors\n",
        "                mismatch_mask = (\n",
        "                    (suspicious_group['Size_Ratio'] > 5) &\n",
        "                    (suspicious_group['Price_Ratio'] < 2)\n",
        "                )\n",
        "                suspicious_group.loc[mismatch_mask, 'Suspicion_Score'] *= 2\n",
        "\n",
        "                luxury_mask = (\n",
        "                    (suspicious_group['Price_Per_SqFt'] > 500) &\n",
        "                    (suspicious_group['Close_Price'] > 1000000)\n",
        "                )\n",
        "                suspicious_group.loc[luxury_mask, 'Suspicion_Score'] *= 0.5\n",
        "\n",
        "                suspicious_cases.append(suspicious_group)\n",
        "\n",
        "              # Log information about skipped groups\n",
        "        if skipped_groups:\n",
        "            skipped_df = pd.DataFrame(skipped_groups)\n",
        "            logger.log_info(\"\\nGroups Skipped Due to Insufficient Data:\")\n",
        "            logger.log_info(\"-\" * 50)\n",
        "            logger.log_info(f\"Total groups skipped: {len(skipped_groups)}\")\n",
        "            logger.log_info(f\"Total properties in skipped groups: {skipped_df['Count'].sum():,}\")\n",
        "            logger.log_info(\"\\nTop 10 skipped groups by size:\")\n",
        "            for _, row in skipped_df.nlargest(10, 'Count').iterrows():\n",
        "                logger.log_info(f\"- {row['Subdivision']} ({row['Property_Type']}): {row['Count']} properties\")\n",
        "            logger.log_info(f\"\\nMedian properties in skipped groups: {skipped_df['Count'].median():.0f}\")\n",
        "\n",
        "        # Process suspicious cases if any were found\n",
        "        correction_stats = {\n",
        "            'total_suspicious': 0,\n",
        "            'fixes_applied': 0,\n",
        "            'largest_correction': 0,\n",
        "            'median_correction': 0\n",
        "        }\n",
        "\n",
        "        if suspicious_cases:\n",
        "            suspicious_df = pd.concat(suspicious_cases)\n",
        "            suspicious_df = suspicious_df.sort_values('Suspicion_Score', ascending=False)\n",
        "\n",
        "            correction_stats['total_suspicious'] = len(suspicious_df)\n",
        "\n",
        "            # Log summary of suspicious cases\n",
        "            logger.log_info(f\"\\nFound {len(suspicious_df):,} suspicious cases\")\n",
        "            logger.log_info(f\"Suspicion score range: {suspicious_df['Suspicion_Score'].min():.2f} to {suspicious_df['Suspicion_Score'].max():.2f}\")\n",
        "\n",
        "            # Process cases needing fixes\n",
        "            cases_to_fix = suspicious_df[suspicious_df['Suspicion_Score'] > suspicion_threshold]\n",
        "            logger.log_info(f\"\\nProceeding with fixes for {len(cases_to_fix):,} cases...\")\n",
        "\n",
        "            fixes_log = []\n",
        "\n",
        "            # Process each case that needs fixing\n",
        "            for idx, case in tqdm(cases_to_fix.iterrows(), desc=\"Applying fixes\"):\n",
        "                case_date = df_work.loc[idx, 'Close_Date']\n",
        "\n",
        "                # Find similar properties\n",
        "                similar_properties = df_work[\n",
        "                    (df_work['Subdivision_Name'] == case['Subdivision_Name']) &\n",
        "                    (df_work['Property_Sub_Type'] == case['Property_Sub_Type']) &\n",
        "                    (df_work['Total_Baths'] == case['Total_Baths']) &\n",
        "                    (abs((df_work['Close_Date'] - case_date).dt.days) <= 365) &\n",
        "                    (df_work.index != idx)\n",
        "                ]\n",
        "\n",
        "                # Get replacement value\n",
        "                if len(similar_properties) >= 3:\n",
        "                    new_rms = similar_properties['RMS_Total'].median()\n",
        "                else:\n",
        "                    # Try without bathroom match\n",
        "                    similar_properties = df_work[\n",
        "                        (df_work['Subdivision_Name'] == case['Subdivision_Name']) &\n",
        "                        (df_work['Property_Sub_Type'] == case['Property_Sub_Type']) &\n",
        "                        (abs((df_work['Close_Date'] - case_date).dt.days) <= 365) &\n",
        "                        (df_work.index != idx)\n",
        "                    ]\n",
        "                    new_rms = similar_properties['RMS_Total'].median()\n",
        "\n",
        "                if pd.notna(new_rms):\n",
        "                    # Log fix details\n",
        "                    fixes_log.append({\n",
        "                        'Index': idx,\n",
        "                        'Subdivision': case['Subdivision_Name'],\n",
        "                        'Property_Type': case['Property_Sub_Type'],\n",
        "                        'Original_RMS': case['RMS_Total'],\n",
        "                        'New_RMS': new_rms,\n",
        "                        'Similar_Properties': len(similar_properties),\n",
        "                        'Close_Date': case_date,\n",
        "                        'Suspicion_Score': case['Suspicion_Score']\n",
        "                    })\n",
        "\n",
        "                    # Apply fix\n",
        "                    df_work.loc[idx, 'RMS_Total'] = new_rms\n",
        "                    correction_stats['fixes_applied'] += 1\n",
        "\n",
        "            # Create fix report\n",
        "            if fixes_log:\n",
        "                fixes_df = pd.DataFrame(fixes_log)\n",
        "                fixes_df['Pct_Change'] = (\n",
        "                    (fixes_df['New_RMS'] - fixes_df['Original_RMS']) /\n",
        "                    fixes_df['Original_RMS'] * 100\n",
        "                )\n",
        "\n",
        "                correction_stats['largest_correction'] = abs(fixes_df['Pct_Change']).max()\n",
        "                correction_stats['median_correction'] = abs(fixes_df['Pct_Change']).median()\n",
        "\n",
        "                # Log fix summary\n",
        "                logger.log_info(\"\\nFix Summary:\")\n",
        "                logger.log_info(f\"Total fixes made: {len(fixes_log):,}\")\n",
        "                logger.log_info(f\"Largest correction: {correction_stats['largest_correction']:.1f}%\")\n",
        "                logger.log_info(f\"Median correction: {correction_stats['median_correction']:.1f}%\")\n",
        "\n",
        "                # Log details of each fix\n",
        "                logger.log_info(\"\\nDetailed Fix Report:\")\n",
        "                logger.log_info(\"-\" * 50)\n",
        "                for fix in fixes_log:\n",
        "                    logger.log_info(f\"\\nProperty in {fix['Subdivision']}\")\n",
        "                    logger.log_info(f\"Type: {fix['Property_Type']}\")\n",
        "                    logger.log_info(f\"Original RMS: {fix['Original_RMS']:,.0f} sq ft\")\n",
        "                    logger.log_info(f\"New RMS: {fix['New_RMS']:,.0f} sq ft\")\n",
        "                    logger.log_info(f\"Change: {((fix['New_RMS'] - fix['Original_RMS']) / fix['Original_RMS'] * 100):,.1f}%\")\n",
        "                    logger.log_info(f\"Based on {fix['Similar_Properties']} similar properties\")\n",
        "\n",
        "        return df_work, correction_stats\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"\\nError in analyze_and_fix_rms_values function:\")\n",
        "        logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "        logger.log_error(f\"Details: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the analysis and fixes\n",
        "try:\n",
        "    logger.log_info(\"Starting RMS Total analysis...\")\n",
        "\n",
        "    # Create a checkpoint of original data\n",
        "    # Convert to numeric type immediately to ensure consistent type\n",
        "    original_rms_values = pd.to_numeric(combined_data['RMS_Total'], errors='coerce')\n",
        "    logger.log_info(\"Created backup of original RMS values\")\n",
        "\n",
        "    # Process the data\n",
        "    corrected_data, stats = analyze_and_fix_rms_values(\n",
        "        combined_data,\n",
        "        suspicion_threshold=1000  # Only fix very suspicious cases\n",
        "    )\n",
        "\n",
        "    # Log summary statistics\n",
        "    logger.log_info(\"\\nCorrection Summary:\")\n",
        "    logger.log_info(\"-\" * 50)\n",
        "    logger.log_info(f\"Total suspicious cases identified: {stats['total_suspicious']:,}\")\n",
        "    logger.log_info(f\"Total fixes applied: {stats['fixes_applied']:,}\")\n",
        "    if stats['fixes_applied'] > 0:\n",
        "        logger.log_info(f\"Largest correction: {stats['largest_correction']:.1f}%\")\n",
        "        logger.log_info(f\"Median correction: {stats['median_correction']:.1f}%\")\n",
        "\n",
        "    # Update main DataFrame\n",
        "    combined_data = corrected_data\n",
        "\n",
        "    # Verify changes with meaningful difference threshold\n",
        "    meaningful_diff = 0.01  # 1% difference threshold\n",
        "\n",
        "    # Ensure both arrays are numeric for comparison\n",
        "    current_rms = pd.to_numeric(combined_data['RMS_Total'], errors='coerce')\n",
        "    original_rms = pd.to_numeric(original_rms_values, errors='coerce')\n",
        "\n",
        "    # Calculate meaningful changes\n",
        "    valid_mask = (current_rms.notna() & original_rms.notna() & (original_rms != 0))\n",
        "    changes_made = (\n",
        "        (abs(current_rms[valid_mask] - original_rms[valid_mask]) / original_rms[valid_mask]) > meaningful_diff\n",
        "    ).sum()\n",
        "\n",
        "    logger.log_info(f\"\\nTotal records meaningfully modified: {changes_made:,}\")\n",
        "    logger.log_info(f\"Any remaining large RMS values (>20,000): {(current_rms > 20000).sum():,}\")\n",
        "\n",
        "    logger.log_info(\"\\nProcess completed successfully! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"RMS Total analysis and correction\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during RMS analysis:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")\n",
        "\n",
        "    # Restore original values if there was an error\n",
        "    combined_data['RMS_Total'] = original_rms_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ed55d093bbf546839a34258b58be4140",
            "19994ba2eda24344805eb8682c72d0be",
            "cc9d4a6c52bf4698b5e13207a6eb947c",
            "e7cc087d565e42b6b9e276e7be7320a2",
            "da2c3ccad85543e5820c6a300985a07d",
            "9846c6ec58a8433085766427545cb79a",
            "7b915589b18e47248e55d81d92e08b14",
            "b781b39254c945d9be441aaa684492b8",
            "d4a5166c8108492eaa64a2bd24d7b72d",
            "f0160ab2289b466ca737cac866445a82",
            "d7c09597395c46d29ff31e85421e1f97",
            "f174308569244417b9e38a4f4ac4a1f9",
            "7711972b8e5c43289e6db5f5771a408f",
            "9fb57e7a18e54a6181cc9e7971384e02",
            "088d7b2ec5e24f80b77df543fc9675d9",
            "2c0572fc1e414d469bd4a6473bffeacd",
            "009b97be026444e4b8bd379b512d80eb",
            "fdc30f0c7c784bf69028339aad71966f",
            "380a85f2bec64265a687160765417809",
            "f5001ee1f4f84a15aafb3ef5675f25ed",
            "eb64f9452afd4715a7f7442f4f346734",
            "af2566f6a5e5422d8c09a37be75512ec",
            "0b727ae4f3ec455889c8f288fecc9274",
            "4e98e46ae97e4c7ea76da103cd546065",
            "89b4c2133c16492c9779ecc8ee488aa0",
            "90efc8e7700648a8963498a30448aabf",
            "740e4fe08cbe48ba9330b25cb96391a9",
            "cbd0e746a4bc4a99a5ed9aadec85e015",
            "2df88486920a465d9015510823d49385",
            "e55a69e02b494d0b96150071fbf47356",
            "2036966dee3a46d3a4985f67012ffe7e",
            "b45fe636754f4a189e03794dd7baf8fc",
            "72e68d17c540450d9f5d63487fa9ca4d"
          ]
        },
        "id": "bR7asXpVDnmI",
        "outputId": "f22134a4-c440-4dc2-a4ee-f6d467d7ca37"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting RMS Total analysis...\n",
            "Created backup of original RMS values\n",
            "Starting RMS Total analysis and correction process...\n",
            "Created working copy of DataFrame\n",
            "\n",
            "Cleaning numeric data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Cleaning numeric columns:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed55d093bbf546839a34258b58be4140"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 27 invalid values from RMS_Total\n",
            "\n",
            "Initial Data Overview:\n",
            "Total properties: 393,664\n",
            "Properties with valid RMS_Total: 392,767\n",
            "Largest RMS_Total value: 20,910\n",
            "Properties over 10,000 sq ft: 9\n",
            "\n",
            "Analyzing property groups...\n",
            "Total groups to process: 1,502\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing groups:   0%|          | 0/1502 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f174308569244417b9e38a4f4ac4a1f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Groups Skipped Due to Insufficient Data:\n",
            "--------------------------------------------------\n",
            "Total groups skipped: 340\n",
            "Total properties in skipped groups: 962\n",
            "\n",
            "Top 10 skipped groups by size:\n",
            "- Applewood Park (Semi Detached (Half Duplex)): 9 properties\n",
            "- Bow Ridge (Apartment): 9 properties\n",
            "- Deer Park (Detached): 9 properties\n",
            "- Downtown High River (Detached): 9 properties\n",
            "- Mount Pleasant (Full Duplex): 9 properties\n",
            "- Oriole Park (Row/Townhouse): 9 properties\n",
            "- South Point (Semi Detached (Half Duplex)): 9 properties\n",
            "- Williamstown (Semi Detached (Half Duplex)): 9 properties\n",
            "- Winston Heights/Mountview (Full Duplex): 9 properties\n",
            "- Bow Meadows (Row/Townhouse): 8 properties\n",
            "\n",
            "Median properties in skipped groups: 2\n",
            "\n",
            "Found 14,767 suspicious cases\n",
            "Suspicion score range: 0.00 to 1208.99\n",
            "\n",
            "Proceeding with fixes for 1 cases...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying fixes: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b727ae4f3ec455889c8f288fecc9274"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fix Summary:\n",
            "Total fixes made: 1\n",
            "Largest correction: 89.6%\n",
            "Median correction: 89.6%\n",
            "\n",
            "Detailed Fix Report:\n",
            "--------------------------------------------------\n",
            "\n",
            "Property in Banff Trail\n",
            "Type: Detached\n",
            "Original RMS: 10,311 sq ft\n",
            "New RMS: 1,076 sq ft\n",
            "Change: -89.6%\n",
            "Based on 56 similar properties\n",
            "\n",
            "Correction Summary:\n",
            "--------------------------------------------------\n",
            "Total suspicious cases identified: 14,767\n",
            "Total fixes applied: 1\n",
            "Largest correction: 89.6%\n",
            "Median correction: 89.6%\n",
            "\n",
            "Total records meaningfully modified: 0\n",
            "Any remaining large RMS values (>20,000): 1\n",
            "\n",
            "Process completed successfully! 🎉\n",
            "Step 9 completed: RMS Total analysis and correction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_snh1uYokNAL"
      },
      "source": [
        "# Data Quality: Price Validation and Error Correction\n",
        "\n",
        "## Overview\n",
        "This code block implements a sophisticated system to detect and correct price errors in our real estate dataset. It uses statistical analysis and market comparisons to identify potential data entry errors, particularly focusing on order-of-magnitude mistakes (missing or extra zeros) that are common in real estate data entry.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input Requirements\n",
        "* Required Columns:\n",
        "  * Original_List_Price: Initial listing price\n",
        "  * Close_Price: Final sale price\n",
        "  * Close_Date: Transaction date\n",
        "  * Subdivision_Name: For market comparisons\n",
        "  * Property_Sub_Type: Property classification\n",
        "  * MLS_Num: Unique identifier\n",
        "\n",
        "### Analysis Components\n",
        "1. Price Validation\n",
        "   * Checks for valid numeric format\n",
        "   * Validates reasonable price range ($1,000 - $100M)\n",
        "   * Identifies null and invalid values\n",
        "   * Handles various price formats (with $, commas, spaces)\n",
        "\n",
        "2. Market Statistics\n",
        "   * Groups properties by:\n",
        "     * Subdivision\n",
        "     * Property type\n",
        "     * Year\n",
        "   * Calculates for each group:\n",
        "     * Median prices\n",
        "     * Price quartiles (Q1, Q3)\n",
        "   * Requires minimum 5 properties per group\n",
        "\n",
        "3. Error Detection\n",
        "   * Compares prices to group medians\n",
        "   * Identifies magnitude errors:\n",
        "     * Prices 100x above median\n",
        "     * Prices 0.01x below median\n",
        "   * Calculates appropriate corrections\n",
        "   * Verifies corrections are reasonable\n",
        "\n",
        "### Correction Process\n",
        "1. Data Preparation\n",
        "   * Cleans price formatting\n",
        "   * Converts to numeric values\n",
        "   * Creates year groupings\n",
        "   * Calculates market statistics\n",
        "\n",
        "2. Error Correction\n",
        "   * Identifies magnitude of error\n",
        "   * Adds or removes zeros as needed\n",
        "   * Validates corrections against market data\n",
        "   * Maintains detailed correction log\n",
        "\n",
        "### Quality Controls\n",
        "* Input validation\n",
        "* Data backups\n",
        "* Reasonable price range checks ($1K - $100M)\n",
        "* Minimum sample size requirements\n",
        "* Detailed logging of all changes\n",
        "* Error recovery capabilities\n",
        "\n",
        "## Process Safety Features\n",
        "1. Data Protection\n",
        "   * Creates working copy of data\n",
        "   * Backs up price columns\n",
        "   * Restores on error\n",
        "   * Validates all corrections\n",
        "\n",
        "2. Error Handling\n",
        "   * Checks for missing columns\n",
        "   * Validates input data types\n",
        "   * Handles null values gracefully\n",
        "   * Logs all errors with details\n",
        "\n",
        "3. Change Tracking\n",
        "   * Records all corrections\n",
        "   * Logs correction types\n",
        "   * Saves detailed report\n",
        "   * Timestamps all changes\n",
        "\n",
        "## Output Details\n",
        "The process provides:\n",
        "1. Invalid Value Report\n",
        "   * Count of invalid prices\n",
        "   * Examples of problematic data\n",
        "   * Null value analysis\n",
        "   * Format error examples\n",
        "\n",
        "2. Market Statistics\n",
        "   * Group-level metrics\n",
        "   * Valid group counts\n",
        "   * Market median prices\n",
        "   * Price distributions\n",
        "\n",
        "3. Correction Summary\n",
        "   * Total properties processed\n",
        "   * Number of corrections\n",
        "   * Types of corrections\n",
        "   * Before/after examples\n",
        "\n",
        "[Code block follows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "bT_KXxjgkM3x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "04b97d8f36ff4957a82c0c85b4d9b82a",
            "92b3a317ca144a5dbcc36253fcb88e6e",
            "8d70ee752b344f26bff7d6c89b08acc0",
            "6c4a18237c714e31a0aa9f21359c1d65",
            "bad63c23f2bd48de8a418c40ae1d80ce",
            "e5467c1fafeb42319e067e45067d194e",
            "7a75cf80e81a43a69f1f02b64ecd0e49",
            "c253bb15c43640e1baced52b194dc36d",
            "2442fb2a03d0431da146a8ef3a462d01",
            "2fac4eb210b9441187271e755690ccd7",
            "0b45a23e631b4875a1cdb9162d0a12ff"
          ]
        },
        "outputId": "63064a76-ba6a-4fba-dcd3-272f185cb8d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting comprehensive price analysis...\n",
            "Created backup of price columns\n",
            "Starting comprehensive price analysis and correction...\n",
            "Created working copy of DataFrame\n",
            "\n",
            "Analyzing current price data...\n",
            "\n",
            "Analyzing Original_List_Price:\n",
            "--------------------------------------------------\n",
            "Found 364 invalid values\n",
            "\n",
            "Examples of invalid values:\n",
            "Original_List_Price  MLS_Num           Property_Sub_Type Close_Date\n",
            "                NaN E4400087                    Detached 2024-08-07\n",
            "                NaN E4396127                    Detached 2024-08-28\n",
            "                NaN E4393149               Row/Townhouse 2024-09-04\n",
            "                NaN E4396386               Row/Townhouse 2024-07-16\n",
            "                NaN E4390924               Row/Townhouse 2024-06-14\n",
            "                NaN E4385380 Semi Detached (Half Duplex) 2024-06-29\n",
            "       $879,990,000 A2139548                    Detached 2024-07-10\n",
            "                NaN E4386378                    Detached 2024-06-06\n",
            "                NaN E4379753                    Detached 2024-05-30\n",
            "                NaN E4376094                    Detached 2024-03-25\n",
            "\n",
            "Null values: 194\n",
            "\n",
            "Sample of non-null invalid values:\n",
            "5186      $879,990,000\n",
            "39110     $519,800,000\n",
            "113067              $0\n",
            "114499              $0\n",
            "115798              $0\n",
            "\n",
            "Analyzing Close_Price:\n",
            "--------------------------------------------------\n",
            "Found 4 invalid values\n",
            "\n",
            "Examples of invalid values:\n",
            " Close_Price  MLS_Num Property_Sub_Type Close_Date\n",
            "     395.000 A1126158          Detached 2021-08-09\n",
            "     233.000 A1123684     Row/Townhouse 2021-07-13\n",
            "     357.500 A1090055          Detached 2021-05-31\n",
            "     340.000 A1022479     Row/Townhouse 2020-12-05\n",
            "\n",
            "Null values: 0\n",
            "\n",
            "Sample of non-null invalid values:\n",
            "104722   395.000\n",
            "109678   233.000\n",
            "114598   357.500\n",
            "129179   340.000\n",
            "\n",
            "Cleaning price data...\n",
            "Cleaned Original_List_Price - converted to numeric values\n",
            "Cleaned Close_Price - converted to numeric values\n",
            "\n",
            "Calculating market statistics...\n",
            "Converted dates and extracted year information\n",
            "Calculating market statistics by location and property type...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-50647b87df9f>:196: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  ).apply(calculate_market_stats)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed statistics for 11,091 out of 15,730 property groups\n",
            "\n",
            "Analyzing potential price errors...\n",
            "Checking individual properties...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/393664 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04b97d8f36ff4957a82c0c85b4d9b82a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Price Correction Summary:\n",
            "------------------------------------------------------------\n",
            "Total properties processed: 393,664\n",
            "Total corrections made: 53\n",
            "\n",
            "Types of corrections made:\n",
            "Removed 2 zeros: 32\n",
            "Added 2 zeros: 20\n",
            "Added 3 zeros: 1\n",
            "\n",
            "Example corrections (first 10):\n",
            " MLS_Num Property_Type     Subdivision Close_Date          Price_Type Original_Price Corrected_Price Market_Median Original_Ratio Correction_Type\n",
            "A2113715      Detached  Garden Heights 2024-05-31 Original_List_Price $89,990,000.00     $899,900.00   $659,500.00        136.45x Removed 2 zeros\n",
            "A2016154      Detached Currie Barracks 2023-02-04 Original_List_Price      $1,150.00   $1,150,000.00 $1,450,000.00          0.00x   Added 3 zeros\n",
            "A1178879      Detached  Canyon Meadows 2022-02-25 Original_List_Price      $5,999.00     $599,900.00   $649,900.00          0.01x   Added 2 zeros\n",
            "C4254583      Detached        Parkdale 2019-11-06 Original_List_Price      $1,809.00     $180,900.00 $1,275,000.00          0.00x   Added 2 zeros\n",
            "C4088179      Detached        Lakeview 2017-03-05 Original_List_Price      $1,299.00     $129,900.00   $644,450.00          0.00x   Added 2 zeros\n",
            "C4060672      Detached    West Springs 2016-06-03 Original_List_Price      $1,450.00     $145,000.00   $749,900.00          0.00x   Added 2 zeros\n",
            "C4007287     Apartment         Mission 2015-06-15 Original_List_Price $37,695,000.00     $376,950.00   $372,900.00        101.09x Removed 2 zeros\n",
            "C4003350      Detached Discovery Ridge 2015-05-15 Original_List_Price      $1,259.00     $125,900.00   $869,000.00          0.00x   Added 2 zeros\n",
            "C3651824      Detached     Signal Hill 2015-03-27 Original_List_Price      $1,050.00     $105,000.00   $674,950.00          0.00x   Added 2 zeros\n",
            "C3648797     Apartment  McKenzie Towne 2015-02-27 Original_List_Price $27,490,000.00     $274,900.00   $259,900.00        105.77x Removed 2 zeros\n",
            "\n",
            "Detailed correction log saved to: price_corrections_20250205_143534.csv\n",
            "\n",
            "Main dataset updated with price corrections! 🎉\n",
            "Step 10 completed: Price error analysis and correction\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# PRICE ERROR ANALYSIS AND CORRECTION - COMPREHENSIVE VERSION\n",
        "# =================================================================================\n",
        "\n",
        "def analyze_and_fix_prices(df):\n",
        "    \"\"\"\n",
        "    Comprehensive analysis and correction of price errors in real estate data.\n",
        "    Shows examples of invalid data, identifies specific types of errors,\n",
        "    and provides detailed reporting of corrections made.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame containing real estate listings\n",
        "\n",
        "    Returns:\n",
        "        tuple: (DataFrame with corrected prices, DataFrame with correction log)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting comprehensive price analysis and correction...\")\n",
        "\n",
        "        # STEP 1: Input validation\n",
        "        # -----------------------\n",
        "        required_columns = [\n",
        "            'Original_List_Price', 'Close_Price', 'Close_Date',\n",
        "            'Subdivision_Name', 'Property_Sub_Type', 'MLS_Num'\n",
        "        ]\n",
        "\n",
        "        # Check for missing columns and log the result\n",
        "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            error_msg = f\"Missing required columns: {', '.join(missing_cols)}\"\n",
        "            logger.log_error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        # Create working copy and correction log\n",
        "        df_work = df.copy()\n",
        "        corrections_log = []\n",
        "        logger.log_info(\"Created working copy of DataFrame\")\n",
        "\n",
        "        # STEP 2: Initial analysis of invalid values\n",
        "        # -----------------------------------------\n",
        "        logger.log_info(\"\\nAnalyzing current price data...\")\n",
        "\n",
        "        def is_valid_price(price):\n",
        "            \"\"\"\n",
        "            Check if a price value is valid and in reasonable range.\n",
        "\n",
        "            Args:\n",
        "                price: The price value to check\n",
        "\n",
        "            Returns:\n",
        "                bool: True if price is valid, False otherwise\n",
        "            \"\"\"\n",
        "            try:\n",
        "                # Convert to string and clean\n",
        "                price_str = str(price).strip()\n",
        "                clean_str = (price_str\n",
        "                           .replace('$', '')\n",
        "                           .replace(',', '')\n",
        "                           .replace(' ', ''))\n",
        "\n",
        "                price_float = float(clean_str)\n",
        "                return 1000 <= price_float <= 100_000_000\n",
        "\n",
        "            except (ValueError, TypeError):\n",
        "                return False\n",
        "\n",
        "        # Analyze each price column\n",
        "        invalid_examples = {}\n",
        "        for col in ['Original_List_Price', 'Close_Price']:\n",
        "            logger.log_info(f\"\\nAnalyzing {col}:\")\n",
        "            logger.log_info(\"-\" * 50)\n",
        "\n",
        "            # Find invalid values\n",
        "            invalid_mask = ~df_work[col].apply(is_valid_price)\n",
        "            invalid_count = invalid_mask.sum()\n",
        "\n",
        "            if invalid_count > 0:\n",
        "                invalid_rows = df_work[invalid_mask].copy()\n",
        "                invalid_examples[col] = invalid_rows\n",
        "\n",
        "                logger.log_info(f\"Found {invalid_count:,} invalid values\")\n",
        "\n",
        "                # Show examples of invalid data\n",
        "                logger.log_info(\"\\nExamples of invalid values:\")\n",
        "                display_cols = [col, 'MLS_Num', 'Property_Sub_Type', 'Close_Date']\n",
        "                display_df = invalid_rows[display_cols].head(10)\n",
        "\n",
        "                if 'Close_Date' in display_df.columns:\n",
        "                    display_df['Close_Date'] = pd.to_datetime(\n",
        "                        display_df['Close_Date']\n",
        "                    ).dt.strftime('%Y-%m-%d')\n",
        "\n",
        "                # Convert DataFrame to string and log each line\n",
        "                df_string = display_df.to_string(index=False)\n",
        "                for line in df_string.split('\\n'):\n",
        "                    logger.log_info(line)\n",
        "\n",
        "                # Analyze types of invalid values\n",
        "                null_count = invalid_rows[col].isnull().sum()\n",
        "                logger.log_info(f\"\\nNull values: {null_count:,}\")\n",
        "\n",
        "                non_null_invalid = invalid_rows[invalid_rows[col].notna()][col]\n",
        "                if len(non_null_invalid) > 0:\n",
        "                    logger.log_info(\"\\nSample of non-null invalid values:\")\n",
        "                    sample_string = non_null_invalid.head().to_string()\n",
        "                    for line in sample_string.split('\\n'):\n",
        "                        logger.log_info(line)\n",
        "\n",
        "        # STEP 3: Clean and prepare price data\n",
        "        # -----------------------------------\n",
        "        logger.log_info(\"\\nCleaning price data...\")\n",
        "\n",
        "        def clean_price(price):\n",
        "            \"\"\"\n",
        "            Convert price to numeric value, handling various formats.\n",
        "\n",
        "            Args:\n",
        "                price: The price value to clean (can be string, float, or other format)\n",
        "\n",
        "            Returns:\n",
        "                float: Cleaned price value, or np.nan if invalid\n",
        "            \"\"\"\n",
        "            if pd.isna(price):\n",
        "                return np.nan\n",
        "\n",
        "            try:\n",
        "                # Convert to string and clean\n",
        "                price_str = str(price).strip()\n",
        "                clean_str = (price_str\n",
        "                           .replace('$', '')\n",
        "                           .replace(',', '')\n",
        "                           .replace(' ', ''))\n",
        "\n",
        "                price_float = float(clean_str)\n",
        "\n",
        "                # Check for reasonable range\n",
        "                if 1000 <= price_float <= 100_000_000:\n",
        "                    return price_float\n",
        "                return np.nan\n",
        "\n",
        "            except (ValueError, TypeError):\n",
        "                return np.nan\n",
        "\n",
        "        # Clean both price columns\n",
        "        for col in ['Original_List_Price', 'Close_Price']:\n",
        "            df_work[col] = df_work[col].apply(clean_price)\n",
        "            logger.log_info(f\"Cleaned {col} - converted to numeric values\")\n",
        "\n",
        "        # STEP 4: Calculate market statistics\n",
        "        # ----------------------------------\n",
        "        logger.log_info(\"\\nCalculating market statistics...\")\n",
        "\n",
        "        # Convert dates and create year field\n",
        "        df_work['Close_Date'] = pd.to_datetime(df_work['Close_Date'])\n",
        "        df_work['Year'] = df_work['Close_Date'].dt.year\n",
        "        logger.log_info(\"Converted dates and extracted year information\")\n",
        "\n",
        "        def calculate_market_stats(group):\n",
        "            \"\"\"\n",
        "            Calculate robust statistics for property groups.\n",
        "\n",
        "            This function computes median and quartile values for each price type\n",
        "            within a group of similar properties.\n",
        "\n",
        "            Args:\n",
        "                group: DataFrame group containing properties with same location/type/year\n",
        "\n",
        "            Returns:\n",
        "                pandas.Series: Statistical measures, or None if insufficient data\n",
        "            \"\"\"\n",
        "            if len(group) < 5:  # Need minimum sample size\n",
        "                return None\n",
        "\n",
        "            stats = {}\n",
        "            for price_type in ['Original_List_Price', 'Close_Price']:\n",
        "                # Get valid prices (non-null and positive)\n",
        "                valid_prices = group[price_type].dropna()\n",
        "                valid_prices = valid_prices[valid_prices > 0]\n",
        "\n",
        "                if len(valid_prices) < 5:  # Need minimum sample size\n",
        "                    continue\n",
        "\n",
        "                stats.update({\n",
        "                    f'{price_type}_median': valid_prices.median(),\n",
        "                    f'{price_type}_q1': valid_prices.quantile(0.25),\n",
        "                    f'{price_type}_q3': valid_prices.quantile(0.75)\n",
        "                })\n",
        "\n",
        "            return pd.Series(stats) if len(stats) >= 6 else None\n",
        "\n",
        "        # Calculate statistics by location, property type, and year\n",
        "        logger.log_info(\"Calculating market statistics by location and property type...\")\n",
        "        market_stats = df_work.groupby(\n",
        "            ['Subdivision_Name', 'Property_Sub_Type', 'Year'],\n",
        "            group_keys=False\n",
        "        ).apply(calculate_market_stats)\n",
        "\n",
        "        # Log summary of market statistics\n",
        "        valid_groups = market_stats.notna().any(axis=1).sum()\n",
        "        total_groups = len(df_work.groupby(['Subdivision_Name', 'Property_Sub_Type', 'Year']))\n",
        "        logger.log_info(f\"Computed statistics for {valid_groups:,} out of {total_groups:,} property groups\")\n",
        "\n",
        "        # STEP 5: Identify and fix price errors\n",
        "        # ------------------------------------\n",
        "        logger.log_info(\"\\nAnalyzing potential price errors...\")\n",
        "\n",
        "        def check_for_price_error(row, stats):\n",
        "            \"\"\"\n",
        "            Check if prices appear to have data entry errors.\n",
        "\n",
        "            Args:\n",
        "                row: Single property row from DataFrame\n",
        "                stats: Market statistics for comparison\n",
        "\n",
        "            Returns:\n",
        "                dict: Correction details if error found, None otherwise\n",
        "            \"\"\"\n",
        "            try:\n",
        "                group_stats = stats.loc[\n",
        "                    (row['Subdivision_Name'],\n",
        "                     row['Property_Sub_Type'],\n",
        "                     row['Year'])\n",
        "                ]\n",
        "\n",
        "                for price_type in ['Original_List_Price', 'Close_Price']:\n",
        "                    price = row[price_type]\n",
        "                    median = group_stats[f'{price_type}_median']\n",
        "\n",
        "                    if pd.isna(price) or pd.isna(median) or price == 0 or median == 0:\n",
        "                        continue\n",
        "\n",
        "                    ratio = price / median\n",
        "\n",
        "                    # Look for obvious magnitude errors\n",
        "                    if ratio > 100 or ratio < 0.01:\n",
        "                        try:\n",
        "                            if ratio > 100:\n",
        "                                magnitude = min(int(np.floor(np.log10(ratio))), 6)\n",
        "                                corrected_price = price / (10 ** magnitude)\n",
        "                                correction_type = f\"Removed {magnitude} zeros\"\n",
        "                            else:\n",
        "                                magnitude = min(int(np.floor(np.log10(1/ratio))), 6)\n",
        "                                corrected_price = price * (10 ** magnitude)\n",
        "                                correction_type = f\"Added {magnitude} zeros\"\n",
        "\n",
        "                            # Verify correction is reasonable\n",
        "                            if 1000 <= corrected_price <= 100_000_000:\n",
        "                                return {\n",
        "                                    'price_type': price_type,\n",
        "                                    'original_price': price,\n",
        "                                    'corrected_price': corrected_price,\n",
        "                                    'correction_type': correction_type,\n",
        "                                    'market_median': median,\n",
        "                                    'original_ratio': ratio\n",
        "                                }\n",
        "                        except (OverflowError, ValueError):\n",
        "                            continue\n",
        "\n",
        "                return None\n",
        "\n",
        "            except (KeyError, AttributeError):\n",
        "                return None\n",
        "\n",
        "        # Process each property\n",
        "        logger.log_info(\"Checking individual properties...\")\n",
        "        total_processed = 0\n",
        "        total_corrected = 0\n",
        "\n",
        "        for idx, row in tqdm(df_work.iterrows(), total=len(df_work)):\n",
        "            total_processed += 1\n",
        "            correction = check_for_price_error(row, market_stats)\n",
        "\n",
        "            if correction:\n",
        "                total_corrected += 1\n",
        "\n",
        "                # Create detailed log entry\n",
        "                log_entry = {\n",
        "                    'MLS_Num': row['MLS_Num'],\n",
        "                    'Property_Type': row['Property_Sub_Type'],\n",
        "                    'Subdivision': row['Subdivision_Name'],\n",
        "                    'Close_Date': row['Close_Date'],\n",
        "                    'Price_Type': correction['price_type'],\n",
        "                    'Original_Price': correction['original_price'],\n",
        "                    'Corrected_Price': correction['corrected_price'],\n",
        "                    'Market_Median': correction['market_median'],\n",
        "                    'Original_Ratio': correction['original_ratio'],\n",
        "                    'Correction_Type': correction['correction_type']\n",
        "                }\n",
        "\n",
        "                # Apply correction\n",
        "                df_work.loc[idx, correction['price_type']] = correction['corrected_price']\n",
        "                corrections_log.append(log_entry)\n",
        "\n",
        "        # STEP 6: Generate comprehensive report\n",
        "        # -----------------------------------\n",
        "        corrections_df = pd.DataFrame(corrections_log) if corrections_log else pd.DataFrame()\n",
        "\n",
        "        if not corrections_df.empty:\n",
        "            logger.log_info(\"\\nPrice Correction Summary:\")\n",
        "            logger.log_info(\"-\" * 60)\n",
        "            logger.log_info(f\"Total properties processed: {total_processed:,}\")\n",
        "            logger.log_info(f\"Total corrections made: {total_corrected:,}\")\n",
        "\n",
        "            # Group corrections by type\n",
        "            correction_types = corrections_df['Correction_Type'].value_counts()\n",
        "            logger.log_info(\"\\nTypes of corrections made:\")\n",
        "            for correction_type, count in correction_types.items():\n",
        "                logger.log_info(f\"{correction_type}: {count:,}\")\n",
        "\n",
        "            # Show examples of corrections\n",
        "            logger.log_info(\"\\nExample corrections (first 10):\")\n",
        "            display_df = corrections_df.head(10).copy()\n",
        "\n",
        "            # Format prices for display\n",
        "            price_cols = ['Original_Price', 'Corrected_Price', 'Market_Median']\n",
        "            for col in price_cols:\n",
        "                display_df[col] = display_df[col].map('${:,.2f}'.format)\n",
        "\n",
        "            # Format ratio\n",
        "            display_df['Original_Ratio'] = display_df['Original_Ratio'].map('{:.2f}x'.format)\n",
        "\n",
        "            # Format date\n",
        "            display_df['Close_Date'] = pd.to_datetime(\n",
        "                display_df['Close_Date']\n",
        "            ).dt.strftime('%Y-%m-%d')\n",
        "\n",
        "            # Convert DataFrame to string and log each line\n",
        "            df_string = display_df.to_string(index=False)\n",
        "            for line in df_string.split('\\n'):\n",
        "                logger.log_info(line)\n",
        "\n",
        "            # Save corrections log\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            log_filename = f'price_corrections_{timestamp}.csv'\n",
        "            corrections_df.to_csv(log_filename, index=False)\n",
        "            logger.log_info(f\"\\nDetailed correction log saved to: {log_filename}\")\n",
        "\n",
        "        else:\n",
        "            logger.log_info(\"\\nNo price corrections needed! ✓\")\n",
        "\n",
        "        return df_work, corrections_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"\\nError in price analysis and correction:\")\n",
        "        logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "        logger.log_error(f\"Details: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the analysis and correction process\n",
        "try:\n",
        "    logger.log_info(\"Starting comprehensive price analysis...\")\n",
        "\n",
        "    # Create backup of price columns\n",
        "    price_backup = combined_data[['Original_List_Price', 'Close_Price']].copy()\n",
        "    logger.log_info(\"Created backup of price columns\")\n",
        "\n",
        "    # Run the analysis and correction process\n",
        "    corrected_data, corrections = analyze_and_fix_prices(combined_data)\n",
        "\n",
        "    # Update main DataFrame if corrections were made\n",
        "    if not corrections.empty:\n",
        "        combined_data = corrected_data\n",
        "        logger.log_info(\"\\nMain dataset updated with price corrections! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"Price error analysis and correction\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during price correction process:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")\n",
        "\n",
        "    # Restore original prices if there was an error\n",
        "    combined_data[['Original_List_Price', 'Close_Price']] = price_backup\n",
        "    logger.log_info(\"\\nRestored original prices due to error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23-LV4K2ana3"
      },
      "source": [
        "# Data Cleaning: Suite Status Standardization\n",
        "\n",
        "## Overview\n",
        "This code block cleans and standardizes the suite status information in our real estate dataset. It converts various text descriptions into three consistent categories, making it easier to analyze secondary suite prevalence and legal status across properties.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input\n",
        "* Raw Suite Status Column\n",
        "  * Contains mixed formats and terminology\n",
        "  * May include null values\n",
        "  * Various text descriptions for same status\n",
        "  * May have inconsistent capitalization\n",
        "\n",
        "### Standardization Categories\n",
        "* Three Standard Values:\n",
        "  * \"No Suite\": Properties without secondary suites\n",
        "  * \"Suite - Illegal\": Properties with unauthorized suites\n",
        "  * \"Suite - Legal\": Properties with permitted suites\n",
        "* Null values preserved (indicating unknown status)\n",
        "\n",
        "### Processing Steps\n",
        "1. Distribution Analysis\n",
        "   * Counts total properties\n",
        "   * Identifies missing values\n",
        "   * Shows initial value distribution\n",
        "   * Calculates data completeness\n",
        "\n",
        "2. Value Standardization\n",
        "   * Handles case sensitivity\n",
        "   * Removes extra whitespace\n",
        "   * Maps values to standard categories\n",
        "   * Preserves null values\n",
        "   * Handles ambiguous cases\n",
        "\n",
        "3. Change Tracking\n",
        "   * Compares original to new values\n",
        "   * Shows example transformations\n",
        "   * Counts modified records\n",
        "   * Reports modification percentage\n",
        "\n",
        "### Quality Checks\n",
        "The code verifies and reports:\n",
        "* Total rows processed\n",
        "* Suite data completeness\n",
        "* Value distribution before/after\n",
        "* Number of changes made\n",
        "* Examples of specific changes\n",
        "\n",
        "## Process Protection\n",
        "* Creates data backup\n",
        "* Works on copy of data\n",
        "* Handles errors gracefully\n",
        "* Restores original data if needed\n",
        "* Provides detailed logging\n",
        "\n",
        "[Code block follows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aXzAV4pNSf09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb99c05-bd83-4ef7-8b59-861355ec3ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting suite data cleaning process...\n",
            "Created backup of suite data\n",
            "Starting suite data standardization...\n",
            "Created working copy of DataFrame\n",
            "\n",
            "Original Suite Distribution:\n",
            "\n",
            "Suite Data Analysis:\n",
            "--------------------------------------------------\n",
            "Total rows: 393,664\n",
            "Rows with suite data: 237,344\n",
            "Percentage with data: 60.29%\n",
            "\n",
            "Current Value Distribution:\n",
            "--------------------------------------------------\n",
            "NULL                  156,320 ( 39.71%)\n",
            "No                    133,232 ( 33.84%)\n",
            "Suite - None           85,374 ( 21.69%)\n",
            "Suite - Illegal        15,269 (  3.88%)\n",
            "Suite - Legal           3,427 (  0.87%)\n",
            "Yes                        40 (  0.01%)\n",
            "Suite - Illegal, Suite - Legal        2 (  0.00%)\n",
            "\n",
            "Applying standardization...\n",
            "\n",
            "Standardized Suite Distribution:\n",
            "\n",
            "Suite Data Analysis:\n",
            "--------------------------------------------------\n",
            "Total rows: 393,664\n",
            "Rows with suite data: 237,344\n",
            "Percentage with data: 60.29%\n",
            "\n",
            "Current Value Distribution:\n",
            "--------------------------------------------------\n",
            "No Suite              218,608 ( 55.53%)\n",
            "NULL                  156,320 ( 39.71%)\n",
            "Suite - Illegal        15,309 (  3.89%)\n",
            "Suite - Legal           3,427 (  0.87%)\n",
            "\n",
            "Example Changes (first 10):\n",
            "--------------------------------------------------\n",
            "NULL                 -> NULL\n",
            "NULL                 -> NULL\n",
            "NULL                 -> NULL\n",
            "No                   -> No Suite\n",
            "No                   -> No Suite\n",
            "No                   -> No Suite\n",
            "No                   -> No Suite\n",
            "No                   -> No Suite\n",
            "No                   -> No Suite\n",
            "No                   -> No Suite\n",
            "\n",
            "Standardization Summary:\n",
            "--------------------------------------------------\n",
            "Total rows processed: 393,664\n",
            "Values changed: 374,968\n",
            "Percentage changed: 95.25%\n",
            "\n",
            "Process completed successfully! 🎉\n",
            "Step 11 completed: Suite status cleaning and standardization\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# SUITE STATUS CLEANING AND STANDARDIZATION\n",
        "# =================================================================================\n",
        "\n",
        "def analyze_suite_distribution(df, column='Suite', show_details=True):\n",
        "    \"\"\"\n",
        "    Analyze the distribution of values in the suite column.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing the suite data\n",
        "        column: Name of the suite column (default: 'Suite')\n",
        "        show_details: Whether to print detailed analysis (default: True)\n",
        "\n",
        "    Returns:\n",
        "        dict: Summary statistics about the suite data\n",
        "    \"\"\"\n",
        "    # Calculate basic statistics about the suite data\n",
        "    total_rows = len(df)\n",
        "    rows_with_data = df[column].notna().sum()\n",
        "    percentage_with_data = (rows_with_data / total_rows) * 100\n",
        "\n",
        "    if show_details:\n",
        "        # Log the initial analysis results\n",
        "        logger.log_info(f\"\\nSuite Data Analysis:\")\n",
        "        logger.log_info(\"-\" * 50)\n",
        "        logger.log_info(f\"Total rows: {total_rows:,}\")\n",
        "        logger.log_info(f\"Rows with suite data: {rows_with_data:,}\")\n",
        "        logger.log_info(f\"Percentage with data: {percentage_with_data:.2f}%\")\n",
        "\n",
        "        logger.log_info(\"\\nCurrent Value Distribution:\")\n",
        "        logger.log_info(\"-\" * 50)\n",
        "\n",
        "        # Calculate and display the distribution of values\n",
        "        value_counts = df[column].value_counts(dropna=False)\n",
        "\n",
        "        # Format and log each row of the distribution table\n",
        "        for value, count in value_counts.items():\n",
        "            value_str = 'NULL' if pd.isna(value) else value\n",
        "            percentage = (count / total_rows) * 100\n",
        "            logger.log_info(f\"{value_str:<20} {count:>8,} ({percentage:>6.2f}%)\")\n",
        "\n",
        "    return {\n",
        "        'total_rows': total_rows,\n",
        "        'rows_with_data': rows_with_data,\n",
        "        'percentage_with_data': percentage_with_data\n",
        "    }\n",
        "\n",
        "def clean_suite(value):\n",
        "    \"\"\"\n",
        "    Clean and standardize suite values to three possible categories:\n",
        "    - \"No Suite\"\n",
        "    - \"Suite - Illegal\"\n",
        "    - \"Suite - Legal\"\n",
        "\n",
        "    Args:\n",
        "        value: Input value to clean\n",
        "\n",
        "    Returns:\n",
        "        str or np.nan: Cleaned suite value\n",
        "    \"\"\"\n",
        "    # Handle null values\n",
        "    if pd.isna(value):\n",
        "        return value\n",
        "\n",
        "    # Convert to string and clean whitespace\n",
        "    value = str(value).strip().lower()\n",
        "\n",
        "    # Define our mapping of values\n",
        "    suite_mapping = {\n",
        "        'no': 'No Suite',\n",
        "        'suite - none': 'No Suite',\n",
        "        'suite - illegal, suite - legal': 'No Suite',  # May need investigation\n",
        "        'suite - illegal': 'Suite - Illegal',\n",
        "        'yes': 'Suite - Illegal',\n",
        "        'suite - legal': 'Suite - Legal'\n",
        "    }\n",
        "\n",
        "    # Return mapped value or default to \"No Suite\"\n",
        "    return suite_mapping.get(value, 'No Suite')\n",
        "\n",
        "def standardize_suite_data(df, column='Suite'):\n",
        "    \"\"\"\n",
        "    Standardize the suite information in the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing the suite data\n",
        "        column: Name of the suite column (default: 'Suite')\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with standardized suite data\n",
        "        dict: Summary of changes made\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Starting suite data standardization...\")\n",
        "\n",
        "    try:\n",
        "        # Create a copy of the DataFrame\n",
        "        df_clean = df.copy()\n",
        "        logger.log_info(\"Created working copy of DataFrame\")\n",
        "\n",
        "        # Store original values for comparison\n",
        "        original_values = df_clean[column].copy()\n",
        "\n",
        "        # Analyze original distribution\n",
        "        logger.log_info(\"\\nOriginal Suite Distribution:\")\n",
        "        original_stats = analyze_suite_distribution(df_clean, column)\n",
        "\n",
        "        # Apply cleaning function\n",
        "        logger.log_info(\"\\nApplying standardization...\")\n",
        "        df_clean[column] = df_clean[column].apply(clean_suite)\n",
        "\n",
        "        # Analyze new distribution\n",
        "        logger.log_info(\"\\nStandardized Suite Distribution:\")\n",
        "        new_stats = analyze_suite_distribution(df_clean, column)\n",
        "\n",
        "        # Calculate changes\n",
        "        changes = (df_clean[column] != original_values).sum()\n",
        "\n",
        "        # Create summary of changes\n",
        "        summary = {\n",
        "            'total_rows': len(df_clean),\n",
        "            'rows_changed': changes,\n",
        "            'percentage_changed': (changes / len(df_clean)) * 100\n",
        "        }\n",
        "\n",
        "        # Show example changes if any were made\n",
        "        if changes > 0:\n",
        "            logger.log_info(\"\\nExample Changes (first 10):\")\n",
        "            logger.log_info(\"-\" * 50)\n",
        "            changed_mask = (df_clean[column] != original_values)\n",
        "            changes_df = pd.DataFrame({\n",
        "                'Original Value': original_values[changed_mask],\n",
        "                'New Value': df_clean.loc[changed_mask, column]\n",
        "            })\n",
        "\n",
        "            # Log each change example\n",
        "            for _, row in changes_df.head(10).iterrows():\n",
        "                orig = 'NULL' if pd.isna(row['Original Value']) else row['Original Value']\n",
        "                new = 'NULL' if pd.isna(row['New Value']) else row['New Value']\n",
        "                logger.log_info(f\"{orig:<20} -> {new}\")\n",
        "\n",
        "        # Log summary statistics\n",
        "        logger.log_info(f\"\\nStandardization Summary:\")\n",
        "        logger.log_info(\"-\" * 50)\n",
        "        logger.log_info(f\"Total rows processed: {summary['total_rows']:,}\")\n",
        "        logger.log_info(f\"Values changed: {summary['rows_changed']:,}\")\n",
        "        logger.log_info(f\"Percentage changed: {summary['percentage_changed']:.2f}%\")\n",
        "\n",
        "        return df_clean, summary\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in standardize_suite_data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the standardization\n",
        "try:\n",
        "    logger.log_info(\"Starting suite data cleaning process...\")\n",
        "\n",
        "    # Create backup of original data\n",
        "    suite_backup = combined_data['Suite'].copy()\n",
        "    logger.log_info(\"Created backup of suite data\")\n",
        "\n",
        "    # Perform the standardization\n",
        "    combined_data, changes = standardize_suite_data(combined_data)\n",
        "\n",
        "    logger.log_info(\"\\nProcess completed successfully! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"Suite status cleaning and standardization\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during suite standardization:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")\n",
        "\n",
        "    # Restore original data if there was an error\n",
        "    combined_data['Suite'] = suite_backup\n",
        "    logger.log_info(\"\\nRestored original suite values due to error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB8YdTauc3U3"
      },
      "source": [
        "# Data Feature Engineering: Basement Analysis and Suite Cross-Reference\n",
        "\n",
        "## Overview\n",
        "This code block analyzes basement descriptions to create standardized feature columns and verifies consistency between basement descriptions and suite information. It extracts key characteristics from text descriptions while preserving the original data.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input Column\n",
        "* Primary Source: 'Basement' column\n",
        "  * Contains text descriptions of basement features\n",
        "  * May include multiple characteristics in one field\n",
        "  * Potential null values\n",
        "  * Varied text formats and terminology\n",
        "\n",
        "### New Feature Columns Created\n",
        "1. Basement_Finish\n",
        "   * Categories:\n",
        "     * 'fully finished'\n",
        "     * 'partially finished'\n",
        "     * 'unfinished'\n",
        "   * Default: 'unfinished' for null values\n",
        "   * Looks for keywords like 'full', 'developed', 'partial'\n",
        "\n",
        "2. Is_Walkout\n",
        "   * Categories:\n",
        "     * 'yes'\n",
        "     * 'no'\n",
        "   * Default: 'no' for null values\n",
        "   * Identifies walk-out basements using keywords\n",
        "\n",
        "3. Suite_Separate_Entry\n",
        "   * Categories:\n",
        "     * 'yes'\n",
        "     * 'no'\n",
        "   * Default: 'no' for null values\n",
        "   * Checks for specific term 'Separate/Exterior Entry'\n",
        "\n",
        "### Cross-Reference System\n",
        "* Compares basement descriptions with Suite column\n",
        "* Updates suite information when:\n",
        "  * Basement mentions suite but Suite column is null\n",
        "  * Basement mentions suite but Suite column says \"No Suite\"\n",
        "* Tracks mismatches and updates\n",
        "* Documents potential data inconsistencies\n",
        "\n",
        "### Quality Checks\n",
        "The code reports:\n",
        "* Distribution of basement finish types\n",
        "* Walkout basement frequency\n",
        "* Separate entry statistics\n",
        "* Suite information updates\n",
        "* Potential data mismatches\n",
        "\n",
        "## Process Protection\n",
        "* Creates data backup\n",
        "* Works on copy of DataFrame\n",
        "* Handles null values appropriately\n",
        "* Includes error recovery\n",
        "* Restores original data if errors occur\n",
        "\n",
        "## Code Organization\n",
        "1. Feature Creation Functions:\n",
        "   * determine_finish_status()\n",
        "   * determine_walkout_status()\n",
        "   * determine_separate_entry()\n",
        "\n",
        "2. Data Validation Steps:\n",
        "   * Cross-references suite information\n",
        "   * Counts mismatches\n",
        "   * Tracks updates made\n",
        "   * Reports discrepancies\n",
        "\n",
        "3. Results Reporting:\n",
        "   * Shows distribution for each new feature\n",
        "   * Reports cross-reference findings\n",
        "   * Provides update statistics\n",
        "   * Documents data quality issues\n",
        "\n",
        "[Code block follows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jK-pG0Chc1wW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd418c9f-e2e5-46bf-ff84-eec311c42ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting basement and suite analysis...\n",
            "======================================================================\n",
            "Created backup of basement and suite data\n",
            "Starting basement feature categorization...\n",
            "Created working copy of DataFrame\n",
            "\n",
            "Categorizing basement finish status...\n",
            "Created Basement_Finish categorization\n",
            "Categorizing walkout status...\n",
            "Created Is_Walkout categorization\n",
            "Categorizing separate entry status...\n",
            "Created Suite_Separate_Entry categorization\n",
            "\n",
            "Cross-referencing suite information...\n",
            "\n",
            "Analysis Results:\n",
            "============================================================\n",
            "\n",
            "Basement Finish Distribution:\n",
            "fully finished      :  268,771 ( 68.27%)\n",
            "unfinished          :  113,890 ( 28.93%)\n",
            "partially finished  :   11,003 (  2.80%)\n",
            "\n",
            "Walkout Status Distribution:\n",
            "no                  :  357,696 ( 90.86%)\n",
            "yes                 :   35,968 (  9.14%)\n",
            "\n",
            "Separate Entry Distribution:\n",
            "no                  :  384,082 ( 97.57%)\n",
            "yes                 :    9,582 (  2.43%)\n",
            "\n",
            "Suite Cross-Reference Results:\n",
            "Basement descriptions with 'suite': 17,407\n",
            "Suite information updated: 5,146\n",
            "Potential mismatches found: 0\n",
            "\n",
            "Analysis completed successfully! 🎉\n",
            "Step 12 completed: Basement and suite analysis\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# BASEMENT AND SUITE ANALYSIS\n",
        "# =================================================================================\n",
        "\n",
        "def categorize_basement_features(df):\n",
        "    \"\"\"\n",
        "    Create standardized columns for basement features and cross-reference with suite data.\n",
        "    Does not modify original columns, only adds new ones with standardized categories.\n",
        "\n",
        "    New columns created:\n",
        "    - Basement_Finish: fully finished, partially finished, or unfinished\n",
        "    - Is_Walkout: yes or no\n",
        "    - Suite_Separate_Entry: yes or no\n",
        "\n",
        "    Also verifies suite information between Basement and Suite columns.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame containing real estate data\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with new categorization columns\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting basement feature categorization...\")\n",
        "\n",
        "        # Create a working copy of the DataFrame\n",
        "        df_work = df.copy()\n",
        "        logger.log_info(\"Created working copy of DataFrame\")\n",
        "\n",
        "        # STEP 1: Create Basement_Finish categorization\n",
        "        # --------------------------------------------\n",
        "        logger.log_info(\"\\nCategorizing basement finish status...\")\n",
        "\n",
        "        def determine_finish_status(value):\n",
        "            \"\"\"\n",
        "            Helper function to determine basement finish status.\n",
        "            Converts various basement descriptions to standardized finish categories.\n",
        "            \"\"\"\n",
        "            if pd.isna(value):\n",
        "                return 'unfinished'  # Default to unfinished if no data\n",
        "\n",
        "            value = str(value).lower()\n",
        "\n",
        "            # Check for fully finished indicators\n",
        "            if any(term in value for term in ['full', 'fully finished', 'developed']):\n",
        "                return 'fully finished'\n",
        "\n",
        "            # Check for partially finished indicators\n",
        "            elif any(term in value for term in ['partial', 'part finished']):\n",
        "                return 'partially finished'\n",
        "\n",
        "            # Everything else is considered unfinished\n",
        "            else:\n",
        "                return 'unfinished'\n",
        "\n",
        "        # Apply finish categorization\n",
        "        df_work['Basement_Finish'] = df_work['Basement'].apply(determine_finish_status)\n",
        "        logger.log_info(\"Created Basement_Finish categorization\")\n",
        "\n",
        "        # STEP 2: Create Is_Walkout categorization\n",
        "        # ---------------------------------------\n",
        "        logger.log_info(\"Categorizing walkout status...\")\n",
        "\n",
        "        def determine_walkout_status(value):\n",
        "            \"\"\"\n",
        "            Helper function to determine walkout status.\n",
        "            Checks for walkout basement indicators.\n",
        "            \"\"\"\n",
        "            if pd.isna(value):\n",
        "                return 'no'\n",
        "\n",
        "            value = str(value).lower()\n",
        "            return 'yes' if any(term in value for term in ['walk-out', 'walkout']) else 'no'\n",
        "\n",
        "        # Apply walkout categorization\n",
        "        df_work['Is_Walkout'] = df_work['Basement'].apply(determine_walkout_status)\n",
        "        logger.log_info(\"Created Is_Walkout categorization\")\n",
        "\n",
        "        # STEP 3: Create Suite_Separate_Entry categorization\n",
        "        # -----------------------------------------------\n",
        "        logger.log_info(\"Categorizing separate entry status...\")\n",
        "\n",
        "        def determine_separate_entry(value):\n",
        "            \"\"\"\n",
        "            Helper function to determine separate entry status.\n",
        "            Looks specifically for the dropdown menu term 'Separate/Exterior Entry'.\n",
        "            \"\"\"\n",
        "            if pd.isna(value):\n",
        "                return 'no'\n",
        "\n",
        "            # Look for exact match of dropdown term\n",
        "            return 'yes' if 'Separate/Exterior Entry' in str(value) else 'no'\n",
        "\n",
        "        # Apply separate entry categorization\n",
        "        df_work['Suite_Separate_Entry'] = df_work['Basement'].apply(determine_separate_entry)\n",
        "        logger.log_info(\"Created Suite_Separate_Entry categorization\")\n",
        "\n",
        "        # STEP 4: Cross-reference suite information\n",
        "        # ---------------------------------------\n",
        "        logger.log_info(\"\\nCross-referencing suite information...\")\n",
        "\n",
        "        # Create a mask for basement descriptions containing 'suite'\n",
        "        basement_suite_mask = df_work['Basement'].str.contains('suite',\n",
        "                                                             case=False,\n",
        "                                                             na=False)\n",
        "\n",
        "        # Count mismatches between Basement and Suite columns\n",
        "        mismatches = 0\n",
        "        updates = 0\n",
        "\n",
        "        for idx in df_work[basement_suite_mask].index:\n",
        "            # Check if this row has a suite in basement but not in Suite column\n",
        "            if pd.isna(df_work.at[idx, 'Suite']):\n",
        "                # Default to illegal suite if not specified\n",
        "                df_work.at[idx, 'Suite'] = 'Suite - Illegal'\n",
        "                updates += 1\n",
        "            elif df_work.at[idx, 'Suite'] == 'No Suite':\n",
        "                # Update to illegal suite if marked as No Suite\n",
        "                df_work.at[idx, 'Suite'] = 'Suite - Illegal'\n",
        "                updates += 1\n",
        "\n",
        "            # Count as mismatch if basement shows suite but Suite column says No Suite\n",
        "            if df_work.at[idx, 'Suite'] == 'No Suite':\n",
        "                mismatches += 1\n",
        "\n",
        "        # STEP 5: Log Analysis Results\n",
        "        # ----------------------------\n",
        "        logger.log_info(\"\\nAnalysis Results:\")\n",
        "        logger.log_info(\"=\" * 60)\n",
        "\n",
        "        # Show Basement_Finish distribution\n",
        "        logger.log_info(\"\\nBasement Finish Distribution:\")\n",
        "        finish_counts = df_work['Basement_Finish'].value_counts()\n",
        "        for status, count in finish_counts.items():\n",
        "            percentage = (count / len(df_work)) * 100\n",
        "            logger.log_info(f\"{status:<20}: {count:>8,} ({percentage:>6.2f}%)\")\n",
        "\n",
        "        # Show Is_Walkout distribution\n",
        "        logger.log_info(\"\\nWalkout Status Distribution:\")\n",
        "        walkout_counts = df_work['Is_Walkout'].value_counts()\n",
        "        for status, count in walkout_counts.items():\n",
        "            percentage = (count / len(df_work)) * 100\n",
        "            logger.log_info(f\"{status:<20}: {count:>8,} ({percentage:>6.2f}%)\")\n",
        "\n",
        "        # Show Suite_Separate_Entry distribution\n",
        "        logger.log_info(\"\\nSeparate Entry Distribution:\")\n",
        "        entry_counts = df_work['Suite_Separate_Entry'].value_counts()\n",
        "        for status, count in entry_counts.items():\n",
        "            percentage = (count / len(df_work)) * 100\n",
        "            logger.log_info(f\"{status:<20}: {count:>8,} ({percentage:>6.2f}%)\")\n",
        "\n",
        "        # Show suite cross-reference results\n",
        "        logger.log_info(\"\\nSuite Cross-Reference Results:\")\n",
        "        logger.log_info(f\"Basement descriptions with 'suite': {basement_suite_mask.sum():,}\")\n",
        "        logger.log_info(f\"Suite information updated: {updates:,}\")\n",
        "        logger.log_info(f\"Potential mismatches found: {mismatches:,}\")\n",
        "\n",
        "        return df_work\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"\\nError in categorize_basement_features:\")\n",
        "        logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "        logger.log_error(f\"Details: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the analysis\n",
        "try:\n",
        "    logger.log_info(\"Starting basement and suite analysis...\")\n",
        "    logger.log_info(\"=\" * 70)\n",
        "\n",
        "    # Create backup of relevant columns\n",
        "    basement_backup = combined_data['Basement'].copy()\n",
        "    suite_backup = combined_data['Suite'].copy() if 'Suite' in combined_data.columns else None\n",
        "    logger.log_info(\"Created backup of basement and suite data\")\n",
        "\n",
        "    # Run the categorization\n",
        "    combined_data = categorize_basement_features(combined_data)\n",
        "\n",
        "    logger.log_info(\"\\nAnalysis completed successfully! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"Basement and suite analysis\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during analysis:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")\n",
        "\n",
        "    # Restore original values if there was an error\n",
        "    combined_data['Basement'] = basement_backup\n",
        "    if suite_backup is not None:\n",
        "        combined_data['Suite'] = suite_backup\n",
        "    logger.log_info(\"\\nRestored original values due to error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WYgyhs-CC3o"
      },
      "source": [
        "# Data Cleaning: Parking Description Standardization\n",
        "\n",
        "## Overview\n",
        "This code block implements a sophisticated system to standardize parking descriptions in our real estate dataset. It uses predefined rules and term frequency analysis to convert varied text descriptions into consistent, meaningful categories while preserving important parking features.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input Data\n",
        "* Column: 'Parking'\n",
        "  * Contains text descriptions of parking features\n",
        "  * May include multiple features per description\n",
        "  * Contains modifiers and auxiliary information\n",
        "  * May have null values\n",
        "  * Varied terminology for same features\n",
        "\n",
        "### Standardization Rules\n",
        "1. Terms to Remove (Modifiers)\n",
        "   * Non-essential descriptors (e.g., 'oversized', 'insulated')\n",
        "   * Location details (e.g., 'alley access', 'garage faces front')\n",
        "   * Surface descriptions (e.g., 'concrete driveway', 'paved')\n",
        "   * General notes (e.g., 'see remarks', 'additional parking')\n",
        "\n",
        "2. Terms to Preserve\n",
        "   * Key features (e.g., 'plug-in', 'carport')\n",
        "   * Garage types (e.g., 'single garage', 'triple garage')\n",
        "   * Special features (e.g., 'electric vehicle charging station')\n",
        "   * Essential categorizations (e.g., 'no garage')\n",
        "\n",
        "3. Term Standardization\n",
        "   * Converts variations to standard terms\n",
        "   * Normalizes similar descriptions\n",
        "   * Handles compound descriptions\n",
        "   * Maintains consistency in terminology\n",
        "\n",
        "### Processing Steps\n",
        "1. Initial Analysis\n",
        "   * Analyzes 'none' values separately\n",
        "   * Counts value frequencies\n",
        "   * Identifies unique descriptions\n",
        "   * Reports data completeness\n",
        "\n",
        "2. Term Processing\n",
        "   * Splits descriptions into terms\n",
        "   * Standardizes each term\n",
        "   * Removes non-essential modifiers\n",
        "   * Preserves important features\n",
        "   * Handles frequency-based inclusion\n",
        "\n",
        "3. Creating Standardized Versions\n",
        "   * Maintains original data\n",
        "   * Creates standardized column\n",
        "   * Handles null values appropriately\n",
        "   * Provides complete version (no nulls)\n",
        "\n",
        "### Quality Controls\n",
        "The code verifies and reports:\n",
        "* Total properties processed\n",
        "* Data completeness statistics\n",
        "* Term frequencies\n",
        "* Standardization results\n",
        "* Final value distribution\n",
        "\n",
        "## Process Protection\n",
        "* Verifies column existence\n",
        "* Creates data backup\n",
        "* Handles errors gracefully\n",
        "* Restores original data if needed\n",
        "* Provides detailed logging\n",
        "\n",
        "## Output Details\n",
        "Two new columns created:\n",
        "1. standardized_parking\n",
        "   * Cleaned and standardized descriptions\n",
        "   * Preserves null values\n",
        "   * Removes non-essential terms\n",
        "   * Maintains important features\n",
        "\n",
        "2. standardized_parking_complete\n",
        "   * No null values\n",
        "   * Converts nulls to 'no parking'\n",
        "   * Complete coverage of dataset\n",
        "   * Ready for analysis\n",
        "\n",
        "[Code block follows]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KoFWvCZ0lrx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a017f6fd-c2c4-45b0-fba8-2ca84c942820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting comprehensive parking analysis...\n",
            "Created backup of parking data\n",
            "\n",
            "Analyzing parking descriptions containing 'none'...\n",
            "==================================================\n",
            "ANALYSIS OF 'NONE' PARKING DESCRIPTIONS\n",
            "==================================================\n",
            "Total descriptions containing 'none': 2,564\n",
            "\n",
            "Unique descriptions containing 'none':\n",
            "1. None, Off Street\n",
            "2. None, Off Street, Parking Pad\n",
            "3. None, On Street\n",
            "4. Alley Access, None, On Street\n",
            "5. None, Off Street, On Street\n",
            "6. Guest, Leased, None, Parkade, See Remarks, Stall\n",
            "7. None, See Remarks\n",
            "8. None, Parking Pad\n",
            "9. None, Unassigned\n",
            "10. None, Off Street, RV Access/Parking\n",
            "11. Front Drive, None, On Street\n",
            "12. None, Underground\n",
            "13. Leased, None\n",
            "14. None, On Street, Parking Pad\n",
            "15. Alley Access, None\n",
            "16. Gravel Driveway, None\n",
            "17. None, On Street, See Remarks\n",
            "18. None, On Street, Permit Required\n",
            "19. Alley Access, None, Other\n",
            "20. None, Outside\n",
            "21. None, On Street, Paved\n",
            "22. Guest, None, Off Street, On Street, Permit Required, See Remarks\n",
            "23. Front Drive, None\n",
            "24. None, On Street, Other, See Remarks\n",
            "25. Guest, None, Permit Required, See Remarks, Underground\n",
            "26. None, Off Street, Unpaved\n",
            "27. None, Other, Outside\n",
            "28. Additional Parking, None, Off Street, Rear Drive\n",
            "29. Common, Garage Door Opener, Heated Garage, None\n",
            "30. Alley Access, None, Off Street, On Street, Parking Lot, Side By Side, Unpaved\n",
            "31. Driveway, None\n",
            "32. Alley Access, None, Off Street, On Street\n",
            "33. None, Off Street, Shared Driveway\n",
            "34. Alley Access, None, Off Street, Outside\n",
            "35. Asphalt, Carport, None, Off Street, On Street, Outside, Parking Pad\n",
            "36. Guest, None\n",
            "37. Alley Access, None, Unassigned\n",
            "38. None, On Street, Parking Lot, Rear Drive\n",
            "39. Common, None, On Street, Outside\n",
            "40. None, Off Street, On Street, Other\n",
            "41. None, On Street, Permit Required, See Remarks, Unassigned\n",
            "42. None, Other\n",
            "43. Asphalt, Assigned, None, Parking Lot, Paved, Plug-In\n",
            "44. Assigned, None\n",
            "45. None, Parkade\n",
            "46. Asphalt, None\n",
            "47. Alley Access, None, See Remarks\n",
            "48. Alley Access, None, Off Street\n",
            "49. Guest, None, On Street\n",
            "50. None, Outside, Parking Lot\n",
            "51. None, Off Street, Stall\n",
            "52. Carport, None, Off Street\n",
            "53. None, Off Street, Other\n",
            "54. Additional Parking, None\n",
            "55. Guest, None, Other\n",
            "56. None, Other, Unassigned\n",
            "57. None, Off Street, See Remarks, Unassigned\n",
            "58. Guest, None, Parking Lot\n",
            "59. Concrete Driveway, None, RV Access/Parking\n",
            "60. None, Off Street, Parking Pad, RV Access/Parking\n",
            "61. None, Other, See Remarks, Unpaved\n",
            "62. None, Parking Pad, RV Access/Parking\n",
            "63. None, Secured, See Remarks\n",
            "64. Concrete Driveway, None\n",
            "65. Alley Access, None, Off Street, Stall\n",
            "66. None, On Street, Unassigned\n",
            "67. None, Tandem, Unpaved\n",
            "68. Assigned, None, Titled\n",
            "69. Assigned, None, Stall\n",
            "70. None, On Street, Unpaved\n",
            "71. None, RV Access/Parking\n",
            "72. Alley Access, None, On Street, Outside\n",
            "73. Assigned, Driveway, None\n",
            "74. Alley Access, Gravel Driveway, None, On Street\n",
            "75. Alley Access, Common, None\n",
            "76. Double Garage Detached, None, On Street\n",
            "77. Alley Access, None, On Street, RV Access/Parking\n",
            "78. None, Unpaved\n",
            "79. None, On Street, Other\n",
            "80. Driveway, None, Parking Pad\n",
            "81. None, Titled\n",
            "82. Driveway, None, Single Garage Attached\n",
            "83. Alley Access, Concrete Driveway, None, Off Street\n",
            "84. Gravel Driveway, None, See Remarks\n",
            "85. Carport, None\n",
            "86. None, Off Street, On Street, Secured, Unpaved\n",
            "87. Alley Access, None, On Street, See Remarks\n",
            "88. None, Outside, Stall\n",
            "89. None, Stall\n",
            "90. Alley Access, None, Parking Pad\n",
            "91. Covered, None\n",
            "92. Stall, Alley Access, None, On Street\n",
            "93. None, On Street, Outside, Permit Required, See Remarks\n",
            "94. None, Off Street, Single Garage Detached\n",
            "95. None, On Street, Outside\n",
            "96. None, Single Garage Detached\n",
            "97. Gravel Driveway, None, On Street\n",
            "98. Assigned, Insulated, Heated Garage, None, On Street, Other, See Remarks, Single Garage Attached\n",
            "99. None, Off Street, On Street, See Remarks\n",
            "100. Stall, None\n",
            "101. Alley Access, None, Outside, See Remarks\n",
            "102. Gravel Driveway, None, Off Street, Oversized, See Remarks\n",
            "103. Stall, None, On Street\n",
            "104. Alley Access, None, Unpaved\n",
            "105. Gravel Driveway, None, Off Street\n",
            "106. None, Workshop in Garage\n",
            "107. Additional Parking, Alley Access, Gravel Driveway, None, On Street, Parking Pad, See Remarks, Unpaved\n",
            "108. Additional Parking, Alley Access, Double Garage Detached, Garage Faces Rear, None, RV Access/Parking\n",
            "109. Driveway, Asphalt, None, On Street\n",
            "110. Double Garage Detached, None\n",
            "111. None, Parking Pad, Unpaved\n",
            "112. None, RV Access/Parking, RV Carport\n",
            "113. Stall, None, Off Street\n",
            "114. Triple Garage Attached, None\n",
            "115. Double Garage Attached, None\n",
            "116. None, Permit Required\n",
            "117. Concrete Driveway, None, On Street\n",
            "118. None, Parking Lot\n",
            "119. Stall, Assigned, None\n",
            "120. None, Off Street, Plug-In\n",
            "121. None, Off Street, Unassigned\n",
            "122. Driveway, None, Paved\n",
            "123. No Garage, None\n",
            "124. Driveway, Concrete Driveway, Garage Door Opener, None\n",
            "125. Detached Garage, Double Garage, Heated Garage, None, RV Access/Parking\n",
            "126. Detached Garage, None\n",
            "127. No Garage, None, Off Street\n",
            "128. Stall, Public Parking, None\n",
            "129. Heated Garage, Indoor, None, Off Street, Underground\n",
            "130. Detached Garage, Double Garage, None\n",
            "131. Detached Garage, Heated Garage, None, RV Access/Parking, Triple Garage\n",
            "132. Public Parking, None, Off Street, RV Access/Parking\n",
            "133. Detached Garage, None, Single Garage\n",
            "134. Garage Faces Front, None\n",
            "135. Carport, No Garage, None\n",
            "136. 220 Volt Wiring, Detached Garage, Heated Garage, None, Oversized\n",
            "137. 220 Volt Wiring, Insulated, Detached Garage, Double Garage, Heated Garage, None, Off Street\n",
            "138. Detached Garage, Double Garage, Heated Garage, None, Oversized, RV Access/Parking\n",
            "139. Attached, Attached Garage, Double Garage, None\n",
            "140. Insulated, Detached Garage, Double Garage, Heated Garage, None, RV Access/Parking\n",
            "141. Detached Garage, Heated Garage, None\n",
            "142. Insulated, Detached Garage, None, RV Access/Parking\n",
            "143. Insulated, Detached Garage, Double Garage, None\n",
            "144. 220 Volt Wiring, Detached Garage, Heated Garage, None\n",
            "145. Insulated, Detached Garage, Double Garage, None, Off Street\n",
            "146. Public Parking, None, Off Street, Parking Pad, RV Access/Parking\n",
            "147. Plug-In, None\n",
            "148. Detached Garage, Double Garage, Heated Garage, None\n",
            "149. Detached Garage, None, Off Street, Single Garage\n",
            "150. Detached Garage, None, Off Street\n",
            "151. Stall, Plug-In, Public Parking, None\n",
            "152. Detached Garage, Double Garage, None, RV Access/Parking\n",
            "153. Insulated, Detached Garage, Heated Garage, None\n",
            "154. 220 Volt Wiring, Detached Garage, None\n",
            "155. Attached Garage, Double Garage, Heated Garage, None\n",
            "156. Insulated, Detached Garage, Double Garage, Heated Garage, None, Parking Pad\n",
            "157. Insulated, Detached Garage, None\n",
            "158. Detached Garage, None, RV Access/Parking, Single Garage\n",
            "159. 220 Volt Wiring, Indoor, Insulated, Detached Garage, Double Garage, None\n",
            "160. Detached Garage, None, Parking Pad\n",
            "161. Public Parking, None\n",
            "162. Insulated, Detached Garage, None, Oversized, Single Garage\n",
            "163. 220 Volt Wiring, Detached Garage, Double Garage, Heated Garage, None\n",
            "164. 220 Volt Wiring, Insulated, Detached Garage, Heated Garage, None, Oversized, Parking Pad\n",
            "165. Double Garage, None\n",
            "166. Public Parking, Detached Garage, None\n",
            "167. Stall, Plug-In, Public Parking, None, Off Street\n",
            "168. Insulated, Detached Garage, Double Garage, Heated Garage, None\n",
            "169. Attached Garage, Double Garage, Heated Garage, None, Triple Garage\n",
            "170. Public Parking, None, Underground\n",
            "171. Insulated, Attached Garage, Double Garage, Heated Garage, None, Off Street, RV Access/Parking\n",
            "172. Attached Garage, None\n",
            "173. Insulated, Detached Garage, Heated Garage, None, Off Street, Oversized, Parking Pad, RV Access/Parking, Triple Garage\n",
            "174. Detached Garage, Heated Garage, None, Off Street\n",
            "175. 220 Volt Wiring, Indoor, Insulated, Detached Garage, Double Garage, Heated Garage, None\n",
            "176. 220 Volt Wiring, Insulated, None\n",
            "177. Detached Garage, Double Garage, None, Off Street\n",
            "178. Stall, Gravel Driveway, None, Off Street\n",
            "179. Stall, Indoor, None, Off Street, Underground\n",
            "180. 220 Volt Wiring, Detached Garage, None, RV Access/Parking\n",
            "181. Insulated, Detached Garage, Heated Garage, None, RV Access/Parking\n",
            "182. 220 Volt Wiring, Insulated, Detached Garage, Double Garage, Heated Garage, None\n",
            "183. Stall, Public Parking, Heated Garage, None, Underground\n",
            "184. Driveway, Detached Garage, Heated Garage, None\n",
            "185. Gravel Driveway, None, Off Street, Parking Pad\n",
            "186. Concrete Driveway, None, Off Street\n",
            "187. Attached Garage, Double Garage, None\n",
            "188. Insulated, Detached Garage, Double Garage, Heated Garage, None, Off Street\n",
            "189. Detached Garage, Heated Garage, None, RV Access/Parking\n",
            "190. 220 Volt Wiring, Insulated, Detached Garage, Double Garage, Heated Garage, None, Oversized\n",
            "191. Insulated, Detached Garage, Double Garage, None, RV Access/Parking\n",
            "192. 220 Volt Wiring, Detached Garage, Double Garage, None\n",
            "193. None, Secured\n",
            "194. 220 Volt Wiring, Detached Garage, Double Garage, None, RV Access/Parking\n",
            "195. Stall, No Garage, None\n",
            "196. No Garage, None, Parking Pad\n",
            "197. No Garage, None, Off Street, Parking Pad\n",
            "198. Detached Garage, Double Garage, None, Oversized, RV Access/Parking\n",
            "199. None, Single Garage\n",
            "200. Owned, None\n",
            "201. Plug-In, Public Parking, None\n",
            "202. Public Parking, Heated Garage, None, Off Street, Other, Secured, Underground\n",
            "203. Detached Garage, Double Garage, None, Triple Garage\n",
            "204. Insulated, Detached Garage, Double Garage, Heated Garage, None, Oversized, RV Access/Parking\n",
            "205. Insulated, Detached Garage, None, Single Garage\n",
            "206. 220 Volt Wiring, Detached Garage, Heated Garage, None, RV Access/Parking\n",
            "207. 220 Volt Wiring, Insulated, Detached Garage, Double Garage, None\n",
            "208. Insulated, Public Parking, Detached Garage, Double Garage, None\n",
            "209. Detached Garage, None, Oversized\n",
            "210. Plug-In, No Garage, None\n",
            "211. Concrete Driveway, None, Parking Pad\n",
            "212. Owned, Heated Garage, None, Secured, Underground\n",
            "213. Detached Garage, Heated Garage, None, Oversized, Single Garage\n",
            "214. Stall, Driveway, Public Parking, No Garage, None, Off Street, RV Access/Parking\n",
            "215. 220 Volt Wiring, Insulated, Detached Garage, Heated Garage, None\n",
            "216. 220 Volt Wiring, Detached Garage, Double Garage, None, Parking Pad\n",
            "217. Detached Garage, None, Off Street, RV Access/Parking\n",
            "218. Detached Garage, None, RV Access/Parking\n",
            "219. Concrete Driveway, None, Off Street, Parking Pad, RV Access/Parking\n",
            "220. 220 Volt Wiring, Detached Garage, Double Garage, None, Oversized\n",
            "221. 220 Volt Wiring, Insulated, Detached Garage, Double Garage, Heated Garage, None, RV Access/Parking\n",
            "222. Public Parking, No Garage, None, Parking Pad\n",
            "223. Public Parking, Detached Garage, Double Garage, None\n",
            "224. Detached Garage, Double Garage, None, Oversized\n",
            "225. Stall, Heated Garage, None, Underground\n",
            "226. Stall, None, Off Street, Parking Pad, RV Access/Parking\n",
            "227. Detached Garage, Double Garage, None, Oversized, Parking Pad, RV Access/Parking\n",
            "228. Public Parking, Heated Garage, None, Secured, Underground\n",
            "229. Insulated, Detached Garage, Double Garage, None, Oversized\n",
            "230. Plug-In, None, Off Street, Parking Pad\n",
            "231. No Garage, None, RV Access/Parking\n",
            "232. 220 Volt Wiring, Detached Garage, None, Off Street, RV Access/Parking\n",
            "233. Public Parking, Heated Garage, None, Underground\n",
            "234. Plug-In, None, Off Street\n",
            "235. Insulated, Detached Garage, Double Garage, Heated Garage, None, RV Access/Parking, Triple Garage\n",
            "236. Detached Garage, Double Garage, Heated Garage, None, Oversized\n",
            "237. 220 Volt Wiring, Insulated, Detached Garage, Double Garage, None, RV Access/Parking\n",
            "238. Detached Garage, Double Garage, None, Off Street, RV Access/Parking\n",
            "239. Detached Garage, None, Triple Garage\n",
            "240. Detached Garage, None, Off Street, RV Access/Parking, Single Garage\n",
            "241. 220 Volt Wiring, Insulated, Detached Garage, Heated Garage, None, Triple Garage\n",
            "242. Stall, Plug-In, None\n",
            "243. 220 Volt Wiring, Detached Garage, Double Garage, Heated Garage, None, Parking Pad, RV Access/Parking\n",
            "244. Detached Garage, None, Oversized, Single Garage\n",
            "245. Insulated, Attached Garage, None, Single Garage\n",
            "246. Attached Garage, None, RV Access/Parking, Single Garage\n",
            "247. Insulated, Detached Garage, Heated Garage, None, Oversized, Triple Garage\n",
            "248. Public Parking, None, Parking Pad\n",
            "249. Attached Garage, None, Single Garage\n",
            "250. Insulated, Carport, Detached Garage, None, RV Access/Parking, Single Garage\n",
            "251. Insulated, Detached Garage, Heated Garage, None, Single Garage\n",
            "252. No Garage, None, Off Street, RV Access/Parking\n",
            "253. Indoor, Owned, Public Parking, Heated Garage, None, Underground\n",
            "254. Insulated, Detached Garage, Heated Garage, None, Oversized\n",
            "255. Detached Garage, Double Garage, None, Single Garage\n",
            "256. Public Parking, None, Off Street\n",
            "257. Indoor, Public Parking, None, Secured, Underground\n",
            "258. 220 Volt Wiring, Attached Garage, None, Single Garage\n",
            "259. Public Parking, Concrete Driveway, None, Underground\n",
            "260. None, Off Street, Underground\n",
            "261. Indoor, Public Parking, Heated Garage, None, Secured, Underground\n",
            "262. Insulated, Detached Garage, Double Garage, None, Oversized, RV Access/Parking\n",
            "263. Double Garage, None, RV Access/Parking\n",
            "264. Heated Garage, None, Secured, Underground\n",
            "265. Stall, None, Underground\n",
            "266. Insulated, Attached Garage, Double Garage, None\n",
            "267. Detached Garage, None, Off Street, Parking Pad, Single Garage\n",
            "268. Stall, Plug-In, None, Off Street\n",
            "269. 220 Volt Wiring, Detached Garage, Heated Garage, None, Oversized, RV Access/Parking\n",
            "270. 220 Volt Wiring, Insulated, Detached Garage, Heated Garage, None, Oversized\n",
            "271. Detached Garage, Double Garage, None, Parking Pad, RV Access/Parking\n",
            "272. Detached Garage, None, Parking Pad, Single Garage\n",
            "273. Stall, Owned, None\n",
            "274. 220 Volt Wiring, Detached Garage, Heated Garage, None, Triple Garage\n",
            "275. Insulated, Detached Garage, Double Garage, Heated Garage, None, Oversized\n",
            "276. Detached Garage, Double Garage, Heated Garage, None, Parking Pad\n",
            "277. Detached Garage, Heated Garage, None, Oversized, RV Access/Parking\n",
            "278. Stall, Detached Garage, None\n",
            "279. Stall, Public Parking, None, Off Street\n",
            "280. Plug-In, Public Parking, None, Off Street\n",
            "281. 220 Volt Wiring, Insulated, Detached Garage, Double Garage, None, Off Street, Oversized, Parking Pad, RV Access/Parking\n",
            "282. Insulated, Double Garage, None\n",
            "283. Detached Garage, Double Garage, Heated Garage, None, Off Street, Parking Pad, RV Access/Parking\n",
            "284. Insulated, None, Single Garage\n",
            "285. 220 Volt Wiring, Insulated, Detached Garage, Double Garage, Heated Garage, None, Oversized, RV Access/Parking\n",
            "286. Detached Garage, Double Garage, None, Parking Pad\n",
            "287. Gravel Driveway, None, Parking Pad\n",
            "288. Insulated, Attached Garage, None\n",
            "289. 220 Volt Wiring, Insulated, Detached Garage, Double Garage, None, Off Street, Oversized, RV Access/Parking\n",
            "290. Owned, Public Parking, Heated Garage, None, Underground\n",
            "291. Stall, Public Parking, No Garage, None\n",
            "292. 220 Volt Wiring, Detached Garage, None, RV Access/Parking, Single Garage\n",
            "293. Attached Garage, Public Parking, None, RV Access/Parking, Single Garage\n",
            "294. Insulated, Detached Garage, None, Off Street, Oversized, Parking Pad, RV Access/Parking\n",
            "295. Attached Garage, None, Off Street, Single Garage\n",
            "296. Driveway, Attached Garage, Concrete Driveway, Double Garage, None\n",
            "297. Detached Garage, Double Garage, Heated Garage, None, Off Street\n",
            "298. 220 Volt Wiring, Detached Garage, Double Garage, Heated Garage, None, Off Street\n",
            "299. 220 Volt Wiring, Detached Garage, Double Garage, None, Oversized, Parking Pad, RV Access/Parking\n",
            "300. Insulated, Detached Garage, None, Off Street\n",
            "301. Attached Garage, Heated Garage, None, Oversized, RV Access/Parking\n",
            "302. Stall, None, Parking Pad\n",
            "303. 220 Volt Wiring, Detached Garage, None, Off Street, Oversized, Parking Pad, Quad or More, RV Access/Parking, Triple Garage\n",
            "304. 220 Volt Wiring, Insulated, Detached Garage, None\n",
            "305. Public Parking, No Garage, None\n",
            "306. Indoor, Public Parking, Heated Garage, None\n",
            "307. Insulated, Attached Garage, Detached Garage, Heated Garage, None, Triple Garage\n",
            "308. None, Off Street, Oversized, RV Access/Parking\n",
            "309. Detached Garage, Heated Garage, None, Triple Garage\n",
            "310. Public Parking, Detached Garage, None, Parking Pad, RV Access/Parking\n",
            "311. Driveway, None, Off Street, Parking Pad\n",
            "312. Plug-In, Detached Garage, Double Garage, None, RV Access/Parking\n",
            "313. 220 Volt Wiring, Detached Garage, Heated Garage, None, Off Street, RV Access/Parking, Triple Garage\n",
            "314. Detached Garage, Gravel Driveway, None\n",
            "315. Stall, Detached Garage, None, RV Access/Parking\n",
            "316. Asphalt, Assigned, None\n",
            "317. Garage Door Opener, Heated Garage, Leased, None, Off Street, See Remarks\n",
            "318. Insulated, Double Garage, None, RV Access/Parking\n",
            "319. Stall, Public Parking, None, Other, Parking Pad, RV Access/Parking\n",
            "320. Insulated, Heated Garage, None\n",
            "321. 220 Volt Wiring, Insulated, Detached Garage, Heated Garage, None, Off Street\n",
            "322. Detached Garage, Double Garage, Heated Garage, None, Parking Pad, RV Access/Parking\n",
            "323. Public Parking, None, Secured, Underground\n",
            "324. Attached Garage, None, Off Street, RV Access/Parking, Single Garage\n",
            "325. 220 Volt Wiring, Detached Garage, Double Garage, None, Off Street\n",
            "326. 220 Volt Wiring, Detached Garage, Double Garage, Heated Garage, None, Oversized\n",
            "327. Public Parking, No Garage, None, RV Access/Parking\n",
            "328. Heated Garage, None, Underground\n",
            "329. Heated Garage, None, Single Garage\n",
            "330. No Garage, None, Plug-In, Stall\n",
            "331. 220 Volt Wiring, Detached Garage, Double Garage, Heated Garage, None, Oversized, Single Garage\n",
            "332. Insulated, Attached Garage, Double Garage, Heated Garage, None\n",
            "333. Detached Garage, Double Garage, None, Off Street, Parking Pad\n",
            "334. Common, None\n",
            "335. Common, Guest, Leased, None, On Street\n",
            "\n",
            "Frequency of each 'none' description:\n",
            "• None, Off Street: 367 occurrences\n",
            "• Detached Garage, Double Garage, None: 235 occurrences\n",
            "• None, On Street: 229 occurrences\n",
            "• Detached Garage, None: 205 occurrences\n",
            "• None, Parking Pad: 138 occurrences\n",
            "• None, Off Street, Parking Pad: 109 occurrences\n",
            "• No Garage, None: 91 occurrences\n",
            "• Detached Garage, None, Single Garage: 79 occurrences\n",
            "• None, RV Access/Parking: 46 occurrences\n",
            "• None, Parking Pad, RV Access/Parking: 38 occurrences\n",
            "• Detached Garage, Double Garage, None, RV Access/Parking: 33 occurrences\n",
            "• Alley Access, None: 29 occurrences\n",
            "• Stall, None: 28 occurrences\n",
            "• None, Off Street, Parking Pad, RV Access/Parking: 27 occurrences\n",
            "• Detached Garage, Double Garage, Heated Garage, None: 25 occurrences\n",
            "• Alley Access, None, On Street: 25 occurrences\n",
            "• None, Unassigned: 23 occurrences\n",
            "• No Garage, None, Off Street: 22 occurrences\n",
            "• No Garage, None, Parking Pad: 21 occurrences\n",
            "• Insulated, Detached Garage, Double Garage, Heated Garage, None: 21 occurrences\n",
            "• None, Off Street, RV Access/Parking: 21 occurrences\n",
            "• None, See Remarks: 21 occurrences\n",
            "• Insulated, Detached Garage, Double Garage, None: 20 occurrences\n",
            "• Attached Garage, None: 18 occurrences\n",
            "• Stall, Public Parking, None: 17 occurrences\n",
            "• Gravel Driveway, None: 15 occurrences\n",
            "• Detached Garage, Double Garage, None, Off Street: 15 occurrences\n",
            "• Insulated, Detached Garage, None: 13 occurrences\n",
            "• None, Single Garage: 11 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Double Garage, None: 11 occurrences\n",
            "• Detached Garage, Double Garage, Heated Garage, None, RV Access/Parking: 11 occurrences\n",
            "• Detached Garage, None, RV Access/Parking: 11 occurrences\n",
            "• Detached Garage, Heated Garage, None: 11 occurrences\n",
            "• Driveway, None: 10 occurrences\n",
            "• Public Parking, None: 10 occurrences\n",
            "• No Garage, None, Off Street, Parking Pad: 9 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Double Garage, Heated Garage, None: 9 occurrences\n",
            "• Insulated, Detached Garage, Double Garage, None, RV Access/Parking: 9 occurrences\n",
            "• None, Other: 9 occurrences\n",
            "• Double Garage, None: 8 occurrences\n",
            "• None, On Street, See Remarks: 8 occurrences\n",
            "• None, Off Street, On Street: 8 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Double Garage, None: 8 occurrences\n",
            "• None, Underground: 7 occurrences\n",
            "• No Garage, None, RV Access/Parking: 7 occurrences\n",
            "• Detached Garage, Double Garage, None, Oversized: 7 occurrences\n",
            "• Detached Garage, None, Off Street: 7 occurrences\n",
            "• Insulated, Detached Garage, Double Garage, Heated Garage, None, RV Access/Parking: 7 occurrences\n",
            "• Insulated, Detached Garage, Heated Garage, None: 7 occurrences\n",
            "• Detached Garage, None, RV Access/Parking, Single Garage: 6 occurrences\n",
            "• Detached Garage, None, Oversized, Single Garage: 6 occurrences\n",
            "• Stall, None, Off Street: 5 occurrences\n",
            "• Guest, None: 5 occurrences\n",
            "• Public Parking, None, Off Street: 5 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Double Garage, Heated Garage, None: 5 occurrences\n",
            "• Leased, None: 5 occurrences\n",
            "• Insulated, Detached Garage, Double Garage, Heated Garage, None, Oversized, RV Access/Parking: 5 occurrences\n",
            "• Plug-In, Public Parking, None: 5 occurrences\n",
            "• Detached Garage, None, Oversized: 5 occurrences\n",
            "• Insulated, Detached Garage, None, RV Access/Parking: 4 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Heated Garage, None: 4 occurrences\n",
            "• Detached Garage, Double Garage, None, Oversized, RV Access/Parking: 4 occurrences\n",
            "• None, On Street, Unassigned: 4 occurrences\n",
            "• Detached Garage, None, Triple Garage: 4 occurrences\n",
            "• Insulated, Detached Garage, None, Single Garage: 4 occurrences\n",
            "• Stall, Plug-In, None: 4 occurrences\n",
            "• Detached Garage, None, Off Street, Single Garage: 4 occurrences\n",
            "• Plug-In, None: 4 occurrences\n",
            "• Insulated, Detached Garage, Double Garage, None, Off Street: 4 occurrences\n",
            "• Insulated, Detached Garage, Double Garage, Heated Garage, None, Oversized: 4 occurrences\n",
            "• None, On Street, Permit Required: 4 occurrences\n",
            "• Attached Garage, None, Single Garage: 3 occurrences\n",
            "• Guest, None, On Street: 3 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Double Garage, Heated Garage, None, Oversized: 3 occurrences\n",
            "• Stall, Plug-In, Public Parking, None: 3 occurrences\n",
            "• Attached Garage, Double Garage, None: 3 occurrences\n",
            "• Insulated, Detached Garage, Double Garage, Heated Garage, None, Off Street: 3 occurrences\n",
            "• Detached Garage, None, Parking Pad: 3 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, None: 3 occurrences\n",
            "• Detached Garage, Heated Garage, None, Off Street: 3 occurrences\n",
            "• Public Parking, No Garage, None, Parking Pad: 3 occurrences\n",
            "• No Garage, None, Off Street, RV Access/Parking: 3 occurrences\n",
            "• Alley Access, None, On Street, Outside: 3 occurrences\n",
            "• None, Parkade: 3 occurrences\n",
            "• Insulated, Detached Garage, Double Garage, None, Oversized: 3 occurrences\n",
            "• Carport, None: 3 occurrences\n",
            "• None, On Street, Outside: 3 occurrences\n",
            "• None, Stall: 3 occurrences\n",
            "• Plug-In, None, Off Street: 3 occurrences\n",
            "• Alley Access, None, Off Street, On Street: 3 occurrences\n",
            "• Assigned, None: 3 occurrences\n",
            "• Detached Garage, None, Parking Pad, Single Garage: 3 occurrences\n",
            "• Detached Garage, Double Garage, None, Parking Pad, RV Access/Parking: 3 occurrences\n",
            "• Stall, None, Underground: 2 occurrences\n",
            "• Detached Garage, None, Off Street, Parking Pad, Single Garage: 2 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Heated Garage, None, Oversized: 2 occurrences\n",
            "• Indoor, Public Parking, Heated Garage, None, Secured, Underground: 2 occurrences\n",
            "• Carport, None, Off Street: 2 occurrences\n",
            "• None, Off Street, Other: 2 occurrences\n",
            "• Additional Parking, None: 2 occurrences\n",
            "• None, On Street, Permit Required, See Remarks, Unassigned: 2 occurrences\n",
            "• None, Outside: 2 occurrences\n",
            "• Double Garage Attached, None: 2 occurrences\n",
            "• Gravel Driveway, None, On Street: 2 occurrences\n",
            "• None, Off Street, On Street, See Remarks: 2 occurrences\n",
            "• Alley Access, None, Unpaved: 2 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Heated Garage, None, Triple Garage: 2 occurrences\n",
            "• Public Parking, Heated Garage, None, Underground: 2 occurrences\n",
            "• Detached Garage, None, Off Street, RV Access/Parking, Single Garage: 2 occurrences\n",
            "• Stall, Heated Garage, None, Underground: 2 occurrences\n",
            "• Public Parking, Heated Garage, None, Secured, Underground: 2 occurrences\n",
            "• None, Off Street, Unassigned: 2 occurrences\n",
            "• Detached Garage, Double Garage, Heated Garage, None, Oversized: 2 occurrences\n",
            "• Detached Garage, Double Garage, None, Off Street, RV Access/Parking: 2 occurrences\n",
            "• Public Parking, Detached Garage, Double Garage, None: 2 occurrences\n",
            "• Insulated, Detached Garage, Heated Garage, None, RV Access/Parking: 2 occurrences\n",
            "• Stall, Public Parking, Heated Garage, None, Underground: 2 occurrences\n",
            "• Public Parking, None, Underground: 2 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Double Garage, Heated Garage, None, Off Street: 2 occurrences\n",
            "• Detached Garage, Double Garage, Heated Garage, None, Oversized, RV Access/Parking: 2 occurrences\n",
            "• Alley Access, None, See Remarks: 2 occurrences\n",
            "• Alley Access, None, Off Street: 2 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Heated Garage, None, RV Access/Parking: 2 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Double Garage, None, RV Access/Parking: 2 occurrences\n",
            "• Alley Access, None, On Street, See Remarks: 2 occurrences\n",
            "• None, Unpaved: 2 occurrences\n",
            "• Insulated, Detached Garage, Heated Garage, None, Oversized: 2 occurrences\n",
            "• Public Parking, No Garage, None: 2 occurrences\n",
            "• Indoor, Public Parking, Heated Garage, None: 2 occurrences\n",
            "• None, Other, Outside: 2 occurrences\n",
            "• None, Off Street, Stall: 2 occurrences\n",
            "• Gravel Driveway, None, Parking Pad: 2 occurrences\n",
            "• Detached Garage, Double Garage, None, Parking Pad: 2 occurrences\n",
            "• Common, Guest, Leased, None, On Street: 1 occurrences\n",
            "• Guest, Leased, None, Parkade, See Remarks, Stall: 1 occurrences\n",
            "• Insulated, Attached Garage, None: 1 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Heated Garage, None, Off Street: 1 occurrences\n",
            "• Detached Garage, Double Garage, Heated Garage, None, Parking Pad, RV Access/Parking: 1 occurrences\n",
            "• Public Parking, None, Secured, Underground: 1 occurrences\n",
            "• Attached Garage, None, Off Street, RV Access/Parking, Single Garage: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Double Garage, None, Off Street: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Double Garage, Heated Garage, None, Oversized: 1 occurrences\n",
            "• Public Parking, No Garage, None, RV Access/Parking: 1 occurrences\n",
            "• Additional Parking, None, Off Street, Rear Drive: 1 occurrences\n",
            "• Common, Garage Door Opener, Heated Garage, None: 1 occurrences\n",
            "• Alley Access, None, Off Street, On Street, Parking Lot, Side By Side, Unpaved: 1 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Double Garage, None, Off Street, Oversized, RV Access/Parking: 1 occurrences\n",
            "• Owned, Public Parking, Heated Garage, None, Underground: 1 occurrences\n",
            "• Stall, Public Parking, No Garage, None: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, None, RV Access/Parking, Single Garage: 1 occurrences\n",
            "• Attached Garage, Public Parking, None, RV Access/Parking, Single Garage: 1 occurrences\n",
            "• Insulated, Detached Garage, None, Off Street, Oversized, Parking Pad, RV Access/Parking: 1 occurrences\n",
            "• Attached Garage, None, Off Street, Single Garage: 1 occurrences\n",
            "• Driveway, Attached Garage, Concrete Driveway, Double Garage, None: 1 occurrences\n",
            "• Detached Garage, Double Garage, Heated Garage, None, Off Street: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Double Garage, Heated Garage, None, Off Street: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Double Garage, None, Oversized, Parking Pad, RV Access/Parking: 1 occurrences\n",
            "• Insulated, Detached Garage, None, Off Street: 1 occurrences\n",
            "• Attached Garage, Heated Garage, None, Oversized, RV Access/Parking: 1 occurrences\n",
            "• Stall, None, Parking Pad: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, None, Off Street, Oversized, Parking Pad, Quad or More, RV Access/Parking, Triple Garage: 1 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, None: 1 occurrences\n",
            "• Insulated, Attached Garage, Detached Garage, Heated Garage, None, Triple Garage: 1 occurrences\n",
            "• None, Off Street, Oversized, RV Access/Parking: 1 occurrences\n",
            "• Detached Garage, Heated Garage, None, Triple Garage: 1 occurrences\n",
            "• Public Parking, Detached Garage, None, Parking Pad, RV Access/Parking: 1 occurrences\n",
            "• Driveway, None, Off Street, Parking Pad: 1 occurrences\n",
            "• Plug-In, Detached Garage, Double Garage, None, RV Access/Parking: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Heated Garage, None, Off Street, RV Access/Parking, Triple Garage: 1 occurrences\n",
            "• Detached Garage, Gravel Driveway, None: 1 occurrences\n",
            "• Stall, Detached Garage, None, RV Access/Parking: 1 occurrences\n",
            "• Asphalt, Assigned, None: 1 occurrences\n",
            "• Garage Door Opener, Heated Garage, Leased, None, Off Street, See Remarks: 1 occurrences\n",
            "• Insulated, Double Garage, None, RV Access/Parking: 1 occurrences\n",
            "• Stall, Public Parking, None, Other, Parking Pad, RV Access/Parking: 1 occurrences\n",
            "• Insulated, Heated Garage, None: 1 occurrences\n",
            "• Heated Garage, Indoor, None, Off Street, Underground: 1 occurrences\n",
            "• Detached Garage, Heated Garage, None, RV Access/Parking, Triple Garage: 1 occurrences\n",
            "• Public Parking, None, Off Street, RV Access/Parking: 1 occurrences\n",
            "• Garage Faces Front, None: 1 occurrences\n",
            "• Carport, No Garage, None: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Heated Garage, None, Oversized: 1 occurrences\n",
            "• Attached, Attached Garage, Double Garage, None: 1 occurrences\n",
            "• Public Parking, None, Off Street, Parking Pad, RV Access/Parking: 1 occurrences\n",
            "• Attached Garage, Double Garage, Heated Garage, None: 1 occurrences\n",
            "• Insulated, Detached Garage, Double Garage, Heated Garage, None, Parking Pad: 1 occurrences\n",
            "• 220 Volt Wiring, Indoor, Insulated, Detached Garage, Double Garage, None: 1 occurrences\n",
            "• Assigned, Insulated, Heated Garage, None, On Street, Other, See Remarks, Single Garage Attached: 1 occurrences\n",
            "• Alley Access, None, Outside, See Remarks: 1 occurrences\n",
            "• Gravel Driveway, None, Off Street, Oversized, See Remarks: 1 occurrences\n",
            "• Stall, None, On Street: 1 occurrences\n",
            "• Gravel Driveway, None, Off Street: 1 occurrences\n",
            "• None, Workshop in Garage: 1 occurrences\n",
            "• Additional Parking, Alley Access, Gravel Driveway, None, On Street, Parking Pad, See Remarks, Unpaved: 1 occurrences\n",
            "• Additional Parking, Alley Access, Double Garage Detached, Garage Faces Rear, None, RV Access/Parking: 1 occurrences\n",
            "• Driveway, Asphalt, None, On Street: 1 occurrences\n",
            "• Double Garage Detached, None: 1 occurrences\n",
            "• None, Parking Pad, Unpaved: 1 occurrences\n",
            "• None, RV Access/Parking, RV Carport: 1 occurrences\n",
            "• Triple Garage Attached, None: 1 occurrences\n",
            "• None, Permit Required: 1 occurrences\n",
            "• Concrete Driveway, None, On Street: 1 occurrences\n",
            "• None, Parking Lot: 1 occurrences\n",
            "• Stall, Assigned, None: 1 occurrences\n",
            "• None, Off Street, Plug-In: 1 occurrences\n",
            "• Driveway, None, Paved: 1 occurrences\n",
            "• Driveway, Concrete Driveway, Garage Door Opener, None: 1 occurrences\n",
            "• Insulated, Detached Garage, None, Oversized, Single Garage: 1 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Heated Garage, None, Oversized, Parking Pad: 1 occurrences\n",
            "• Public Parking, Detached Garage, None: 1 occurrences\n",
            "• Stall, Plug-In, Public Parking, None, Off Street: 1 occurrences\n",
            "• Attached Garage, Double Garage, Heated Garage, None, Triple Garage: 1 occurrences\n",
            "• Insulated, Attached Garage, Double Garage, Heated Garage, None, Off Street, RV Access/Parking: 1 occurrences\n",
            "• Insulated, Detached Garage, Heated Garage, None, Off Street, Oversized, Parking Pad, RV Access/Parking, Triple Garage: 1 occurrences\n",
            "• 220 Volt Wiring, Indoor, Insulated, Detached Garage, Double Garage, Heated Garage, None: 1 occurrences\n",
            "• 220 Volt Wiring, Insulated, None: 1 occurrences\n",
            "• Stall, Gravel Driveway, None, Off Street: 1 occurrences\n",
            "• Stall, Indoor, None, Off Street, Underground: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, None, RV Access/Parking: 1 occurrences\n",
            "• Driveway, Detached Garage, Heated Garage, None: 1 occurrences\n",
            "• Gravel Driveway, None, Off Street, Parking Pad: 1 occurrences\n",
            "• Concrete Driveway, None, Off Street: 1 occurrences\n",
            "• Detached Garage, Heated Garage, None, RV Access/Parking: 1 occurrences\n",
            "• None, Secured: 1 occurrences\n",
            "• Stall, No Garage, None: 1 occurrences\n",
            "• Owned, None: 1 occurrences\n",
            "• Public Parking, Heated Garage, None, Off Street, Other, Secured, Underground: 1 occurrences\n",
            "• Detached Garage, Double Garage, None, Triple Garage: 1 occurrences\n",
            "• Insulated, Public Parking, Detached Garage, Double Garage, None: 1 occurrences\n",
            "• Plug-In, No Garage, None: 1 occurrences\n",
            "• Concrete Driveway, None, Parking Pad: 1 occurrences\n",
            "• Owned, Heated Garage, None, Secured, Underground: 1 occurrences\n",
            "• Detached Garage, Heated Garage, None, Oversized, Single Garage: 1 occurrences\n",
            "• Stall, Driveway, Public Parking, No Garage, None, Off Street, RV Access/Parking: 1 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Heated Garage, None: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Double Garage, None, Parking Pad: 1 occurrences\n",
            "• Detached Garage, None, Off Street, RV Access/Parking: 1 occurrences\n",
            "• Concrete Driveway, None, Off Street, Parking Pad, RV Access/Parking: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Double Garage, None, Oversized: 1 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Double Garage, Heated Garage, None, RV Access/Parking: 1 occurrences\n",
            "• Alley Access, None, Off Street, Stall: 1 occurrences\n",
            "• None, Tandem, Unpaved: 1 occurrences\n",
            "• Assigned, None, Titled: 1 occurrences\n",
            "• Assigned, None, Stall: 1 occurrences\n",
            "• None, On Street, Unpaved: 1 occurrences\n",
            "• Assigned, Driveway, None: 1 occurrences\n",
            "• Alley Access, Gravel Driveway, None, On Street: 1 occurrences\n",
            "• Alley Access, Common, None: 1 occurrences\n",
            "• Double Garage Detached, None, On Street: 1 occurrences\n",
            "• Alley Access, None, On Street, RV Access/Parking: 1 occurrences\n",
            "• None, On Street, Other: 1 occurrences\n",
            "• Driveway, None, Parking Pad: 1 occurrences\n",
            "• None, Titled: 1 occurrences\n",
            "• Driveway, None, Single Garage Attached: 1 occurrences\n",
            "• Alley Access, Concrete Driveway, None, Off Street: 1 occurrences\n",
            "• Gravel Driveway, None, See Remarks: 1 occurrences\n",
            "• None, Off Street, On Street, Secured, Unpaved: 1 occurrences\n",
            "• None, Outside, Stall: 1 occurrences\n",
            "• Alley Access, None, Parking Pad: 1 occurrences\n",
            "• Covered, None: 1 occurrences\n",
            "• Stall, Alley Access, None, On Street: 1 occurrences\n",
            "• None, On Street, Outside, Permit Required, See Remarks: 1 occurrences\n",
            "• None, Off Street, Single Garage Detached: 1 occurrences\n",
            "• None, Single Garage Detached: 1 occurrences\n",
            "• Stall, None, Off Street, Parking Pad, RV Access/Parking: 1 occurrences\n",
            "• Detached Garage, Double Garage, None, Oversized, Parking Pad, RV Access/Parking: 1 occurrences\n",
            "• Plug-In, None, Off Street, Parking Pad: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, None, Off Street, RV Access/Parking: 1 occurrences\n",
            "• Insulated, Detached Garage, Double Garage, Heated Garage, None, RV Access/Parking, Triple Garage: 1 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Double Garage, None, RV Access/Parking: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Double Garage, Heated Garage, None, Parking Pad, RV Access/Parking: 1 occurrences\n",
            "• Insulated, Attached Garage, None, Single Garage: 1 occurrences\n",
            "• Attached Garage, None, RV Access/Parking, Single Garage: 1 occurrences\n",
            "• Insulated, Detached Garage, Heated Garage, None, Oversized, Triple Garage: 1 occurrences\n",
            "• Public Parking, None, Parking Pad: 1 occurrences\n",
            "• Insulated, Carport, Detached Garage, None, RV Access/Parking, Single Garage: 1 occurrences\n",
            "• Insulated, Detached Garage, Heated Garage, None, Single Garage: 1 occurrences\n",
            "• Indoor, Owned, Public Parking, Heated Garage, None, Underground: 1 occurrences\n",
            "• Detached Garage, Double Garage, None, Single Garage: 1 occurrences\n",
            "• None, Off Street, Shared Driveway: 1 occurrences\n",
            "• Alley Access, None, Off Street, Outside: 1 occurrences\n",
            "• Asphalt, Carport, None, Off Street, On Street, Outside, Parking Pad: 1 occurrences\n",
            "• Alley Access, None, Unassigned: 1 occurrences\n",
            "• None, On Street, Parking Lot, Rear Drive: 1 occurrences\n",
            "• Common, None, On Street, Outside: 1 occurrences\n",
            "• None, Off Street, On Street, Other: 1 occurrences\n",
            "• Asphalt, Assigned, None, Parking Lot, Paved, Plug-In: 1 occurrences\n",
            "• Asphalt, None: 1 occurrences\n",
            "• None, Outside, Parking Lot: 1 occurrences\n",
            "• Guest, None, Other: 1 occurrences\n",
            "• None, Other, Unassigned: 1 occurrences\n",
            "• None, Off Street, See Remarks, Unassigned: 1 occurrences\n",
            "• Guest, None, Parking Lot: 1 occurrences\n",
            "• Concrete Driveway, None, RV Access/Parking: 1 occurrences\n",
            "• None, Other, See Remarks, Unpaved: 1 occurrences\n",
            "• None, Secured, See Remarks: 1 occurrences\n",
            "• Concrete Driveway, None: 1 occurrences\n",
            "• Indoor, Public Parking, None, Secured, Underground: 1 occurrences\n",
            "• 220 Volt Wiring, Attached Garage, None, Single Garage: 1 occurrences\n",
            "• Public Parking, Concrete Driveway, None, Underground: 1 occurrences\n",
            "• None, Off Street, Underground: 1 occurrences\n",
            "• Insulated, Detached Garage, Double Garage, None, Oversized, RV Access/Parking: 1 occurrences\n",
            "• Double Garage, None, RV Access/Parking: 1 occurrences\n",
            "• Heated Garage, None, Secured, Underground: 1 occurrences\n",
            "• Insulated, Attached Garage, Double Garage, None: 1 occurrences\n",
            "• Stall, Plug-In, None, Off Street: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Heated Garage, None, Oversized, RV Access/Parking: 1 occurrences\n",
            "• Stall, Owned, None: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Heated Garage, None, Triple Garage: 1 occurrences\n",
            "• Detached Garage, Double Garage, Heated Garage, None, Parking Pad: 1 occurrences\n",
            "• Detached Garage, Heated Garage, None, Oversized, RV Access/Parking: 1 occurrences\n",
            "• Stall, Detached Garage, None: 1 occurrences\n",
            "• Stall, Public Parking, None, Off Street: 1 occurrences\n",
            "• Plug-In, Public Parking, None, Off Street: 1 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Double Garage, None, Off Street, Oversized, Parking Pad, RV Access/Parking: 1 occurrences\n",
            "• Insulated, Double Garage, None: 1 occurrences\n",
            "• Detached Garage, Double Garage, Heated Garage, None, Off Street, Parking Pad, RV Access/Parking: 1 occurrences\n",
            "• Insulated, None, Single Garage: 1 occurrences\n",
            "• 220 Volt Wiring, Insulated, Detached Garage, Double Garage, Heated Garage, None, Oversized, RV Access/Parking: 1 occurrences\n",
            "• Heated Garage, None, Underground: 1 occurrences\n",
            "• Heated Garage, None, Single Garage: 1 occurrences\n",
            "• No Garage, None, Plug-In, Stall: 1 occurrences\n",
            "• 220 Volt Wiring, Detached Garage, Double Garage, Heated Garage, None, Oversized, Single Garage: 1 occurrences\n",
            "• Insulated, Attached Garage, Double Garage, Heated Garage, None: 1 occurrences\n",
            "• Detached Garage, Double Garage, None, Off Street, Parking Pad: 1 occurrences\n",
            "• Common, None: 1 occurrences\n",
            "• Front Drive, None, On Street: 1 occurrences\n",
            "• None, On Street, Parking Pad: 1 occurrences\n",
            "• Alley Access, None, Other: 1 occurrences\n",
            "• None, On Street, Paved: 1 occurrences\n",
            "• Guest, None, Off Street, On Street, Permit Required, See Remarks: 1 occurrences\n",
            "• Front Drive, None: 1 occurrences\n",
            "• None, On Street, Other, See Remarks: 1 occurrences\n",
            "• Guest, None, Permit Required, See Remarks, Underground: 1 occurrences\n",
            "• None, Off Street, Unpaved: 1 occurrences\n",
            "Starting parking analysis and standardization...\n",
            "\n",
            "=== INITIAL DATA ANALYSIS ===\n",
            "Total properties in dataset: 393,664\n",
            "Properties with parking data: 378,317 (96.1%)\n",
            "Properties with no parking data (nulls): 15,347 (3.9%)\n",
            "\n",
            "Collecting term frequencies...\n",
            "\n",
            "Term frequency analysis:\n",
            "• double garage attached: 126,069 occurrences\n",
            "• double garage detached: 70,299 occurrences\n",
            "• single garage attached: 50,760 occurrences\n",
            "• underground: 42,855 occurrences\n",
            "• heated garage: 40,283 occurrences\n",
            "• stall: 40,070 occurrences\n",
            "• assigned: 33,179 occurrences\n",
            "• titled: 32,390 occurrences\n",
            "• parking pad: 26,745 occurrences\n",
            "• off street: 23,415 occurrences\n",
            "\n",
            "Applying standardization rules...\n",
            "\n",
            "=== STANDARDIZATION RESULTS ===\n",
            "Original null values: 15,347\n",
            "New null values created: 0\n",
            "Total 'no parking' designations: 16,260\n",
            "\n",
            "Top 10 standardized parking types:\n",
            "• double garage attached: 106,655 properties (27.1%)\n",
            "• double garage detached: 54,110 properties (13.7%)\n",
            "• single garage attached: 16,645 properties (4.2%)\n",
            "• no parking: 16,260 properties (4.1%)\n",
            "• stall, assigned: 13,193 properties (3.4%)\n",
            "• single garage detached: 12,090 properties (3.1%)\n",
            "• parking pad: 11,130 properties (2.8%)\n",
            "• single garage attached, single garage attached: 8,042 properties (2.0%)\n",
            "• stall: 7,419 properties (1.9%)\n",
            "• off street: 6,574 properties (1.7%)\n",
            "\n",
            "Parking analysis completed successfully! 🎉\n",
            "Step 13 completed: Parking analysis and standardization\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# PARKING ANALYSIS AND STANDARDIZATION\n",
        "# =================================================================================\n",
        "\n",
        "# Define our standardization rules as a constant at the module level\n",
        "# These rules help us clean and standardize parking descriptions consistently\n",
        "PARKING_RULES = {\n",
        "    # Terms to remove (common modifiers that don't affect parking type)\n",
        "    'remove_terms': {\n",
        "        'oversized', 'insulated', 'indoor', 'garage door opener', 'driveway',\n",
        "        'concrete driveway', 'alley access', 'garage faces front',\n",
        "        'garage faces rear', 'see remarks', 'additional parking', 'secured',\n",
        "        'none', 'gravel driveway', 'enclosed', 'workshop in garage', 'paved'\n",
        "    },\n",
        "\n",
        "    # Important terms to preserve regardless of frequency\n",
        "    'preserve_terms': {\n",
        "        'plug-in', 'single garage', 'no garage', 'quad or more attached',\n",
        "        'triple garage', 'quad or more detached', 'heated driveway',\n",
        "        'electric vehicle charging station', 'carport'\n",
        "    },\n",
        "\n",
        "    # Term standardization mapping - converts variations to standard terms\n",
        "    'term_standardization': {\n",
        "        'in garage electric vehicle charging station(s)': 'electric vehicle charging station',\n",
        "        'private electric vehicle charging station(s)': 'electric vehicle charging station',\n",
        "        'attached carport': 'carport',\n",
        "        'attached garage': 'single garage attached',\n",
        "        'detached garage': 'single garage detached'\n",
        "    }\n",
        "}\n",
        "\n",
        "def analyze_parking_none_values(df, parking_column='Parking'):\n",
        "    \"\"\"\n",
        "    Analyze rows where 'none' appears in the parking description.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame containing parking data\n",
        "        parking_column (str): Name of the column containing parking descriptions\n",
        "\n",
        "    Returns:\n",
        "        tuple: (count of 'none' descriptions, unique descriptions, value counts)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"\\nAnalyzing parking descriptions containing 'none'...\")\n",
        "\n",
        "        # Input validation - make sure the column exists\n",
        "        if parking_column not in df.columns:\n",
        "            raise ValueError(f\"Column '{parking_column}' not found in DataFrame\")\n",
        "\n",
        "        # Create mask for rows containing 'none' - handle nulls safely by filling them first\n",
        "        none_mask = df[parking_column].fillna('').str.contains('none', case=False)\n",
        "        none_descriptions = df[none_mask][parking_column]\n",
        "\n",
        "        # Get unique descriptions and their counts\n",
        "        unique_descriptions = none_descriptions.unique()\n",
        "        value_counts = none_descriptions.value_counts()\n",
        "\n",
        "        # Log the analysis results with clear formatting\n",
        "        logger.log_info(\"=\" * 50)\n",
        "        logger.log_info(\"ANALYSIS OF 'NONE' PARKING DESCRIPTIONS\")\n",
        "        logger.log_info(\"=\" * 50)\n",
        "        logger.log_info(f\"Total descriptions containing 'none': {len(none_descriptions):,}\")\n",
        "\n",
        "        if len(unique_descriptions) > 0:\n",
        "            logger.log_info(\"\\nUnique descriptions containing 'none':\")\n",
        "            for idx, desc in enumerate(unique_descriptions, 1):\n",
        "                logger.log_info(f\"{idx}. {desc}\")\n",
        "\n",
        "            logger.log_info(\"\\nFrequency of each 'none' description:\")\n",
        "            for desc, count in value_counts.items():\n",
        "                logger.log_info(f\"• {desc}: {count:,} occurrences\")\n",
        "        else:\n",
        "            logger.log_info(\"\\nNo descriptions containing 'none' found.\")\n",
        "\n",
        "        return len(none_descriptions), unique_descriptions, value_counts\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_parking_none_values: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def clean_parking_description(description, term_frequencies, rules=PARKING_RULES):\n",
        "    \"\"\"\n",
        "    Clean and standardize a single parking description.\n",
        "\n",
        "    Args:\n",
        "        description: Original parking description\n",
        "        term_frequencies (dict): Frequency counts of parking terms\n",
        "        rules (dict): Dictionary containing standardization rules\n",
        "\n",
        "    Returns:\n",
        "        str or None: Cleaned parking description, or None if invalid\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Handle null values\n",
        "        if pd.isna(description):\n",
        "            return description\n",
        "\n",
        "        # Convert to lowercase and split into terms\n",
        "        description = str(description).lower().strip()\n",
        "        terms = [term.strip() for term in description.split(',')]\n",
        "\n",
        "        # Special handling for 'none' only descriptions\n",
        "        if len(terms) == 1 and terms[0] == 'none':\n",
        "            return 'no parking'\n",
        "\n",
        "        # Remove 'none' if it exists with other terms\n",
        "        terms = [term for term in terms if term != 'none']\n",
        "\n",
        "        # First apply term standardization\n",
        "        standardized_terms = []\n",
        "        for term in terms:\n",
        "            # Check if the exact term exists in our standardization mapping\n",
        "            if term in rules['term_standardization']:\n",
        "                standardized_terms.append(rules['term_standardization'][term])\n",
        "            else:\n",
        "                standardized_terms.append(term)\n",
        "\n",
        "        # Then filter terms based on rules and frequency\n",
        "        cleaned_terms = []\n",
        "        for term in standardized_terms:\n",
        "            # Keep term if it's preserved or frequent enough and not in remove list\n",
        "            if (term in rules['preserve_terms'] or\n",
        "                (term_frequencies.get(term, 0) >= 2000 and\n",
        "                 term not in rules['remove_terms'])):\n",
        "                cleaned_terms.append(term)\n",
        "\n",
        "        # Return standardized string or 'no parking' if no terms remain\n",
        "        return ', '.join(cleaned_terms) if cleaned_terms else 'no parking'\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error cleaning parking description '{description}': {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def analyze_and_standardize_parking(df, parking_column='Parking', frequency_threshold=2000):\n",
        "    \"\"\"\n",
        "    Analyze parking descriptions and create standardized versions while preserving original data.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame containing parking data\n",
        "        parking_column (str): Name of the column containing parking descriptions\n",
        "        frequency_threshold (int): Minimum frequency for terms to be retained\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with added standardized parking columns\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting parking analysis and standardization...\")\n",
        "\n",
        "        # Input validation\n",
        "        if parking_column not in df.columns:\n",
        "            raise ValueError(f\"Column '{parking_column}' not found in DataFrame\")\n",
        "\n",
        "        # Create working copy of DataFrame to preserve original data\n",
        "        df_copy = df.copy()\n",
        "\n",
        "        # Log initial statistics about our data\n",
        "        total_rows = len(df)\n",
        "        original_nulls = df[parking_column].isnull().sum()\n",
        "        original_valid = total_rows - original_nulls\n",
        "\n",
        "        logger.log_info(\"\\n=== INITIAL DATA ANALYSIS ===\")\n",
        "        logger.log_info(f\"Total properties in dataset: {total_rows:,}\")\n",
        "        logger.log_info(f\"Properties with parking data: {original_valid:,} ({(original_valid/total_rows*100):.1f}%)\")\n",
        "        logger.log_info(f\"Properties with no parking data (nulls): {original_nulls:,} ({(original_nulls/total_rows*100):.1f}%)\")\n",
        "\n",
        "        # First pass: collect and standardize all terms for frequency analysis\n",
        "        logger.log_info(\"\\nCollecting term frequencies...\")\n",
        "        all_terms = []\n",
        "\n",
        "        # Process non-null values only\n",
        "        valid_descriptions = df[parking_column].dropna()\n",
        "        for description in valid_descriptions:\n",
        "            # Split into terms and standardize each\n",
        "            terms = [term.strip().lower() for term in str(description).split(',')]\n",
        "\n",
        "            # Apply standardization rules first\n",
        "            standardized_terms = []\n",
        "            for term in terms:\n",
        "                if term in PARKING_RULES['term_standardization']:\n",
        "                    standardized_terms.append(PARKING_RULES['term_standardization'][term])\n",
        "                else:\n",
        "                    standardized_terms.append(term)\n",
        "\n",
        "            # Only include terms that aren't in remove_terms\n",
        "            filtered_terms = [term for term in standardized_terms\n",
        "                            if term not in PARKING_RULES['remove_terms']]\n",
        "            all_terms.extend(filtered_terms)\n",
        "\n",
        "        # Calculate term frequencies\n",
        "        term_frequencies = pd.Series(all_terms).value_counts().to_dict()\n",
        "\n",
        "        # Log frequency analysis\n",
        "        logger.log_info(\"\\nTerm frequency analysis:\")\n",
        "        for term, freq in sorted(term_frequencies.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "            logger.log_info(f\"• {term}: {freq:,} occurrences\")\n",
        "\n",
        "        # Second pass: clean and standardize descriptions\n",
        "        logger.log_info(\"\\nApplying standardization rules...\")\n",
        "        df_copy['standardized_parking'] = df[parking_column].apply(\n",
        "            lambda x: clean_parking_description(x, term_frequencies)\n",
        "        )\n",
        "\n",
        "        # Convert nulls to 'no parking' in a separate column\n",
        "        df_copy['standardized_parking_complete'] = df_copy['standardized_parking'].fillna('no parking')\n",
        "\n",
        "        # Calculate and log final statistics\n",
        "        final_nulls = df_copy['standardized_parking'].isnull().sum()\n",
        "        no_parking_count = (df_copy['standardized_parking_complete'] == 'no parking').sum()\n",
        "\n",
        "        logger.log_info(\"\\n=== STANDARDIZATION RESULTS ===\")\n",
        "        logger.log_info(f\"Original null values: {original_nulls:,}\")\n",
        "        logger.log_info(f\"New null values created: {final_nulls - original_nulls:,}\")\n",
        "        logger.log_info(f\"Total 'no parking' designations: {no_parking_count:,}\")\n",
        "\n",
        "        # Log distribution of standardized values\n",
        "        logger.log_info(\"\\nTop 10 standardized parking types:\")\n",
        "        value_counts = df_copy['standardized_parking_complete'].value_counts()\n",
        "        for type_, count in value_counts.head(10).items():\n",
        "            percentage = (count/total_rows*100)\n",
        "            logger.log_info(f\"• {type_}: {count:,} properties ({percentage:.1f}%)\")\n",
        "\n",
        "        return df_copy\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_and_standardize_parking: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the analysis\n",
        "try:\n",
        "    logger.log_info(\"Starting comprehensive parking analysis...\")\n",
        "\n",
        "    # First verify the Parking column exists\n",
        "    if 'Parking' not in combined_data.columns:\n",
        "        raise ValueError(\"Required column 'Parking' not found in DataFrame\")\n",
        "\n",
        "    # Create backup of parking data\n",
        "    parking_backup = combined_data['Parking'].copy()\n",
        "    logger.log_info(\"Created backup of parking data\")\n",
        "\n",
        "    # First analyze 'none' values\n",
        "    none_count, none_uniques, none_frequencies = analyze_parking_none_values(combined_data)\n",
        "\n",
        "    # Then perform full standardization\n",
        "    combined_data = analyze_and_standardize_parking(combined_data)\n",
        "\n",
        "    logger.log_info(\"\\nParking analysis completed successfully! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"Parking analysis and standardization\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during parking analysis:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")\n",
        "\n",
        "    # Restore original data if there was an error\n",
        "    if 'parking_backup' in locals():\n",
        "        combined_data['Parking'] = parking_backup\n",
        "        logger.log_info(\"\\nRestored original parking values due to error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLZa6Ewe2xnd"
      },
      "source": [
        "# Data Cleaning: Commission Rate Pattern Recognition and Standardization\n",
        "\n",
        "## This needs work. There are still lots of OTHER type commissions that aren't getting caught by the algorithm.\n",
        "\n",
        "## Overview\n",
        "This code block implements an intelligent pattern recognition system to standardize real estate commission descriptions. It handles various text formats and patterns to extract and standardize commission rates, with special attention to the common 3.5/1.5 and 3.0/1.5 split commission structures.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input Processing\n",
        "1. Text Handling Functions\n",
        "   * Converts fractions to decimals (e.g., \"3 1/2%\" → \"3.5%\")\n",
        "   * Normalizes decimal formats (handles both '.' and ',' separators)\n",
        "   * Corrects common input errors (e.g., \"3/5%\" → \"3.5%\")\n",
        "   * Standardizes text variations (e.g., \"1st $100K\" → \"first $100K\")\n",
        "\n",
        "2. Pattern Recognition\n",
        "   * Common patterns (3.5/1.5 or 3.0/1.5)\n",
        "   * Simple split notation (x/y format)\n",
        "   * Complex text descriptions\n",
        "   * Mixed number formats\n",
        "\n",
        "### Key Components\n",
        "1. Pattern Identification\n",
        "   * Recognizes common commission structures\n",
        "   * Counts pattern frequencies\n",
        "   * Handles variations in notation\n",
        "   * Validates rate ranges (0-10%)\n",
        "\n",
        "2. Text Standardization\n",
        "   * Identifies commission percentages\n",
        "   * Handles split commission structures\n",
        "   * Standardizes to consistent format\n",
        "   * Preserves key commission terms\n",
        "\n",
        "3. Error Correction\n",
        "   * Fixes common typos\n",
        "   * Converts fraction notations\n",
        "   * Normalizes decimal formats\n",
        "   * Handles edge cases\n",
        "\n",
        "### Output Formats\n",
        "Standard patterns produced:\n",
        "* Split commission: \"x% on first $100K, y% on balance\"\n",
        "* Single rate: \"x%\"\n",
        "* Multiple rates: \"x%, y%, z%\"\n",
        "* Non-standard: \"Other\"\n",
        "* Missing data: \"Missing\"\n",
        "\n",
        "## Quality Controls\n",
        "* Minimum frequency thresholds\n",
        "* Rate range validation (0-10%)\n",
        "* Pattern consistency checks\n",
        "* Detailed logging of changes\n",
        "* Sample analysis of non-standard cases\n",
        "\n",
        "## Process Protection\n",
        "* Creates data backup\n",
        "* Validates input data\n",
        "* Handles errors gracefully\n",
        "* Restores original data if needed\n",
        "* Times processing duration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cOerIOUy2x6O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13686785-7815-465a-e47c-31a1573fe91c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing values in Commission column: 228220\n",
            "Percentage of missing values in Commission column: 57.97%\n",
            "\n",
            "Total number of unique Commission values (excluding NaN): 45232\n",
            "\n",
            "20 Sample Unique Commission values (excluding NaN):\n",
            "['3.5% First 100K and 1.5% Remainder of Sale'\n",
            " '3.5% on first $100,000 and 1.5% on the remaining'\n",
            " '3.5% on the first $100,000 & 1.5% on balance of SP'\n",
            " '3.5% ON THE FIRST $100K AND 1.5% ON THE BALANCE'\n",
            " '3.5% on the 1st $100,000 + 1.5% on the balance of Final Sale Price'\n",
            " '3.5% on the first $100,000 & 1.5% on the remaining balance of Sale'\n",
            " '3.5% on the first $100,000 + 1.5% on the balance of the sale price'\n",
            " '3.5% on the first $100,000 and 1.5% on the balance.'\n",
            " '3.5% on the first $100K and 1.5% on the balance. '\n",
            " '3.5% on the first $100,000 and 1.5% on the balance of sale price'\n",
            " '3.5% on the First $100k/1.5% on the Balance of the Sale Price Net of GST'\n",
            " '3.5% on 1st $100,000 & 1.5% on balance of sale price'\n",
            " '3.5% on the 1st $100K & 1.5% BOSP' '3.5 / 1.5' '3.5/1.5' '3.5%/1.5%'\n",
            " '3.5% on the first 100K and 1.5% on. the remainder'\n",
            " '3.5% on first $100k; 1.5% on BOSP'\n",
            " '3.5% on the first $100,000, 1.5% on the balance' '3.5% / 1.5%']\n"
          ]
        }
      ],
      "source": [
        "# Assuming your DataFrame is called 'combined_data'\n",
        "missing_commission_count = combined_data['Commission'].isnull().sum()\n",
        "total_commission_count = len(combined_data['Commission'])\n",
        "missing_commission_percentage = (missing_commission_count / total_commission_count) * 100\n",
        "\n",
        "print(f\"Number of missing values in Commission column: {missing_commission_count}\")\n",
        "print(f\"Percentage of missing values in Commission column: {missing_commission_percentage:.2f}%\")\n",
        "\n",
        "# Get unique values (excluding NaN)\n",
        "unique_commission_values = combined_data['Commission'].dropna().unique()\n",
        "total_unique_count = len(unique_commission_values)\n",
        "\n",
        "print(f\"\\nTotal number of unique Commission values (excluding NaN): {total_unique_count}\")\n",
        "\n",
        "# Show 20 sample unique values\n",
        "print(f\"\\n20 Sample Unique Commission values (excluding NaN):\\n{unique_commission_values[:20]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JueBN-wc5nYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd039b09-a017-4ae0-cb9a-9696fced92d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting commission standardization process...\n",
            "Created backup of commission data\n",
            "Processing 165,444 rows with commission data out of 393,664 total rows\n",
            "Starting commission standardization for 165,444 rows...\n",
            "Initial unique values: 45,232\n",
            "\n",
            "Analyzing 'Other' category (6,908 entries)\n",
            "\n",
            "Random sample of 'Other' entries:\n",
            "1. 3.5 /1.5\n",
            "2. 3.5/100000   1.5 Bal.\n",
            "3. 3.5-100,1.5\n",
            "4. 3.5 / 1.5\n",
            "5. 3.5/1,5\n",
            "6. 3.5/100 1.5/bal\n",
            "7. 3.0/100 1.5/Bal\n",
            "8. 3.5/100K 1.5/bal\n",
            "9. Seller will negotiate; $2 Listing Brokerage\n",
            "10. 3/100 1.5/Bal\n",
            "\n",
            "Processing completed in 2.13 seconds\n",
            "\n",
            "Commission standardization completed successfully! 🎉\n",
            "Step 14 completed: Commission standardization\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# COMMISSION ANALYSIS AND STANDARDIZATION\n",
        "# =================================================================================\n",
        "\n",
        "\n",
        "def identify_common_patterns(series, min_frequency=10):\n",
        "    \"\"\"\n",
        "    Identify common commission patterns in a pandas Series.\n",
        "    Uses defaultdict to efficiently count pattern occurrences.\n",
        "\n",
        "    Args:\n",
        "        series: pandas Series containing commission descriptions\n",
        "        min_frequency: minimum occurrences for a pattern to be considered common\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list of patterns, dict of pattern counts)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize defaultdict to count patterns\n",
        "        # defaultdict automatically creates a new entry with value 0 when a new key is encountered\n",
        "        pattern_counts = defaultdict(int)\n",
        "\n",
        "        # Process each non-null value\n",
        "        for text in series.dropna():\n",
        "            numbers = extract_commission_numbers(text)\n",
        "            if numbers:\n",
        "                pattern_counts[numbers] += 1\n",
        "\n",
        "        # Filter for patterns meeting minimum frequency\n",
        "        common_patterns = {\n",
        "            pattern: count for pattern, count in pattern_counts.items()\n",
        "            if count >= min_frequency\n",
        "        }\n",
        "\n",
        "        return list(common_patterns.keys()), common_patterns\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in identify_common_patterns: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def identify_common_commission_pattern(text):\n",
        "    \"\"\"\n",
        "    Checks for common commission patterns (3.5/1.5 or 3.0/1.5) in text.\n",
        "\n",
        "    Args:\n",
        "        text: Input text to analyze\n",
        "\n",
        "    Returns:\n",
        "        tuple or None: Commission rates if pattern found, None otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = str(text).lower()\n",
        "        commission_terms = ['%', 'percent', 'commission', 'balance', 'remainder', 'first']\n",
        "\n",
        "        # Check for 3.5/1.5 pattern\n",
        "        if '3.5' in text and '1.5' in text:\n",
        "            if any(word in text for word in commission_terms):\n",
        "                return (3.5, 1.5)\n",
        "\n",
        "        # Check for 3.0/1.5 pattern\n",
        "        if ('3.0' in text or ' 3 ' in text or '3%' in text) and '1.5' in text:\n",
        "            if any(word in text for word in commission_terms):\n",
        "                return (3.0, 1.5)\n",
        "\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in identify_common_commission_pattern: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def convert_fraction_to_decimal(text):\n",
        "    \"\"\"\n",
        "    Converts written fractions to decimal numbers.\n",
        "\n",
        "    Args:\n",
        "        text: Input text containing fractions\n",
        "\n",
        "    Returns:\n",
        "        str: Text with fractions converted to decimals\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = str(text)\n",
        "        text = re.sub(r'\\s*&\\s*', ' ', text)\n",
        "        mixed_pattern = r'(\\d+)[\\s-]*(\\d+)/(\\d+)'\n",
        "\n",
        "        def convert_mixed_number(match):\n",
        "            try:\n",
        "                whole = int(match.group(1))\n",
        "                numerator = int(match.group(2))\n",
        "                denominator = int(match.group(3))\n",
        "                if denominator == 0:\n",
        "                    return match.group(0)\n",
        "                result = whole + (numerator / denominator)\n",
        "                return f\"{result}\"\n",
        "            except (ValueError, ZeroDivisionError):\n",
        "                return match.group(0)\n",
        "\n",
        "        prev_text = None\n",
        "        while prev_text != text:\n",
        "            prev_text = text\n",
        "            text = re.sub(mixed_pattern, convert_mixed_number, text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in convert_fraction_to_decimal: {str(e)}\")\n",
        "        return text\n",
        "\n",
        "def normalize_decimal(text):\n",
        "    \"\"\"Normalizes decimal numbers by handling different formats.\"\"\"\n",
        "    try:\n",
        "        text = re.sub(r'(\\d+),(\\d+)(?=\\s*%?)', r'\\1.\\2', text)\n",
        "        text = re.sub(r'(\\d+)\\.\\s+(\\d+)', r'\\1.\\2', text)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in normalize_decimal: {str(e)}\")\n",
        "        return text\n",
        "\n",
        "def correct_common_errors(text):\n",
        "    \"\"\"Corrects common input errors in commission values.\"\"\"\n",
        "    try:\n",
        "        text = str(text)\n",
        "        corrections = [\n",
        "            (r'1/5%', '1.5%'),\n",
        "            (r'2/5%', '2.5%'),\n",
        "            (r'3/5%', '3.5%'),\n",
        "            (r'4/5%', '4.5%'),\n",
        "            (r'1/5\\b', '1.5'),\n",
        "            (r'2/5\\b', '2.5'),\n",
        "            (r'3/5\\b', '3.5'),\n",
        "            (r'4/5\\b', '4.5'),\n",
        "        ]\n",
        "\n",
        "        for pattern, replacement in corrections:\n",
        "            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in correct_common_errors: {str(e)}\")\n",
        "        return text\n",
        "\n",
        "def extract_commission_numbers(text):\n",
        "    \"\"\"\n",
        "    Extracts commission percentages from text.\n",
        "\n",
        "    Args:\n",
        "        text: Input text containing commission information\n",
        "\n",
        "    Returns:\n",
        "        tuple or None: Extracted commission rates if found\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Try common patterns first\n",
        "        common_pattern = identify_common_commission_pattern(text)\n",
        "        if common_pattern:\n",
        "            return common_pattern\n",
        "\n",
        "        # Handle x/y notation\n",
        "        simple_split = re.match(r'^(\\d+\\.?\\d*)/(\\d+\\.?\\d*)$', text.strip())\n",
        "        if simple_split:\n",
        "            try:\n",
        "                first = float(simple_split.group(1))\n",
        "                second = float(simple_split.group(2))\n",
        "                if 0 < first < 10 and 0 < second < 10:\n",
        "                    return (first, second)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        # Apply standard text processing\n",
        "        text = correct_common_errors(text)\n",
        "        text = convert_fraction_to_decimal(text)\n",
        "        text = normalize_decimal(text)\n",
        "        text = re.sub(r'\\b1st\\b|\\bfirst\\b|\\b1\\b(?=\\s*\\$?\\s*100)', 'first', text)\n",
        "\n",
        "        # Extract percentage numbers\n",
        "        numbers = []\n",
        "        pattern = r'(?:^|[^\\d.])(\\d+\\.?\\d*)\\s*(?:%|(?=\\s*(?:balance|remainder|of|on)))'\n",
        "\n",
        "        for match in re.finditer(pattern, text):\n",
        "            try:\n",
        "                num = float(match.group(1))\n",
        "                if 0 < num < 10:\n",
        "                    numbers.append(num)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        return tuple(numbers) if numbers else None\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in extract_commission_numbers: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def standardize_commission_column(df, commission_column, min_frequency=10):\n",
        "    \"\"\"\n",
        "    Main function for pattern matching standardization.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing commission data\n",
        "        commission_column: Name of commission column\n",
        "        min_frequency: Minimum frequency for pattern consideration\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with standardized commission values\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(f\"Starting commission standardization for {len(df):,} rows...\")\n",
        "        logger.log_info(f\"Initial unique values: {df[commission_column].nunique():,}\")\n",
        "\n",
        "        patterns, counts = identify_common_patterns(df[commission_column], min_frequency)\n",
        "\n",
        "        def standardize_value(text):\n",
        "            text = str(text).lower()\n",
        "            numbers = extract_commission_numbers(text)\n",
        "            if numbers and numbers in counts and counts[numbers] >= min_frequency:\n",
        "                if len(numbers) == 2:\n",
        "                    return f\"{numbers[0]}% on first $100K, {numbers[1]}% on balance\"\n",
        "                elif len(numbers) == 1:\n",
        "                    return f\"{numbers[0]}%\"\n",
        "                else:\n",
        "                    return f\"{', '.join(f'{n}%' for n in numbers)}\"\n",
        "            return 'Other'\n",
        "\n",
        "        df_result = df.copy()\n",
        "        df_result['standardized_commission'] = df[commission_column].apply(standardize_value)\n",
        "\n",
        "        return df_result\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in standardize_commission_column: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def analyze_other_category(df, original_column, standardized_column):\n",
        "    \"\"\"Analyzes entries that couldn't be standardized.\"\"\"\n",
        "    try:\n",
        "        other_entries = df[df[standardized_column] == 'Other'][original_column]\n",
        "\n",
        "        logger.log_info(f\"\\nAnalyzing 'Other' category ({len(other_entries):,} entries)\")\n",
        "\n",
        "        if len(other_entries) > 0:\n",
        "            sample = other_entries.sample(min(10, len(other_entries)))\n",
        "            logger.log_info(\"\\nRandom sample of 'Other' entries:\")\n",
        "            for idx, entry in enumerate(sample, 1):\n",
        "                logger.log_info(f\"{idx}. {entry}\")\n",
        "\n",
        "        return {'count': len(other_entries)}\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_other_category: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def pattern_matching_approach(df, commission_column, min_frequency=50):\n",
        "    \"\"\"\n",
        "    Pattern matching approach for commission standardization.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing commission data\n",
        "        commission_column: Name of commission column\n",
        "        min_frequency: Minimum frequency threshold\n",
        "\n",
        "    Returns:\n",
        "        tuple: (processed DataFrame, analysis results, validation results)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        valid_data = df[df[commission_column].notna()].copy()\n",
        "        logger.log_info(f\"Processing {len(valid_data):,} rows with commission data out of {len(df):,} total rows\")\n",
        "\n",
        "        processed_data = standardize_commission_column(valid_data, commission_column, min_frequency)\n",
        "\n",
        "        df_result = df.copy()\n",
        "        df_result['standardized_commission'] = 'Missing'\n",
        "        valid_indices = valid_data.index\n",
        "        df_result.loc[valid_indices, 'standardized_commission'] = processed_data['standardized_commission']\n",
        "\n",
        "        other_analysis = analyze_other_category(\n",
        "            df_result,\n",
        "            commission_column,\n",
        "            'standardized_commission'\n",
        "        )\n",
        "\n",
        "        end_time = time.time()\n",
        "        logger.log_info(f\"\\nProcessing completed in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "        return df_result, other_analysis, {}\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in pattern_matching_approach: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the analysis\n",
        "try:\n",
        "    logger.log_info(\"Starting commission standardization process...\")\n",
        "\n",
        "    # Create backup of commission data\n",
        "    commission_backup = combined_data['Commission'].copy()\n",
        "    logger.log_info(\"Created backup of commission data\")\n",
        "\n",
        "    # Run standardization\n",
        "    df_pattern, other_analysis, validation_results = pattern_matching_approach(combined_data, 'Commission')\n",
        "\n",
        "    # Update main DataFrame\n",
        "    combined_data = df_pattern\n",
        "\n",
        "    logger.log_info(\"\\nCommission standardization completed successfully! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"Commission standardization\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during commission standardization:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")\n",
        "\n",
        "    # Restore original data if there was an error\n",
        "    if 'commission_backup' in locals():\n",
        "        combined_data['Commission'] = commission_backup\n",
        "        logger.log_info(\"\\nRestored original commission values due to error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2hR9VRVPkFy"
      },
      "source": [
        "# Data Quality: Numeric Column Statistical Analysis\n",
        "\n",
        "## Overview\n",
        "This code block performs comprehensive statistical analysis on all numeric columns in the dataset. It identifies potential outliers, calculates key statistical measures, and flags columns that may need attention due to data quality issues.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Analysis Components\n",
        "1. Basic Statistics\n",
        "  * Mean (average value)\n",
        "  * Median (middle value)\n",
        "  * Standard Deviation (measure of spread)\n",
        "  * Minimum and Maximum values\n",
        "  * Missing value counts\n",
        "\n",
        "2. Outlier Detection\n",
        "  * Uses Interquartile Range (IQR) method\n",
        "  * Calculates Q1 (25th percentile) and Q3 (75th percentile)\n",
        "  * Sets boundaries at Q1 - 1.5*IQR and Q3 + 1.5*IQR\n",
        "  * Identifies values outside these boundaries\n",
        "  * Reports outlier counts and percentages\n",
        "\n",
        "3. Distribution Analysis\n",
        "  * Calculates skewness (measure of asymmetry)\n",
        "  * Identifies extreme values\n",
        "  * Provides sample outlier values\n",
        "  * Shows value ranges\n",
        "\n",
        "### Process Steps\n",
        "1. Column Identification\n",
        "  * Finds all numeric columns (int64 and float64)\n",
        "  * Excludes non-numeric data\n",
        "  * Processes each column separately\n",
        "\n",
        "2. Statistical Calculation\n",
        "  * Handles missing values appropriately\n",
        "  * Computes comprehensive statistics\n",
        "  * Identifies outliers systematically\n",
        "  * Stores results for comparison\n",
        "\n",
        "3. Results Reporting\n",
        "  * Ranks columns by outlier percentage\n",
        "  * Shows detailed statistics per column\n",
        "  * Provides sample outlier values\n",
        "  * Highlights problematic columns\n",
        "\n",
        "### Quality Metrics\n",
        "For each numeric column:\n",
        "* Total value count\n",
        "* Missing value count\n",
        "* Outlier count and percentage\n",
        "* Value range\n",
        "* Distribution characteristics\n",
        "* Sample outlier values\n",
        "\n",
        "## Results Format\n",
        "The code provides:\n",
        "1. Summary Overview\n",
        "  * Total columns analyzed\n",
        "  * Columns ranked by outlier percentage\n",
        "  * Missing value counts\n",
        "\n",
        "2. Detailed Statistics\n",
        "  * Full statistical breakdown\n",
        "  * Outlier information\n",
        "  * Distribution measures\n",
        "  * Value ranges\n",
        "\n",
        "3. Top Problems\n",
        "  * Most problematic columns\n",
        "  * Detailed statistics for top 5\n",
        "  * Sample outlier values\n",
        "  * Missing value counts\n",
        "\n",
        "[Code block follows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LytJWYJJPsks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6eec68e-28c1-4e78-9a61-dc5d139ae557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing numeric columns for potential data quality issues...\n",
            "\n",
            "Numeric Column Analysis Summary:\n",
            "Total numeric columns analyzed: 11\n",
            "\n",
            "Columns ranked by percentage of outliers:\n",
            "\n",
            "Latitude:\n",
            "  Outliers: 29,125 (7.41%)\n",
            "  Range: -34.637512 to 71.65898\n",
            "  Missing Values: 532\n",
            "  Sample outlier values: [50.566401, 50.588089, 52.234015, 52.242444, 52.289004]\n",
            "\n",
            "Original_List_Price:\n",
            "  Outliers: 18,009 (4.58%)\n",
            "  Range: 1399.0 to 84900000.0\n",
            "  Missing Values: 364\n",
            "  Sample outlier values: [3400000.0, 3700000.0, 3750000.0, 4750000.0, 4800000.0]\n",
            "\n",
            "Close_Price:\n",
            "  Outliers: 16,758 (4.26%)\n",
            "  Range: 2500.0 to 11100000.0\n",
            "  Missing Values: 4\n",
            "  Sample outlier values: [3350000.0, 3450000.0, 3700000.0, 4650000.0, 4700000.0]\n",
            "\n",
            "Price_Per_SqFt:\n",
            "  Outliers: 15,581 (3.97%)\n",
            "  Range: 0.2363083164300203 to 370000.0\n",
            "  Missing Values: 897\n",
            "  Sample outlier values: [658.5879873551106, 776.9423558897244, 820.4518430439953, 1058.5585585585586, 1219.1103789126853]\n",
            "\n",
            "Longitude:\n",
            "  Outliers: 10,991 (2.80%)\n",
            "  Range: -129.681213 to 151.209438\n",
            "  Missing Values: 532\n",
            "  Sample outlier values: [-114.516012, -114.496963, -114.474973, -114.46318, -114.46184]\n",
            "\n",
            "Total_Baths:\n",
            "  Outliers: 7,069 (1.80%)\n",
            "  Range: 0.0 to 82.0\n",
            "  Missing Values: 3\n",
            "  Sample outlier values: [5.0, 5.0, 5.0, 6.0, 6.0]\n",
            "\n",
            "Bedrooms_Below_Grade:\n",
            "  Outliers: 2,689 (1.73%)\n",
            "  Range: 0.0 to 21.0\n",
            "  Missing Values: 237,930\n",
            "  Sample outlier values: [3.0, 3.0, 3.0, 3.0, 4.0]\n",
            "\n",
            "RMS_Total:\n",
            "  Outliers: 5,955 (1.52%)\n",
            "  Range: 1.0 to 20910.0\n",
            "  Missing Values: 897\n",
            "  Sample outlier values: [3035.0, 3542.0, 4205.0, 4440.0, 5985.0]\n",
            "\n",
            "Year_Built:\n",
            "  Outliers: 5,514 (1.40%)\n",
            "  Range: 0.0 to 191980.0\n",
            "  Missing Values: 86\n",
            "  Sample outlier values: [1912.0, 1912.0, 1929.0, 1930.0, 1930.0]\n",
            "\n",
            "Bedrooms_Above_Grade:\n",
            "  Outliers: 4,579 (1.19%)\n",
            "  Range: 0.0 to 12.0\n",
            "  Missing Values: 8,181\n",
            "  Sample outlier values: [5.0, 5.0, 5.0, 5.0, 5.0]\n",
            "\n",
            "Full_Baths:\n",
            "  Outliers: 2,442 (0.63%)\n",
            "  Range: 0.0 to 82.0\n",
            "  Missing Values: 8,628\n",
            "  Sample outlier values: [5.0, 5.0, 5.0, 5.0, 6.0]\n",
            "\n",
            "Detailed Analysis of Top 5 Problematic Columns:\n",
            "                 Column  Outlier %       Mean     Median  Minimum      Maximum\n",
            "8              Latitude      7.408     51.106     51.057  -34.638       71.659\n",
            "0   Original_List_Price      4.579 489348.754 425000.000 1399.000 84900000.000\n",
            "1           Close_Price      4.257 468317.118 415000.000 2500.000 11100000.000\n",
            "10       Price_Per_SqFt      3.967    332.600    305.776    0.236   370000.000\n",
            "9             Longitude      2.796   -114.029   -114.067 -129.681      151.209\n"
          ]
        }
      ],
      "source": [
        "def analyze_numeric_columns(df):\n",
        "    \"\"\"\n",
        "    Performs a detailed analysis of all numeric columns in the dataset,\n",
        "    identifying potential outliers and data quality issues.\n",
        "\n",
        "    This function examines each numeric column to understand:\n",
        "    - Basic statistics (mean, median, standard deviation)\n",
        "    - Number and percentage of outliers using different methods\n",
        "    - Extreme values that might indicate data errors\n",
        "    - Distribution characteristics\n",
        "    \"\"\"\n",
        "    print(\"Analyzing numeric columns for potential data quality issues...\")\n",
        "\n",
        "    # First, identify numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    # Create a DataFrame to store our analysis results\n",
        "    analysis_results = []\n",
        "\n",
        "    for column in numeric_cols:\n",
        "        # Get column data, dropping any NaN values\n",
        "        data = df[column].dropna()\n",
        "\n",
        "        # Calculate basic statistics\n",
        "        mean = data.mean()\n",
        "        median = data.median()\n",
        "        std = data.std()\n",
        "\n",
        "        # Calculate quartiles and IQR\n",
        "        q1 = data.quantile(0.25)\n",
        "        q3 = data.quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "\n",
        "        # Define outlier boundaries\n",
        "        lower_bound = q1 - (1.5 * iqr)\n",
        "        upper_bound = q3 + (1.5 * iqr)\n",
        "\n",
        "        # Count outliers\n",
        "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "        outlier_count = len(outliers)\n",
        "        outlier_percentage = (outlier_count / len(data)) * 100\n",
        "\n",
        "        # Get extreme values\n",
        "        min_val = data.min()\n",
        "        max_val = data.max()\n",
        "\n",
        "        # Calculate skewness to understand distribution\n",
        "        skewness = data.skew()\n",
        "\n",
        "        # Store results\n",
        "        analysis_results.append({\n",
        "            'Column': column,\n",
        "            'Total Values': len(data),\n",
        "            'Missing Values': df[column].isna().sum(),\n",
        "            'Mean': mean,\n",
        "            'Median': median,\n",
        "            'Std Dev': std,\n",
        "            'Minimum': min_val,\n",
        "            'Maximum': max_val,\n",
        "            'Outliers': outlier_count,\n",
        "            'Outlier %': outlier_percentage,\n",
        "            'Skewness': skewness,\n",
        "            'Sample Outliers': sorted(outliers.head().tolist()) if len(outliers) > 0 else []\n",
        "        })\n",
        "\n",
        "    # Convert results to DataFrame for easy viewing\n",
        "    results_df = pd.DataFrame(analysis_results)\n",
        "\n",
        "    # Sort by outlier percentage to identify most problematic columns\n",
        "    results_df = results_df.sort_values('Outlier %', ascending=False)\n",
        "\n",
        "    print(\"\\nNumeric Column Analysis Summary:\")\n",
        "    print(f\"Total numeric columns analyzed: {len(numeric_cols)}\")\n",
        "\n",
        "    print(\"\\nColumns ranked by percentage of outliers:\")\n",
        "    for _, row in results_df.iterrows():\n",
        "        print(f\"\\n{row['Column']}:\")\n",
        "        print(f\"  Outliers: {row['Outliers']:,} ({row['Outlier %']:.2f}%)\")\n",
        "        print(f\"  Range: {row['Minimum']} to {row['Maximum']}\")\n",
        "        print(f\"  Missing Values: {row['Missing Values']:,}\")\n",
        "        if len(row['Sample Outliers']) > 0:\n",
        "            print(f\"  Sample outlier values: {row['Sample Outliers']}\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Run the analysis on our dataset\n",
        "numeric_analysis = analyze_numeric_columns(combined_data)\n",
        "\n",
        "# Print detailed statistics for the top 5 most problematic columns\n",
        "print(\"\\nDetailed Analysis of Top 5 Problematic Columns:\")\n",
        "print(numeric_analysis[['Column', 'Outlier %', 'Mean', 'Median', 'Minimum', 'Maximum']].head().to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1umsohHVg66"
      },
      "source": [
        "# Data Validation: Geographic Coordinate Analysis by City\n",
        "\n",
        "## Overview\n",
        "This code block performs city-specific analysis of property coordinates to identify potential errors in latitude and longitude values. It uses statistical methods to establish expected coordinate ranges for each city and flags properties with coordinates that fall outside these ranges.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input Requirements\n",
        "* Required Columns:\n",
        "  * City: Location identifier\n",
        "  * Latitude: Property's north-south position\n",
        "  * Longitude: Property's east-west position\n",
        "\n",
        "### Analysis Process\n",
        "1. City-Level Statistics\n",
        "   * Counts properties per city\n",
        "   * Calculates coordinate ranges\n",
        "   * Computes basic statistics:\n",
        "     * Mean coordinates\n",
        "     * Median coordinates\n",
        "     * Minimum/maximum values\n",
        "\n",
        "2. Boundary Calculation\n",
        "   * Uses IQR (Interquartile Range) method for each city:\n",
        "     * Calculates Q1 (25th percentile)\n",
        "     * Calculates Q3 (75th percentile)\n",
        "     * Computes IQR = Q3 - Q1\n",
        "     * Sets boundaries at Q1 - 1.5*IQR and Q3 + 1.5*IQR\n",
        "   * Creates separate boundaries for:\n",
        "     * Latitude values\n",
        "     * Longitude values\n",
        "\n",
        "3. Outlier Detection\n",
        "   * Identifies properties with coordinates:\n",
        "     * Below minimum boundaries\n",
        "     * Above maximum boundaries\n",
        "     * Missing values (null)\n",
        "   * Groups outliers by city\n",
        "   * Reports extreme values\n",
        "\n",
        "### Quality Checks\n",
        "The code verifies and reports:\n",
        "* Data completeness (missing values)\n",
        "* Data types of coordinate columns\n",
        "* Coordinate ranges by city\n",
        "* Number of outliers found\n",
        "* Distribution of outliers across cities\n",
        "\n",
        "## Process Protection\n",
        "* Validates required columns\n",
        "* Creates coordinate backup\n",
        "* Handles errors gracefully\n",
        "* Restores original data if needed\n",
        "* Provides detailed logging\n",
        "\n",
        "## Output Details\n",
        "1. Initial Analysis:\n",
        "   * Property counts by city\n",
        "   * Missing coordinate counts\n",
        "   * Data type verification\n",
        "\n",
        "2. Statistical Summary:\n",
        "   * City-specific coordinate ranges\n",
        "   * Outlier counts per city\n",
        "   * Extreme value analysis\n",
        "\n",
        "3. Problem Properties:\n",
        "   * List of outlier properties\n",
        "   * City boundary definitions\n",
        "   * Detailed outlier statistics\n",
        "\n",
        "[Code block follows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PvghpTmXXuvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91160c2e-f171-41d3-c5b9-8a553c402186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting coordinate analysis process...\n",
            "Created backup of coordinate data\n",
            "\n",
            "Coordinate column data types:\n",
            "Latitude:  float64\n",
            "Longitude: float64\n",
            "\n",
            "Missing coordinate counts:\n",
            "Latitude:  532 missing values\n",
            "Longitude: 532 missing values\n",
            "Starting regional coordinate analysis...\n",
            "\n",
            "Properties by City:\n",
            "Calgary             : 322,587 properties\n",
            "Red Deer            : 24,638 properties\n",
            "Airdrie             : 22,482 properties\n",
            "Cochrane            : 10,619 properties\n",
            "Okotoks             : 9,236 properties\n",
            "High River          : 4,102 properties\n",
            "\n",
            "Calculating coordinate statistics by city...\n",
            "\n",
            "Coordinate Statistics by City:\n",
            "\n",
            "Airdrie:\n",
            "Latitude range:  26.77 to 58.0109\n",
            "Longitude range: -127.2763 to -0.376\n",
            "\n",
            "Calgary:\n",
            "Latitude range:  -34.6375 to 58.1857\n",
            "Longitude range: -127.7564 to 151.2094\n",
            "\n",
            "Cochrane:\n",
            "Latitude range:  26.9147 to 71.659\n",
            "Longitude range: -123.0844 to -69.8599\n",
            "\n",
            "High River:\n",
            "Latitude range:  29.8913 to 52.2343\n",
            "Longitude range: -122.3388 to -72.8832\n",
            "\n",
            "Okotoks:\n",
            "Latitude range:  -32.0439 to 51.8113\n",
            "Longitude range: -124.1646 to 115.997\n",
            "\n",
            "Red Deer:\n",
            "Latitude range:  32.4491 to 58.1333\n",
            "Longitude range: -129.6812 to -92.0532\n",
            "\n",
            "Calculating city-specific coordinate boundaries...\n",
            "Identifying coordinate outliers...\n",
            "\n",
            "Found 1,116 properties with coordinates outside their city's normal range\n",
            "\n",
            "Outliers by City:\n",
            "Calgary             : 704 outliers\n",
            "Airdrie             : 189 outliers\n",
            "Cochrane            : 99 outliers\n",
            "Okotoks             : 68 outliers\n",
            "Red Deer            : 31 outliers\n",
            "High River          : 25 outliers\n",
            "\n",
            "Extreme Coordinate Analysis by City:\n",
            "\n",
            "Calgary:\n",
            "Latitude range:  -34.6375 to 58.1857\n",
            "Longitude range: -127.7564 to 151.2094\n",
            "\n",
            "Okotoks:\n",
            "Latitude range:  -32.0439 to 51.8113\n",
            "Longitude range: -124.1646 to 115.9970\n",
            "\n",
            "Airdrie:\n",
            "Latitude range:  26.7700 to 58.0109\n",
            "Longitude range: -127.2763 to -0.3760\n",
            "\n",
            "Cochrane:\n",
            "Latitude range:  26.9147 to 71.6590\n",
            "Longitude range: -123.0844 to -69.8599\n",
            "\n",
            "Red Deer:\n",
            "Latitude range:  32.4491 to 58.1333\n",
            "Longitude range: -129.6812 to -92.0532\n",
            "\n",
            "High River:\n",
            "Latitude range:  29.8913 to 52.2343\n",
            "Longitude range: -122.3388 to -72.8832\n",
            "\n",
            "Coordinate analysis completed successfully! 🎉\n",
            "Step 15 completed: Coordinate analysis\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# COORDINATE ANALYSIS AND VALIDATION\n",
        "# =================================================================================\n",
        "\n",
        "def analyze_regional_coordinates(df):\n",
        "    \"\"\"\n",
        "    Analyzes coordinate data by city to identify potential errors and outliers.\n",
        "\n",
        "    This function:\n",
        "    1. Identifies which cities are present in the data\n",
        "    2. Calculates typical coordinate ranges for each city\n",
        "    3. Identifies problematic coordinates using statistical methods\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame containing property data with coordinates\n",
        "\n",
        "    Returns:\n",
        "        tuple: (DataFrame of outlier properties, dict of city boundaries)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Input validation\n",
        "        required_columns = ['City', 'Latitude', 'Longitude']\n",
        "        for col in required_columns:\n",
        "            if col not in df.columns:\n",
        "                raise ValueError(f\"Required column '{col}' not found in DataFrame\")\n",
        "\n",
        "        logger.log_info(\"Starting regional coordinate analysis...\")\n",
        "\n",
        "        # Analyze city distribution\n",
        "        city_counts = df['City'].value_counts()\n",
        "        logger.log_info(\"\\nProperties by City:\")\n",
        "        for city, count in city_counts.items():\n",
        "            logger.log_info(f\"{city:<20}: {count:,} properties\")\n",
        "\n",
        "        # Calculate coordinate statistics for each city\n",
        "        logger.log_info(\"\\nCalculating coordinate statistics by city...\")\n",
        "        city_stats = df.groupby('City').agg({\n",
        "            'Latitude': ['count', 'mean', 'median', 'min', 'max'],\n",
        "            'Longitude': ['count', 'mean', 'median', 'min', 'max']\n",
        "        }).round(4)\n",
        "\n",
        "        # Log summary statistics for each city\n",
        "        logger.log_info(\"\\nCoordinate Statistics by City:\")\n",
        "        for city in city_stats.index:\n",
        "            logger.log_info(f\"\\n{city}:\")\n",
        "            logger.log_info(f\"Latitude range:  {city_stats.loc[city, ('Latitude', 'min')]} to {city_stats.loc[city, ('Latitude', 'max')]}\")\n",
        "            logger.log_info(f\"Longitude range: {city_stats.loc[city, ('Longitude', 'min')]} to {city_stats.loc[city, ('Longitude', 'max')]}\")\n",
        "\n",
        "        # Calculate city-specific coordinate boundaries using IQR method\n",
        "        logger.log_info(\"\\nCalculating city-specific coordinate boundaries...\")\n",
        "        city_boundaries = {}\n",
        "\n",
        "        for city in df['City'].unique():\n",
        "            city_data = df[df['City'] == city]\n",
        "\n",
        "            # Calculate latitude boundaries using IQR method\n",
        "            lat_q1 = city_data['Latitude'].quantile(0.25)\n",
        "            lat_q3 = city_data['Latitude'].quantile(0.75)\n",
        "            lat_iqr = lat_q3 - lat_q1\n",
        "            lat_min = lat_q1 - (1.5 * lat_iqr)\n",
        "            lat_max = lat_q3 + (1.5 * lat_iqr)\n",
        "\n",
        "            # Calculate longitude boundaries using IQR method\n",
        "            lon_q1 = city_data['Longitude'].quantile(0.25)\n",
        "            lon_q3 = city_data['Longitude'].quantile(0.75)\n",
        "            lon_iqr = lon_q3 - lon_q1\n",
        "            lon_min = lon_q1 - (1.5 * lon_iqr)\n",
        "            lon_max = lon_q3 + (1.5 * lon_iqr)\n",
        "\n",
        "            city_boundaries[city] = {\n",
        "                'lat_min': lat_min,\n",
        "                'lat_max': lat_max,\n",
        "                'lon_min': lon_min,\n",
        "                'lon_max': lon_max\n",
        "            }\n",
        "\n",
        "        # Identify outliers based on city-specific boundaries\n",
        "        logger.log_info(\"Identifying coordinate outliers...\")\n",
        "        outliers = pd.DataFrame()\n",
        "\n",
        "        for city in df['City'].unique():\n",
        "            city_data = df[df['City'] == city].copy()\n",
        "            bounds = city_boundaries[city]\n",
        "\n",
        "            # Find properties outside the normal range for their city\n",
        "            city_outliers = city_data[\n",
        "                (city_data['Latitude'] < bounds['lat_min']) |\n",
        "                (city_data['Latitude'] > bounds['lat_max']) |\n",
        "                (city_data['Longitude'] < bounds['lon_min']) |\n",
        "                (city_data['Longitude'] > bounds['lon_max']) |\n",
        "                city_data['Latitude'].isna() |\n",
        "                city_data['Longitude'].isna()\n",
        "            ]\n",
        "\n",
        "            outliers = pd.concat([outliers, city_outliers])\n",
        "\n",
        "        # Log outlier analysis results\n",
        "        logger.log_info(f\"\\nFound {len(outliers):,} properties with coordinates outside their city's normal range\")\n",
        "\n",
        "        # Analyze outliers by city\n",
        "        outlier_by_city = outliers['City'].value_counts()\n",
        "        logger.log_info(\"\\nOutliers by City:\")\n",
        "        for city, count in outlier_by_city.items():\n",
        "            logger.log_info(f\"{city:<20}: {count:,} outliers\")\n",
        "\n",
        "        # Analyze extreme coordinate values\n",
        "        logger.log_info(\"\\nExtreme Coordinate Analysis by City:\")\n",
        "        for city in outliers['City'].unique():\n",
        "            city_outliers = outliers[outliers['City'] == city]\n",
        "            logger.log_info(f\"\\n{city}:\")\n",
        "            logger.log_info(f\"Latitude range:  {city_outliers['Latitude'].min():.4f} to {city_outliers['Latitude'].max():.4f}\")\n",
        "            logger.log_info(f\"Longitude range: {city_outliers['Longitude'].min():.4f} to {city_outliers['Longitude'].max():.4f}\")\n",
        "\n",
        "        return outliers, city_boundaries\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_regional_coordinates: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the analysis\n",
        "try:\n",
        "    logger.log_info(\"Starting coordinate analysis process...\")\n",
        "\n",
        "    # Create backup of coordinate data\n",
        "    coordinate_backup = combined_data[['Latitude', 'Longitude']].copy()\n",
        "    logger.log_info(\"Created backup of coordinate data\")\n",
        "\n",
        "    # Verify data types\n",
        "    logger.log_info(\"\\nCoordinate column data types:\")\n",
        "    logger.log_info(f\"Latitude:  {combined_data['Latitude'].dtype}\")\n",
        "    logger.log_info(f\"Longitude: {combined_data['Longitude'].dtype}\")\n",
        "\n",
        "    # Check for missing coordinates\n",
        "    missing_lat = combined_data['Latitude'].isna().sum()\n",
        "    missing_lon = combined_data['Longitude'].isna().sum()\n",
        "    logger.log_info(\"\\nMissing coordinate counts:\")\n",
        "    logger.log_info(f\"Latitude:  {missing_lat:,} missing values\")\n",
        "    logger.log_info(f\"Longitude: {missing_lon:,} missing values\")\n",
        "\n",
        "    # Run the coordinate analysis\n",
        "    problem_properties, city_boundaries = analyze_regional_coordinates(combined_data)\n",
        "\n",
        "    logger.log_info(\"\\nCoordinate analysis completed successfully! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"Coordinate analysis\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during coordinate analysis:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")\n",
        "\n",
        "    # Restore original data if there was an error\n",
        "    if 'coordinate_backup' in locals():\n",
        "        combined_data[['Latitude', 'Longitude']] = coordinate_backup\n",
        "        logger.log_info(\"\\nRestored original coordinate values due to error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdz3tLZBHKsj"
      },
      "source": [
        "# Data Validation: Geographic Coordinate Analysis by City\n",
        "\n",
        "## Overview\n",
        "This code block performs city-specific analysis of property coordinates to identify potential errors in latitude and longitude values. It uses statistical methods to establish expected coordinate ranges for each city and flags properties with coordinates that fall outside these ranges.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input Requirements\n",
        "* Required Columns:\n",
        "  * City: Location identifier\n",
        "  * Latitude: Property's north-south position\n",
        "  * Longitude: Property's east-west position\n",
        "\n",
        "### Analysis Process\n",
        "1. City-Level Statistics\n",
        "   * Counts properties per city\n",
        "   * Calculates coordinate ranges\n",
        "   * Computes basic statistics:\n",
        "     * Mean coordinates\n",
        "     * Median coordinates\n",
        "     * Minimum/maximum values\n",
        "\n",
        "2. Boundary Calculation\n",
        "   * Uses IQR (Interquartile Range) method for each city:\n",
        "     * Calculates Q1 (25th percentile)\n",
        "     * Calculates Q3 (75th percentile)\n",
        "     * Computes IQR = Q3 - Q1\n",
        "     * Sets boundaries at Q1 - 1.5*IQR and Q3 + 1.5*IQR\n",
        "   * Creates separate boundaries for:\n",
        "     * Latitude values\n",
        "     * Longitude values\n",
        "\n",
        "3. Outlier Detection\n",
        "   * Identifies properties with coordinates:\n",
        "     * Below minimum boundaries\n",
        "     * Above maximum boundaries\n",
        "     * Missing values (null)\n",
        "   * Groups outliers by city\n",
        "   * Reports extreme values\n",
        "\n",
        "### Quality Checks\n",
        "The code verifies and reports:\n",
        "* Data completeness (missing values)\n",
        "* Data types of coordinate columns\n",
        "* Coordinate ranges by city\n",
        "* Number of outliers found\n",
        "* Distribution of outliers across cities\n",
        "\n",
        "## Process Protection\n",
        "* Validates required columns\n",
        "* Creates coordinate backup\n",
        "* Handles errors gracefully\n",
        "* Restores original data if needed\n",
        "* Provides detailed logging\n",
        "\n",
        "## Output Details\n",
        "1. Initial Analysis:\n",
        "   * Property counts by city\n",
        "   * Missing coordinate counts\n",
        "   * Data type verification\n",
        "\n",
        "2. Statistical Summary:\n",
        "   * City-specific coordinate ranges\n",
        "   * Outlier counts per city\n",
        "   * Extreme value analysis\n",
        "\n",
        "3. Problem Properties:\n",
        "   * List of outlier properties\n",
        "   * City boundary definitions\n",
        "   * Detailed outlier statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6OoPplzaYmc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c91768a07dea427e9eae26029b661b16",
            "be08d8dd490d45b1b99882e95b2576a6",
            "d11ba07bf32f43bc9c344a180809df1c",
            "ccafa9da58f04c838c95879d2c16deb6",
            "beea3cbadc754516b386f5fb30fb4f9b",
            "930e36293a3c4d9cb7ec6c335095a548",
            "795e6734ac9c48c79fcd91c0b8ba9846",
            "45a4fc854cad4cb5965af3897251e128",
            "bc16ff194ba74cd4b27bbf2f3fdceffc",
            "1f60851e687447e0bb8f2381a7af158c",
            "fc6cb8e7c8f54992b04c3faa841f1df8",
            "0af6cae12e03416a96f4cfa2d1194672",
            "d2e608f90f0541d6b16ff7cd9d82a57b",
            "e0ec4ad4ee4c42f083bd9952728c101f",
            "e26aed9c10654302ae3e016172421bb0",
            "4686df10069c48c79502f1210f5bef41",
            "b0aa73345b3f4478be0a9961bde44f05",
            "91941451e1bf4184906e48712f70d09f",
            "a7d16fa724e34d8a9898a077f10fc334",
            "b7849d39befe4cbaa56e3e1afad6f424",
            "c71d97861b624630bd4ca6bd1f06efb9",
            "567d8ba44acb4640a2aa14650cb37bc7",
            "8f8f89d811484d9e8dc01939936b0507",
            "4ce9cb1ebecf47abb83577a474dbc133",
            "49ac48f7db92418f9c55da459587fc35",
            "1ce07207849a4b01903f428665d21d57",
            "a5cb1d5d4a144a4f84fa216e8db1ef74",
            "0db098ec4c3245d0a921d0c73a89ba78",
            "8f86c383fb1a415090de413403f651a5",
            "b63c3cf70c234e438c09af095f117e3b",
            "3743ba26f4c54bdfad2b2d01b21d5ecb",
            "fac2c722d178430691ad199dbc178436",
            "4aed070d3e30403cb35e3efaaf642165",
            "b139a95017204b50858dc75c81ceef0d",
            "163e4d9fee0543f7ac6599f392ab1766",
            "f01e9805bd49493ebe61e7e13356fc3a",
            "5636888b426840afa0ea2a893a742a86",
            "4912116e7f8b4779a5cf9adb70fca343",
            "30fcd308e0ab4a84b48cf3a6af487d2b",
            "3540a0c106f54a6796bad9df8c8bdae8",
            "6bd556317869431eaff7a7f14824bf7d",
            "816c9fba649a40d6a9805e13c54e486d",
            "10eb3a3b96bc41f7847e9fb3e11fd104",
            "0c038b7a75b24e588542d417c0851c64",
            "f947a4ef5ecf4b8cbad2e838fa760348",
            "f8e0258542244c1bab45ca25f93f0e8b",
            "a5a66cdf6df14e3ca788ec29c6b2242a",
            "a8305f740a074480b05deaaa18ef8f23",
            "ac4c7c97d9304a5cad09104d40c1dea5",
            "a7b50062620c42e3aba48db2f49b8083",
            "ee756010c69e4d6daafc3ab434ca0f19",
            "11be7334f6f64455be2905f7b0d11a58",
            "8badf1afb6234bddb5b2f4e0da8b00c4",
            "aa66bc2ae01446c09c091a949d6e1d9d",
            "5592e8daf90d43da903aa1742060e384",
            "3f00865bd5954eb482b5ae1d6ae43cef",
            "78bc8fbd12194e95831f9f96a7f073a6",
            "0a01b87abc784d56b54d5768027d0128",
            "255aed489d9c419e8609dde3fecd7fc9",
            "b9392babcf2b42cebc61a5eccdf50da4",
            "76ff17a3b8e04438b5f64c0e79d64cce",
            "53f8be3f9f3a470698386553b47dcbb5",
            "444fa3874cef42549e2ae379b73b0f83",
            "7e06ec02cc1240d780ecc50b6b99d2ad",
            "0012cebc355942718ea8ea5bbbc1bd7d",
            "6f71500f3dbc4a83acc34c1882c58318"
          ]
        },
        "outputId": "ad0208c3-9c89-42b3-c063-438cb7f46619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting coordinate cleaning process...\n",
            "Created backup of coordinate data\n",
            "Starting coordinate cleaning process...\n",
            "Created working copy of DataFrame\n",
            "\n",
            "Processing Calgary...\n",
            "Found 704 properties with invalid coordinates in Calgary\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Calgary outliers:   0%|          | 0/704 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c91768a07dea427e9eae26029b661b16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Okotoks...\n",
            "Found 68 properties with invalid coordinates in Okotoks\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Okotoks outliers:   0%|          | 0/68 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0af6cae12e03416a96f4cfa2d1194672"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Airdrie...\n",
            "Found 189 properties with invalid coordinates in Airdrie\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Airdrie outliers:   0%|          | 0/189 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f8f89d811484d9e8dc01939936b0507"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Cochrane...\n",
            "Found 99 properties with invalid coordinates in Cochrane\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Cochrane outliers:   0%|          | 0/99 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b139a95017204b50858dc75c81ceef0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Red Deer...\n",
            "Found 31 properties with invalid coordinates in Red Deer\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Red Deer outliers:   0%|          | 0/31 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f947a4ef5ecf4b8cbad2e838fa760348"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing High River...\n",
            "Found 25 properties with invalid coordinates in High River\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing High River outliers:   0%|          | 0/25 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f00865bd5954eb482b5ae1d6ae43cef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Coordinate Cleaning Summary:\n",
            "--------------------------------------------------\n",
            "Total properties processed: 393,664\n",
            "Fixed using postal code centers: 760\n",
            "Fixed using subdivision centers: 350\n",
            "Unable to fix: 6\n",
            "\n",
            "Verification by City:\n",
            "--------------------------------------------------\n",
            "Calgary: 0 remaining invalid coordinates\n",
            "Okotoks: 0 remaining invalid coordinates\n",
            "Airdrie: 6 remaining invalid coordinates\n",
            "Cochrane: 0 remaining invalid coordinates\n",
            "Red Deer: 0 remaining invalid coordinates\n",
            "High River: 0 remaining invalid coordinates\n",
            "\n",
            "Final Coordinate Check:\n",
            "--------------------------------------------------\n",
            "Remaining null Latitude: 0\n",
            "Remaining null Longitude: 0\n",
            "\n",
            "Coordinate cleaning completed successfully! 🎉\n",
            "Step 16 completed: Coordinate cleaning\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# COORDINATE CLEANING AND CORRECTION\n",
        "# =================================================================================\n",
        "\n",
        "def clean_regional_coordinates(df, city_boundaries):\n",
        "    \"\"\"\n",
        "    Cleans latitude and longitude data based on city-specific boundaries and patterns.\n",
        "\n",
        "    This function acts like a local real estate expert, using postal codes and subdivisions\n",
        "    to correct coordinates that fall outside expected ranges. It maintains geographic\n",
        "    accuracy by processing each city separately.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame containing property data\n",
        "        city_boundaries (dict): Dictionary of coordinate boundaries for each city\n",
        "\n",
        "    Returns:\n",
        "        tuple: (DataFrame with cleaned coordinates, dict with summary of changes)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting coordinate cleaning process...\")\n",
        "\n",
        "        # Input validation\n",
        "        required_columns = ['City', 'Latitude', 'Longitude', 'Postal_Code', 'Subdivision_Name', 'MLS_Num']\n",
        "        for col in required_columns:\n",
        "            if col not in df.columns:\n",
        "                raise ValueError(f\"Required column '{col}' not found in DataFrame\")\n",
        "\n",
        "        # Create a working copy of our data\n",
        "        df_clean = df.copy()\n",
        "        logger.log_info(\"Created working copy of DataFrame\")\n",
        "\n",
        "        # Initialize counters to track our changes\n",
        "        fixes = {\n",
        "            'postal_code': 0,\n",
        "            'subdivision': 0,\n",
        "            'unfixed': 0\n",
        "        }\n",
        "\n",
        "        # Process each city separately\n",
        "        for city in df_clean['City'].unique():\n",
        "            logger.log_info(f\"\\nProcessing {city}...\")\n",
        "\n",
        "            # Get the boundaries for this city\n",
        "            bounds = city_boundaries.get(city)\n",
        "            if not bounds:\n",
        "                logger.log_error(f\"No boundary data found for {city}, skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Create mask for this city's outliers\n",
        "            city_mask = df_clean['City'] == city\n",
        "            outliers = (\n",
        "                (df_clean['Latitude'] < bounds['lat_min']) |\n",
        "                (df_clean['Latitude'] > bounds['lat_max']) |\n",
        "                (df_clean['Longitude'] < bounds['lon_min']) |\n",
        "                (df_clean['Longitude'] > bounds['lon_max']) |\n",
        "                df_clean['Latitude'].isna() |\n",
        "                df_clean['Longitude'].isna()\n",
        "            ) & city_mask\n",
        "\n",
        "            outlier_count = outliers.sum()\n",
        "            logger.log_info(f\"Found {outlier_count:,} properties with invalid coordinates in {city}\")\n",
        "\n",
        "            if outlier_count > 0:\n",
        "                # Calculate reliable center points using only valid coordinates\n",
        "                valid_coords = ~outliers & city_mask\n",
        "\n",
        "                # Calculate postal code centers\n",
        "                postal_centers = df_clean[valid_coords].groupby('Postal_Code').agg({\n",
        "                    'Latitude': 'median',\n",
        "                    'Longitude': 'median',\n",
        "                    'MLS_Num': 'count'\n",
        "                }).rename(columns={'MLS_Num': 'property_count'})\n",
        "\n",
        "                # Calculate subdivision centers as backup\n",
        "                subdivision_centers = df_clean[valid_coords].groupby('Subdivision_Name').agg({\n",
        "                    'Latitude': 'median',\n",
        "                    'Longitude': 'median',\n",
        "                    'MLS_Num': 'count'\n",
        "                }).rename(columns={'MLS_Num': 'property_count'})\n",
        "\n",
        "                # Process each outlier\n",
        "                for idx in tqdm(df_clean[outliers].index,\n",
        "                              desc=f\"Processing {city} outliers\",\n",
        "                              total=outlier_count):\n",
        "                    postal_code = df_clean.loc[idx, 'Postal_Code']\n",
        "                    subdivision = df_clean.loc[idx, 'Subdivision_Name']\n",
        "\n",
        "                    # Try fixing by postal code first (if we have enough examples)\n",
        "                    if (postal_code in postal_centers.index and\n",
        "                        postal_centers.loc[postal_code, 'property_count'] >= 5):\n",
        "                        df_clean.loc[idx, 'Latitude'] = postal_centers.loc[postal_code, 'Latitude']\n",
        "                        df_clean.loc[idx, 'Longitude'] = postal_centers.loc[postal_code, 'Longitude']\n",
        "                        fixes['postal_code'] += 1\n",
        "\n",
        "                    # If postal code doesn't work, try subdivision\n",
        "                    elif (subdivision in subdivision_centers.index and\n",
        "                          subdivision_centers.loc[subdivision, 'property_count'] >= 5):\n",
        "                        df_clean.loc[idx, 'Latitude'] = subdivision_centers.loc[subdivision, 'Latitude']\n",
        "                        df_clean.loc[idx, 'Longitude'] = subdivision_centers.loc[subdivision, 'Longitude']\n",
        "                        fixes['subdivision'] += 1\n",
        "\n",
        "                    else:\n",
        "                        fixes['unfixed'] += 1\n",
        "\n",
        "        # Log comprehensive summary\n",
        "        logger.log_info(\"\\nCoordinate Cleaning Summary:\")\n",
        "        logger.log_info(\"-\" * 50)\n",
        "        logger.log_info(f\"Total properties processed: {len(df_clean):,}\")\n",
        "        logger.log_info(f\"Fixed using postal code centers: {fixes['postal_code']:,}\")\n",
        "        logger.log_info(f\"Fixed using subdivision centers: {fixes['subdivision']:,}\")\n",
        "        logger.log_info(f\"Unable to fix: {fixes['unfixed']:,}\")\n",
        "\n",
        "        # Verify results by city\n",
        "        logger.log_info(\"\\nVerification by City:\")\n",
        "        logger.log_info(\"-\" * 50)\n",
        "        for city in df_clean['City'].unique():\n",
        "            bounds = city_boundaries[city]\n",
        "            city_mask = df_clean['City'] == city\n",
        "            remaining_outliers = (\n",
        "                (df_clean['Latitude'] < bounds['lat_min']) |\n",
        "                (df_clean['Latitude'] > bounds['lat_max']) |\n",
        "                (df_clean['Longitude'] < bounds['lon_min']) |\n",
        "                (df_clean['Longitude'] > bounds['lon_max']) |\n",
        "                df_clean['Latitude'].isna() |\n",
        "                df_clean['Longitude'].isna()\n",
        "            ) & city_mask\n",
        "\n",
        "            logger.log_info(f\"{city}: {remaining_outliers.sum():,} remaining invalid coordinates\")\n",
        "\n",
        "        return df_clean, fixes\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in clean_regional_coordinates: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the cleaning process\n",
        "try:\n",
        "    logger.log_info(\"Starting coordinate cleaning process...\")\n",
        "\n",
        "    # Create backup of coordinate data\n",
        "    coordinate_backup = combined_data[['Latitude', 'Longitude']].copy()\n",
        "    logger.log_info(\"Created backup of coordinate data\")\n",
        "\n",
        "    # Run the cleaning process\n",
        "    cleaned_data, fix_summary = clean_regional_coordinates(combined_data, city_boundaries)\n",
        "\n",
        "    # Update main DataFrame with cleaned coordinates\n",
        "    combined_data = cleaned_data\n",
        "\n",
        "    # Verify final results\n",
        "    logger.log_info(\"\\nFinal Coordinate Check:\")\n",
        "    logger.log_info(\"-\" * 50)\n",
        "    null_coords = combined_data[['Latitude', 'Longitude']].isna().sum()\n",
        "    for col, count in null_coords.items():\n",
        "        logger.log_info(f\"Remaining null {col}: {count:,}\")\n",
        "\n",
        "    logger.log_info(\"\\nCoordinate cleaning completed successfully! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"Coordinate cleaning\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during coordinate cleaning:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")\n",
        "\n",
        "    # Restore original data if there was an error\n",
        "    if 'coordinate_backup' in locals():\n",
        "        combined_data[['Latitude', 'Longitude']] = coordinate_backup\n",
        "        logger.log_info(\"\\nRestored original coordinate values due to error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWASzGeTwuVl"
      },
      "source": [
        "# Data Cleaning: Property Build Year Analysis and Correction\n",
        "\n",
        "## Overview\n",
        "This code block analyzes and cleans property construction years, handling common data entry issues like two-digit years and invalid dates. It uses neighborhood patterns to make intelligent corrections while maintaining data integrity.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Analysis Components\n",
        "1. Initial Data Assessment\n",
        "   * Converts years to numeric format\n",
        "   * Identifies missing values\n",
        "   * Reports basic statistics\n",
        "   * Groups properties by century\n",
        "   * Flags suspicious years\n",
        "\n",
        "2. Year Range Validation\n",
        "   * Earliest valid year: 1880 (Calgary's early development)\n",
        "   * Latest valid year: Current year + 1 (for under construction)\n",
        "   * Flags years outside valid range\n",
        "   * Reports suspicious entries\n",
        "\n",
        "3. Century Distribution Analysis\n",
        "   * Before 1800: Likely errors\n",
        "   * 1800s: Historic properties\n",
        "   * 1900s: Established properties\n",
        "   * 2000s: Modern properties\n",
        "\n",
        "### Cleaning Process\n",
        "1. Two-Digit Year Handling\n",
        "   * Years 00-24: Converted to 2000s\n",
        "     * Example: '22' → 2022\n",
        "   * Years 25-99: Converted to 1900s\n",
        "     * Example: '85' → 1985\n",
        "\n",
        "2. Invalid Year Correction\n",
        "   * Primary Method: Neighborhood Median\n",
        "     * Uses valid years from same subdivision\n",
        "     * Maintains neighborhood context\n",
        "   * Fallback Method: Overall Median\n",
        "     * Used when neighborhood data unavailable\n",
        "     * Provides reasonable estimate\n",
        "\n",
        "### Quality Controls\n",
        "The code tracks:\n",
        "* Total properties processed\n",
        "* Invalid years identified\n",
        "* Corrections by method:\n",
        "  * Neighborhood-based fixes\n",
        "  * Overall median fixes\n",
        "* Distribution before/after cleaning\n",
        "\n",
        "## Process Protection\n",
        "* Creates data backup\n",
        "* Validates year ranges\n",
        "* Uses working copy\n",
        "* Handles null values\n",
        "* Error recovery system\n",
        "\n",
        "## Results Format\n",
        "1. Initial Analysis:\n",
        "   * Basic statistics\n",
        "   * Century distribution\n",
        "   * Suspicious year counts\n",
        "\n",
        "2. Cleaning Summary:\n",
        "   * Invalid years found\n",
        "   * Fixes applied\n",
        "   * Method breakdown\n",
        "\n",
        "3. Final Distribution:\n",
        "   * Earliest properties\n",
        "   * Most recent properties\n",
        "   * Sample distribution\n",
        "\n",
        "[Code block follows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "toQqoUHZyBOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1e3ed4-1655-43dc-8676-cb7a28432d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Year Built analysis and cleaning process...\n",
            "Created backup of Year Built data\n",
            "\n",
            "Analyzing current state of Year Built data...\n",
            "Starting Year Built analysis...\n",
            "\n",
            "Basic Statistics:\n",
            "--------------------------------------------------\n",
            "Total properties: 393,664\n",
            "Missing values: 86\n",
            "\n",
            "Value Ranges:\n",
            "Minimum year: 0\n",
            "Maximum year: 191980\n",
            "\n",
            "Distribution by Century:\n",
            "1900s: 199,081 properties\n",
            "2000s: 194,439 properties\n",
            "1800s: 36 properties\n",
            "Before 1800: 22 properties\n",
            "\n",
            "Suspicious Years Found:\n",
            "Year 0: 15 properties\n",
            "Year 72: 1 properties\n",
            "Year 197: 1 properties\n",
            "Year 208: 1 properties\n",
            "Year 217: 1 properties\n",
            "Year 812: 1 properties\n",
            "Year 1071: 1 properties\n",
            "Year 1776: 1 properties\n",
            "Year 1878: 1 properties\n",
            "Year 2025: 64 properties\n",
            "Year 2026: 1 properties\n",
            "Year 2920: 1 properties\n",
            "Year 20117: 1 properties\n",
            "Year 191980: 1 properties\n",
            "\n",
            "Cleaning Year Built data...\n",
            "Starting Year Built cleaning process...\n",
            "Setting valid year range as 1880 to 2026\n",
            "Fixing two-digit year formats...\n",
            "\n",
            "Found 10 invalid years\n",
            "\n",
            "Year Built Cleaning Summary:\n",
            "--------------------------------------------------\n",
            "Total properties processed: 393,664\n",
            "Invalid years identified: 10\n",
            "Fixed using neighborhood median: 10\n",
            "Fixed using overall median: 10\n",
            "\n",
            "Final Year Distribution (Sample):\n",
            "\n",
            "Earliest years:\n",
            "1893: 3 properties\n",
            "1895: 2 properties\n",
            "1898: 4 properties\n",
            "1900: 26 properties\n",
            "1901: 1 properties\n",
            "\n",
            "Most recent years:\n",
            "2022: 2,207 properties\n",
            "2023: 2,595 properties\n",
            "2024: 1,599 properties\n",
            "2025: 64 properties\n",
            "2026: 1 properties\n",
            "\n",
            "Analyzing final state of Year Built data...\n",
            "Starting Year Built analysis...\n",
            "\n",
            "Basic Statistics:\n",
            "--------------------------------------------------\n",
            "Total properties: 393,664\n",
            "Missing values: 86\n",
            "\n",
            "Value Ranges:\n",
            "Minimum year: 1893\n",
            "Maximum year: 2026\n",
            "\n",
            "Distribution by Century:\n",
            "1900s: 199,107 properties\n",
            "2000s: 194,436 properties\n",
            "1800s: 35 properties\n",
            "Before 1800: 0 properties\n",
            "\n",
            "Suspicious Years Found:\n",
            "Year 2025: 64 properties\n",
            "Year 2026: 1 properties\n",
            "\n",
            "Year Built cleaning completed successfully! 🎉\n",
            "Step 17 completed: Year Built analysis and cleaning\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# YEAR BUILT ANALYSIS AND CLEANING\n",
        "# =================================================================================\n",
        "\n",
        "def analyze_year_built_data(df):\n",
        "    \"\"\"\n",
        "    Analyzes Year_Built data to identify potential issues and patterns.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame containing property data\n",
        "\n",
        "    Returns:\n",
        "        pandas.Series: Processed year data for further analysis\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting Year Built analysis...\")\n",
        "\n",
        "        # Convert Year_Built to numeric, coercing errors to NaN\n",
        "        year_data = pd.to_numeric(df['Year_Built'], errors='coerce')\n",
        "\n",
        "        # Log basic statistics\n",
        "        logger.log_info(\"\\nBasic Statistics:\")\n",
        "        logger.log_info(\"-\" * 50)\n",
        "        logger.log_info(f\"Total properties: {len(df):,}\")\n",
        "        logger.log_info(f\"Missing values: {year_data.isna().sum():,}\")\n",
        "\n",
        "        # Log value ranges\n",
        "        if not year_data.empty:\n",
        "            logger.log_info(f\"\\nValue Ranges:\")\n",
        "            logger.log_info(f\"Minimum year: {year_data.min():.0f}\")\n",
        "            logger.log_info(f\"Maximum year: {year_data.max():.0f}\")\n",
        "\n",
        "        # Analyze distribution by century\n",
        "        logger.log_info(\"\\nDistribution by Century:\")\n",
        "        century_bins = [-float('inf'), 1800, 1900, 2000, float('inf')]\n",
        "        century_labels = ['Before 1800', '1800s', '1900s', '2000s']\n",
        "        century_dist = pd.cut(year_data, bins=century_bins, labels=century_labels)\n",
        "\n",
        "        for century, count in century_dist.value_counts().items():\n",
        "            logger.log_info(f\"{century}: {count:,} properties\")\n",
        "\n",
        "        # Identify suspicious years\n",
        "        suspicious = year_data[(year_data < 1880) | (year_data > 2024)]\n",
        "        if len(suspicious) > 0:\n",
        "            logger.log_info(\"\\nSuspicious Years Found:\")\n",
        "            for year, count in suspicious.value_counts().sort_index().items():\n",
        "                logger.log_info(f\"Year {year:.0f}: {count:,} properties\")\n",
        "\n",
        "        return year_data\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_year_built_data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def clean_year_built(df):\n",
        "    \"\"\"\n",
        "    Cleans Year_Built data using reasonable assumptions about Calgary's real estate market.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame containing property data\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with cleaned Year_Built values\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting Year Built cleaning process...\")\n",
        "\n",
        "        # Create working copy and set valid year ranges\n",
        "        df_clean = df.copy()\n",
        "        earliest_valid_year = 1880\n",
        "        current_year = datetime.now().year\n",
        "        latest_valid_year = current_year + 1  # Allow for properties under construction\n",
        "\n",
        "        logger.log_info(f\"Setting valid year range as {earliest_valid_year} to {latest_valid_year}\")\n",
        "\n",
        "        # Convert to numeric values\n",
        "        df_clean['Year_Built'] = pd.to_numeric(df_clean['Year_Built'], errors='coerce')\n",
        "\n",
        "        def fix_two_digit_year(year):\n",
        "            \"\"\"Helper function to handle two-digit years appropriately.\"\"\"\n",
        "            if pd.isna(year):\n",
        "                return year\n",
        "            if 0 <= year <= (latest_valid_year - 2000):\n",
        "                return year + 2000  # Convert years like '22' to 2022\n",
        "            if year < 100:\n",
        "                return year + 1900  # Convert years like '99' to 1999\n",
        "            return year\n",
        "\n",
        "        # Apply two-digit year fixes\n",
        "        logger.log_info(\"Fixing two-digit year formats...\")\n",
        "        df_clean['Year_Built'] = df_clean['Year_Built'].apply(fix_two_digit_year)\n",
        "\n",
        "        # Identify valid and invalid years\n",
        "        valid_years = ((df_clean['Year_Built'] >= earliest_valid_year) &\n",
        "                      (df_clean['Year_Built'] <= latest_valid_year))\n",
        "\n",
        "        invalid_mask = ~valid_years & ~df_clean['Year_Built'].isna()\n",
        "        invalid_count = invalid_mask.sum()\n",
        "\n",
        "        if invalid_count > 0:\n",
        "            logger.log_info(f\"\\nFound {invalid_count:,} invalid years\")\n",
        "\n",
        "            # Calculate neighborhood medians from valid years\n",
        "            neighborhood_medians = (df_clean[valid_years]\n",
        "                                  .groupby('Subdivision_Name')['Year_Built']\n",
        "                                  .median())\n",
        "\n",
        "            # Fix invalid years using neighborhood medians where possible\n",
        "            fixes = {'neighborhood': 0, 'overall': 0}\n",
        "\n",
        "            for idx in df_clean[invalid_mask].index:\n",
        "                neighborhood = df_clean.loc[idx, 'Subdivision_Name']\n",
        "                if neighborhood in neighborhood_medians:\n",
        "                    df_clean.loc[idx, 'Year_Built'] = neighborhood_medians[neighborhood]\n",
        "                    fixes['neighborhood'] += 1\n",
        "\n",
        "            # Use overall median for any remaining invalid years\n",
        "            still_invalid = ~valid_years & ~df_clean['Year_Built'].isna()\n",
        "            if still_invalid.any():\n",
        "                overall_median = df_clean[valid_years]['Year_Built'].median()\n",
        "                df_clean.loc[still_invalid, 'Year_Built'] = overall_median\n",
        "                fixes['overall'] = still_invalid.sum()\n",
        "\n",
        "        # Log summary statistics\n",
        "        logger.log_info(\"\\nYear Built Cleaning Summary:\")\n",
        "        logger.log_info(\"-\" * 50)\n",
        "        logger.log_info(f\"Total properties processed: {len(df_clean):,}\")\n",
        "        logger.log_info(f\"Invalid years identified: {invalid_count:,}\")\n",
        "        if invalid_count > 0:\n",
        "            logger.log_info(f\"Fixed using neighborhood median: {fixes['neighborhood']:,}\")\n",
        "            logger.log_info(f\"Fixed using overall median: {fixes['overall']:,}\")\n",
        "\n",
        "        # Log final distribution\n",
        "        logger.log_info(\"\\nFinal Year Distribution (Sample):\")\n",
        "        year_dist = df_clean['Year_Built'].value_counts().sort_index()\n",
        "        logger.log_info(\"\\nEarliest years:\")\n",
        "        for year, count in year_dist.head().items():\n",
        "            logger.log_info(f\"{year:.0f}: {count:,} properties\")\n",
        "        logger.log_info(\"\\nMost recent years:\")\n",
        "        for year, count in year_dist.tail().items():\n",
        "            logger.log_info(f\"{year:.0f}: {count:,} properties\")\n",
        "\n",
        "        return df_clean\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in clean_year_built: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the analysis and cleaning\n",
        "try:\n",
        "    logger.log_info(\"Starting Year Built analysis and cleaning process...\")\n",
        "\n",
        "    # Create backup of Year Built data\n",
        "    year_built_backup = combined_data['Year_Built'].copy()\n",
        "    logger.log_info(\"Created backup of Year Built data\")\n",
        "\n",
        "    # Run initial analysis\n",
        "    logger.log_info(\"\\nAnalyzing current state of Year Built data...\")\n",
        "    year_data = analyze_year_built_data(combined_data)\n",
        "\n",
        "    # Clean the data\n",
        "    logger.log_info(\"\\nCleaning Year Built data...\")\n",
        "    cleaned_data = clean_year_built(combined_data)\n",
        "\n",
        "    # Update main DataFrame\n",
        "    combined_data = cleaned_data\n",
        "\n",
        "    # Run final analysis\n",
        "    logger.log_info(\"\\nAnalyzing final state of Year Built data...\")\n",
        "    year_data = analyze_year_built_data(combined_data)\n",
        "\n",
        "    logger.log_info(\"\\nYear Built cleaning completed successfully! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"Year Built analysis and cleaning\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during Year Built analysis and cleaning:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")\n",
        "\n",
        "    # Restore original data if there was an error\n",
        "    if 'year_built_backup' in locals():\n",
        "        combined_data['Year_Built'] = year_built_backup\n",
        "        logger.log_info(\"\\nRestored original Year Built values due to error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOx4-PEPbw93"
      },
      "source": [
        "# Data Analysis: Property Style Distribution\n",
        "\n",
        "## Overview\n",
        "This code block analyzes the distribution of property styles in our dataset, identifying the most common styles and potential variations in style naming. This analysis helps understand our property style data before standardization.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Analysis Components\n",
        "1. Basic Statistics\n",
        "  * Counts unique styles\n",
        "  * Calculates total properties\n",
        "  * Identifies missing style data\n",
        "  * Computes style frequencies\n",
        "\n",
        "2. Top Style Analysis\n",
        "  * Shows most common styles\n",
        "  * Calculates percentages\n",
        "  * Configurable number of top styles\n",
        "  * Clear frequency reporting\n",
        "\n",
        "3. Variation Detection (Optional)\n",
        "  * Identifies similar style names\n",
        "  * Case-insensitive comparison\n",
        "  * Groups related styles\n",
        "  * Reports potential variations\n",
        "\n",
        "### Quality Metrics\n",
        "The code reports:\n",
        "* Total unique styles found\n",
        "* Properties with missing styles\n",
        "* Distribution of top styles\n",
        "* Style name variations\n",
        "\n",
        "### Style Pattern Analysis\n",
        "* Main Style Detection\n",
        " * Counts occurrence frequency\n",
        " * Calculates style percentages\n",
        " * Ranks by popularity\n",
        " * Shows top N styles\n",
        "\n",
        "* Style Variation Detection\n",
        " * Case-insensitive matching\n",
        " * Substring identification\n",
        " * Groups similar styles\n",
        " * Reports variation counts\n",
        "\n",
        "## Process Protection\n",
        "* Input validation\n",
        "* Error handling\n",
        "* Controlled output options\n",
        "* Clear logging format\n",
        "* Exception tracking\n",
        "\n",
        "## Output Format\n",
        "1. Summary Statistics:\n",
        "  * Unique style count\n",
        "  * Total properties\n",
        "  * Missing data count\n",
        "\n",
        "2. Top Styles Report:\n",
        "  * Style name\n",
        "  * Property count\n",
        "  * Percentage of total\n",
        "  * Ranked by frequency\n",
        "\n",
        "3. Variation Analysis (Optional):\n",
        "  * Similar style groups\n",
        "  * Variation counts\n",
        "  * Pattern identification\n",
        "  * Potential standardization needs\n",
        "\n",
        "[Code block follows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZgLHof-tbxgC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ee12fd-66f1-4230-fed9-3b90a2c1f9d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting property style analysis...\n",
            "\n",
            "Style Data Summary:\n",
            "Total unique styles: 73\n",
            "Total properties: 393,664\n",
            "Properties missing style: 5,791\n",
            "\n",
            "Top 10 Styles:\n",
            "2 Storey: 170,727 (43.4%)\n",
            "Bungalow: 67,315 (17.1%)\n",
            "Single Level Unit: 34,444 (8.7%)\n",
            "Bi-Level: 25,207 (6.4%)\n",
            "4 Level Split: 16,248 (4.1%)\n",
            "Apartment: 12,850 (3.3%)\n",
            "Low-Rise(1-4): 9,088 (2.3%)\n",
            "2 Storey, Side by Side: 8,588 (2.2%)\n",
            "3 Storey: 8,385 (2.1%)\n",
            "2 Storey Split: 7,714 (2.0%)\n",
            "\n",
            "Potential Style Variations:\n",
            "2 Storey: 61 potential variations\n",
            "Bungalow: 40 potential variations\n",
            "Bi-Level: 37 potential variations\n",
            "4 Level Split: 26 potential variations\n",
            "Apartment: 34 potential variations\n",
            "Low-Rise(1-4): 20 potential variations\n",
            "2 Storey, Side by Side: 7 potential variations\n",
            "3 Storey: 19 potential variations\n",
            "2 Storey Split: 19 potential variations\n",
            "High-Rise (5+): 14 potential variations\n",
            "3 Level Split: 15 potential variations\n",
            "1 and Half Storey: 20 potential variations\n",
            "Bungalow, Side by Side: 8 potential variations\n",
            "Apartment, Low-Rise(1-4): 5 potential variations\n",
            "Bi-Level, Side by Side: 7 potential variations\n",
            "5 Level Split: 13 potential variations\n",
            "Townhouse: 33 potential variations\n",
            "Multi Level Unit: 24 potential variations\n",
            "2 and Half Storey: 10 potential variations\n",
            "Loft/Bachelor/Studio: 18 potential variations\n",
            "Apartment, High-Rise (5+): 9 potential variations\n",
            "Single Wide Mobile Home: 4 potential variations\n",
            "Side by Side: 50 potential variations\n",
            "3 Storey, Side by Side: 5 potential variations\n",
            "2 Storey, Townhouse: 2 potential variations\n",
            "4 Level Split, Side by Side: 6 potential variations\n",
            "Modified Bi-Level: 10 potential variations\n",
            "Penthouse: 18 potential variations\n",
            "Modular Home: 5 potential variations\n",
            "Bungalow, Up/Down: 4 potential variations\n",
            "Double Wide Mobile Home: 4 potential variations\n",
            "Bi-Level, Up/Down: 4 potential variations\n",
            "3 Level Split, Side by Side: 2 potential variations\n",
            "2 Storey, Up/Down: 2 potential variations\n",
            "Stacked Townhouse: 2 potential variations\n",
            "3 Storey, Townhouse: 5 potential variations\n",
            "Acreage with Residence, Bungalow: 3 potential variations\n",
            "Split Level: 21 potential variations\n",
            "Bungalow, Townhouse: 3 potential variations\n",
            "2 Storey, 2 Storey Split: 6 potential variations\n",
            "1 and Half Storey, Side by Side: 2 potential variations\n",
            "Side by Side, Villa: 2 potential variations\n",
            "Apartment, Bungalow: 3 potential variations\n",
            "Apartment, Penthouse: 3 potential variations\n",
            "Mobile: 10 potential variations\n",
            "2 and Half Storey, Side by Side: 2 potential variations\n",
            "2 Storey, 3 Storey: 5 potential variations\n",
            "Apartment, Low-Rise(1-4), Multi Level Unit: 6 potential variations\n",
            "5 Level Split, Side by Side: 4 potential variations\n",
            "Side by Side, Townhouse: 10 potential variations\n",
            "Apartment, Low-Rise(1-4), Penthouse: 5 potential variations\n",
            "Bi-Level, Bungalow: 6 potential variations\n",
            "4 Storey: 8 potential variations\n",
            "2 Storey, Back Split: 2 potential variations\n",
            "Acreage with Residence, 2 Storey: 5 potential variations\n",
            "Apartment, Multi Level Unit: 3 potential variations\n",
            "2 Storey, Acreage with Residence: 3 potential variations\n",
            "2 Storey, Side by Side, Townhouse: 5 potential variations\n",
            "4 Level Split, Townhouse: 2 potential variations\n",
            "2 Storey Split, Side by Side: 5 potential variations\n",
            "Not Applicable: 1 potential variations\n",
            "Acreage with Residence, Bi-Level: 3 potential variations\n",
            "Townhouse, Up/Down: 4 potential variations\n",
            "3 Level Split, Back Split: 2 potential variations\n",
            "1 and Half Storey, Bungalow: 4 potential variations\n",
            "1 and Half Storey, 2 Storey: 8 potential variations\n",
            "Apartment, High-Rise (5+), Penthouse: 5 potential variations\n",
            "4 Level Split, Back Split: 2 potential variations\n",
            "Side by Side, Split Level: 7 potential variations\n",
            "3 Storey, Apartment: 2 potential variations\n",
            "2 Storey, 4 Level Split: 3 potential variations\n",
            "Bi-Level, Modified Bi-Level: 3 potential variations\n",
            "2 Storey, Apartment: 3 potential variations\n",
            "Up/Down: 23 potential variations\n",
            "4 Level Split, Up/Down: 2 potential variations\n",
            "Bi-Level, Townhouse: 3 potential variations\n",
            "Low-Rise(1-4), Multi Level Unit: 10 potential variations\n",
            "High-Rise (5+), Penthouse: 3 potential variations\n",
            "High-Rise (5+), Low-Rise(1-4): 3 potential variations\n",
            "Villa: 1 potential variations\n",
            "4 Storey, Apartment: 3 potential variations\n",
            "Bungalow, Walk Out: 2 potential variations\n",
            "Back Split: 10 potential variations\n",
            "Bungalow, Modular Home: 2 potential variations\n",
            "Acreage with Residence: 23 potential variations\n",
            "2 Storey, Walk Out: 2 potential variations\n",
            "2 and Half Storey, 2 Storey: 6 potential variations\n",
            "3 Storey, Up/Down: 2 potential variations\n",
            "Modular Home, Single Wide Mobile Home: 4 potential variations\n",
            "5 Storey: 1 potential variations\n",
            "Modified Bi-Level, Side by Side: 5 potential variations\n",
            "Apartment, Loft/Bachelor/Studio: 7 potential variations\n",
            "3 Storey, Back Split: 2 potential variations\n",
            "Loft/Bachelor/Studio, Multi Level Unit: 6 potential variations\n",
            "1 and Half Storey, Bi-Level: 2 potential variations\n",
            "5 Level Split, Townhouse: 2 potential variations\n",
            "Multi Level Unit, Penthouse: 11 potential variations\n",
            "2 Storey, Bungalow: 6 potential variations\n",
            "Back Split, Bi-Level: 2 potential variations\n",
            "Apartment, High-Rise (5+), Multi Level Unit: 6 potential variations\n",
            "2 Storey, Bi-Level: 4 potential variations\n",
            "Other: 4 potential variations\n",
            "2 Storey, 3 Level Split: 4 potential variations\n",
            "Acreage with Residence, 1 and Half Storey: 4 potential variations\n",
            "2 Storey, 3 Storey, Townhouse: 5 potential variations\n",
            "4 Level Split, Acreage with Residence: 3 potential variations\n",
            "2 Storey, 5 Level Split: 2 potential variations\n",
            "Low-Rise(1-4), Side by Side: 2 potential variations\n",
            "2 Storey Split, Back Split: 3 potential variations\n",
            "Bungalow, Split Entry: 2 potential variations\n",
            "Apartment, Low-Rise(1-4), Multi Level Unit, Penthouse: 9 potential variations\n",
            "Low-Rise(1-4), Penthouse: 4 potential variations\n",
            "Apartment, Loft/Bachelor/Studio, Low-Rise(1-4): 7 potential variations\n",
            "2 Storey Split, 4 Level Split: 3 potential variations\n",
            "Apartment, Loft/Bachelor/Studio, Multi Level Unit: 6 potential variations\n",
            "3 Level Split, Bi-Level: 2 potential variations\n",
            "Apartment, Up/Down: 2 potential variations\n",
            "1 and Half Storey, Townhouse: 2 potential variations\n",
            "3 Storey, 4 Level Split: 2 potential variations\n",
            "Bungalow, Single Wide Mobile Home: 3 potential variations\n",
            "Apartment, Multi Level Unit, Penthouse: 5 potential variations\n",
            "Apartment, High-Rise (5+), Low-Rise(1-4): 5 potential variations\n",
            "Bi-Level, Bungalow, Side by Side: 6 potential variations\n",
            "Apartment, Loft/Bachelor/Studio, Low-Rise(1-4), Multi Level Unit, Penthouse: 14 potential variations\n",
            "3 Level Split, Split Level: 3 potential variations\n",
            "2 and Half Storey, 3 Storey: 2 potential variations\n",
            "Apartment, Side by Side: 3 potential variations\n",
            "Bungalow, Double Wide Mobile Home: 3 potential variations\n",
            "1 and Half Storey, Back Split: 2 potential variations\n",
            "2 Storey, Split Level: 2 potential variations\n",
            "1 and Half Storey, 2 Storey Split: 5 potential variations\n",
            "Acreage with Residence, 4 Level Split: 3 potential variations\n",
            "2 Storey Split, Townhouse: 3 potential variations\n",
            "Apartment, Other: 2 potential variations\n",
            "Split Level, Townhouse: 6 potential variations\n",
            "Apartment, Bi-Level: 2 potential variations\n",
            "4 Level Split, Split Level: 3 potential variations\n",
            "Apartment, High-Rise (5+), Multi Level Unit, Penthouse: 9 potential variations\n",
            "Loft/Bachelor/Studio, Low-Rise(1-4): 8 potential variations\n",
            "Apartment, Not Applicable: 2 potential variations\n",
            "Back Split, Bungalow: 2 potential variations\n",
            "Double Wide Mobile Home, Modular Home: 3 potential variations\n",
            "1 and Half Storey, Up/Down: 2 potential variations\n",
            "4 Level Split, Split Level, Townhouse: 5 potential variations\n",
            "High-Rise (5+), Multi Level Unit, Penthouse: 6 potential variations\n",
            "4 Level Split, Bungalow: 2 potential variations\n",
            "Loft/Bachelor/Studio, Penthouse: 2 potential variations\n",
            "3 Level Split, Up/Down: 2 potential variations\n",
            "5 Level Split, Split Level: 3 potential variations\n",
            "Acreage with Residence, 2 Storey, Side by Side: 5 potential variations\n",
            "3 Level Split, 3 Storey, Townhouse: 5 potential variations\n",
            "3 Level Split, 4 Level Split, Side by Side: 4 potential variations\n",
            "Acreage with Residence, 2 Storey, 2 Storey Split: 5 potential variations\n",
            "Loft/Bachelor/Studio, Low-Rise(1-4), Multi Level Unit: 8 potential variations\n",
            "2 Storey, Bungalow, Side by Side: 5 potential variations\n",
            "3 Storey, Side by Side, Townhouse: 5 potential variations\n",
            "1 and Half Storey, 2 Storey, Side by Side: 5 potential variations\n",
            "4 Level Split, Side by Side, Townhouse: 5 potential variations\n",
            "5 Level Split, Up/Down: 2 potential variations\n",
            "1 and Half Storey, Bungalow, Side by Side: 5 potential variations\n",
            "Acreage with Residence, Bungalow, Side by Side: 5 potential variations\n",
            "Back Split, Split Level: 2 potential variations\n",
            "2 and Half Storey, Bi-Level: 2 potential variations\n",
            "4 Level Split, 5 Level Split, Side by Side: 5 potential variations\n",
            "4 Level Split, Side by Side, Split Level: 5 potential variations\n",
            "2 Storey Split, Acreage with Residence: 3 potential variations\n",
            "4 Level Split, 5 Level Split: 4 potential variations\n",
            "Bungalow, Other: 3 potential variations\n",
            "1 and Half Storey, Acreage with Residence: 2 potential variations\n",
            "Bi-Level, Split Entry: 2 potential variations\n",
            "Apartment, High-Rise (5+), Loft/Bachelor/Studio: 7 potential variations\n",
            "Apartment, Loft/Bachelor/Studio, Multi Level Unit, Penthouse: 8 potential variations\n",
            "Bungalow, Mobile: 3 potential variations\n",
            "High-Rise (5+), Loft/Bachelor/Studio: 5 potential variations\n",
            "4 Storey, Acreage with Residence: 2 potential variations\n",
            "Stacked Townhouse, Up/Down: 4 potential variations\n",
            "2 Storey, Low-Rise(1-4), Multi Level Unit, Townhouse: 5 potential variations\n",
            "4 Storey, Up/Down: 2 potential variations\n",
            "2 and Half Storey, Townhouse: 2 potential variations\n",
            "3 Storey, Multi Level Unit, Side by Side, Townhouse: 6 potential variations\n",
            "Walk Out: 6 potential variations\n",
            "Bi-Level, Bungalow, Up/Down: 5 potential variations\n",
            "Bungalow, One & 3/4: 2 potential variations\n",
            "Contemporary: 1 potential variations\n",
            "2 Storey, Contemporary, Walk Out: 3 potential variations\n",
            "2 Storey, Loft/Bachelor/Studio: 2 potential variations\n",
            "4 Level Split, Walk Out: 2 potential variations\n",
            "Bi-Level, Walk Out: 2 potential variations\n",
            "Apartment, Side by Side, Townhouse: 5 potential variations\n",
            "Apartment, High-Rise (5+), Loft/Bachelor/Studio, Multi Level Unit: 9 potential variations\n",
            "Bungalow, Modified Bi-Level: 3 potential variations\n",
            "3 Level Split, Split Level, Townhouse: 5 potential variations\n",
            "2 and Half Storey, 3 Level Split, 4 Level Split: 3 potential variations\n",
            "Loft/Bachelor/Studio, Low-Rise(1-4), Penthouse: 5 potential variations\n",
            "5 Level Split, Side by Side, Townhouse: 5 potential variations\n",
            "2 Storey, Bi-Level, Side by Side: 5 potential variations\n",
            "4 Storey, 5 Level Split: 2 potential variations\n",
            "Bungalow, Townhouse, Up/Down: 5 potential variations\n",
            "2 Storey Split, Split Level: 3 potential variations\n",
            "Acreage with Residence, Side by Side, Split Level: 4 potential variations\n",
            "Low-Rise(1-4), Multi Level Unit, Penthouse: 8 potential variations\n",
            "Acreage with Residence, Bi-Level, Up/Down: 5 potential variations\n",
            "2 Storey Split, Bi-Level, Split Level: 6 potential variations\n",
            "1 and Half Storey, 2 Storey, Bungalow: 5 potential variations\n",
            "2 Storey, 4 Level Split, 5 Level Split: 5 potential variations\n",
            "2 Storey Split, 3 Storey: 3 potential variations\n",
            "One & 3/4: 2 potential variations\n",
            "Acreage with Residence, 4 Level Split, Side by Side: 5 potential variations\n",
            "Double Wide Mobile Home, Mobile: 2 potential variations\n",
            "Mobile, Modular Home: 3 potential variations\n",
            "Loft/Bachelor/Studio, Up/Down: 2 potential variations\n",
            "A-Frame: 1 potential variations\n",
            "4 Storey, Side by Side: 3 potential variations\n",
            "1 and Half Storey, 2 Storey, Modified Bi-Level: 6 potential variations\n",
            "Loft/Bachelor/Studio, Low-Rise(1-4), Multi Level Unit, Penthouse: 10 potential variations\n",
            "4 Level Split, Bi-Level, Split Level: 5 potential variations\n",
            "3 Level Split, Modified Bi-Level: 3 potential variations\n",
            "Bi-Level, Bungalow, Side by Side, Split Level: 8 potential variations\n",
            "Bungalow, Mobile, Modular Home, Single Wide Mobile Home: 7 potential variations\n",
            "2 Storey, 2 Storey Split, Side by Side: 6 potential variations\n",
            "Acreage with Residence, 1 and Half Storey, Cottage/Cabin: 3 potential variations\n",
            "4 Storey, Side by Side, Townhouse: 5 potential variations\n",
            "2 Storey Split, 5 Level Split: 3 potential variations\n",
            "5 Level Split, Back Split: 2 potential variations\n",
            "3 Storey, Bungalow: 2 potential variations\n",
            "Bi-Level, Modified Bi-Level, Side by Side: 6 potential variations\n",
            "Bungalow, Cottage/Cabin: 2 potential variations\n",
            "Modified Bi-Level, Up/Down: 4 potential variations\n",
            "2 Storey, 3 Level Split, 4 Level Split: 4 potential variations\n",
            "2 Storey, 3 Level Split, 3 Storey, Townhouse: 7 potential variations\n",
            "2 Storey, Side by Side, Split Level, Townhouse: 7 potential variations\n",
            "2 Storey Split, Apartment: 3 potential variations\n",
            "Multi Level Unit, Up/Down: 2 potential variations\n",
            "Acreage with Residence, 3 Storey, Side by Side: 4 potential variations\n",
            "Acreage with Residence, 2 Storey, Bungalow: 5 potential variations\n",
            "Bungalow, Other, Split Entry: 4 potential variations\n",
            "2 Storey, A-Frame, Bungalow, Cottage/Cabin: 4 potential variations\n",
            "5 Storey, Side by Side: 2 potential variations\n",
            "Acreage with Residence, Low-Rise(1-4): 2 potential variations\n",
            "2 Storey, Acreage with Residence, Side by Side: 4 potential variations\n",
            "4 Level Split, Acreage with Residence, Up/Down: 4 potential variations\n",
            "Low-Rise(1-4), Up/Down: 2 potential variations\n",
            "High-Rise (5+), Up/Down: 2 potential variations\n",
            "Multi Level Unit, Side by Side: 3 potential variations\n",
            "2 and Half Storey, 2 Storey, 2 Storey Split: 6 potential variations\n",
            "2 and Half Storey, 2 Storey, 2 Storey Split, Side by Side: 9 potential variations\n",
            "High-Rise (5+), Multi Level Unit: 5 potential variations\n",
            "2 Storey, Modified Bi-Level: 4 potential variations\n",
            "1 and Half Storey, 3 Level Split: 2 potential variations\n",
            "Bi-Level, Split Level: 4 potential variations\n",
            "Bi-Level, Side by Side, Split Level: 5 potential variations\n",
            "Modified Bi-Level, Townhouse: 4 potential variations\n",
            "2 and Half Storey, 2 Storey, Side by Side: 5 potential variations\n",
            "5 Level Split, Split Level, Townhouse: 5 potential variations\n",
            "Acreage with Residence, 1 and Half Storey, Bungalow: 5 potential variations\n",
            "Apartment, High-Rise (5+), Loft/Bachelor/Studio, Multi Level Unit, Penthouse: 11 potential variations\n",
            "1 and Half Storey, One & 3/4: 2 potential variations\n",
            "3 Storey, Split Level: 2 potential variations\n",
            "4 Level Split, Bi-Level: 3 potential variations\n",
            "Bungalow, Split Level: 2 potential variations\n",
            "2 and Half Storey, 2 Storey, 3 Storey: 5 potential variations\n",
            "2 Storey, 3 Storey, Side by Side: 5 potential variations\n",
            "4 Level Split, 4 Storey: 2 potential variations\n",
            "2 Storey, Bungalow, Up/Down: 5 potential variations\n",
            "Bi-Level, Side by Side, Townhouse: 5 potential variations\n",
            "Bi-Level, Other: 2 potential variations\n",
            "Split Entry: 4 potential variations\n",
            "Bungalow, Side by Side, Walk Out: 4 potential variations\n",
            "Apartment, Loft/Bachelor/Studio, Low-Rise(1-4), Multi Level Unit: 10 potential variations\n",
            "1 and Half Storey, 2 Storey, Bi-Level, Bungalow: 7 potential variations\n",
            "2 Storey, Apartment, Bungalow: 5 potential variations\n",
            "4 Storey, Apartment, Penthouse: 5 potential variations\n",
            "1 and Half Storey, 2 Storey Split, Bungalow: 6 potential variations\n",
            "Split Entry, Up/Down: 2 potential variations\n",
            "2 Storey Split, Bi-Level: 4 potential variations\n",
            "3 Level Split, Townhouse: 2 potential variations\n",
            "\n",
            "Style analysis completed successfully! 🎉\n",
            "Step 18 completed: Style data analysis\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# STYLE DATA ANALYSIS\n",
        "# =================================================================================\n",
        "\n",
        "def analyze_style_distribution(df, top_n=10, log_variations=False):\n",
        "    \"\"\"\n",
        "    Analyzes the distribution of property styles with controlled output.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame containing property data\n",
        "        top_n (int): Number of top styles to display (default: 10)\n",
        "        log_variations (bool): Whether to log detailed style variations (default: False)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (style value counts, dictionary of similar style groups)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Input validation\n",
        "        if 'Style' not in df.columns:\n",
        "            raise ValueError(\"Required column 'Style' not found in DataFrame\")\n",
        "\n",
        "        # Calculate basic statistics\n",
        "        style_counts = df['Style'].value_counts()\n",
        "        total_properties = len(df)\n",
        "        missing_count = df['Style'].isna().sum()\n",
        "\n",
        "        # Concise logging of overview statistics\n",
        "        logger.log_info(\"\\nStyle Data Summary:\")\n",
        "        logger.log_info(f\"Total unique styles: {style_counts.nunique():,}\")\n",
        "        logger.log_info(f\"Total properties: {total_properties:,}\")\n",
        "        logger.log_info(f\"Properties missing style: {missing_count:,}\")\n",
        "\n",
        "        # Log top styles more concisely\n",
        "        logger.log_info(f\"\\nTop {top_n} Styles:\")\n",
        "        for style, count in style_counts.head(top_n).items():\n",
        "            percentage = (count / total_properties) * 100\n",
        "            logger.log_info(f\"{style}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "        # Optionally analyze variations (with controlled output)\n",
        "        style_groups = {}\n",
        "        if log_variations:\n",
        "            for style in style_counts.index:\n",
        "                style_lower = style.lower()\n",
        "                similar_styles = [\n",
        "                    other for other in style_counts.index\n",
        "                    if (style != other and\n",
        "                        (style_lower in other.lower() or\n",
        "                         other.lower() in style_lower))\n",
        "                ]\n",
        "\n",
        "                if similar_styles:\n",
        "                    style_groups[style] = similar_styles\n",
        "\n",
        "            if style_groups:\n",
        "                logger.log_info(\"\\nPotential Style Variations:\")\n",
        "                for main_style, similar_styles in style_groups.items():\n",
        "                    logger.log_info(f\"{main_style}: {len(similar_styles)} potential variations\")\n",
        "            else:\n",
        "                logger.log_info(\"No significant style name variations found.\")\n",
        "\n",
        "        return style_counts, style_groups\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_style_distribution: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the analysis\n",
        "try:\n",
        "    logger.log_info(\"Starting property style analysis...\")\n",
        "\n",
        "    # Run the analysis with controlled output\n",
        "    style_counts, style_variations = analyze_style_distribution(\n",
        "        combined_data,\n",
        "        top_n=10,      # Show top 10 styles\n",
        "        log_variations=True  # Optional detailed variation logging\n",
        "    )\n",
        "\n",
        "    logger.log_info(\"\\nStyle analysis completed successfully! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"Style data analysis\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during style analysis:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning: Property Style Standardization\n",
        "\n",
        "## Overview\n",
        "This code block implements a systematic approach to standardize property style descriptions into a consistent set of categories. It handles various text variations and formats while maintaining the original data for reference.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Style Mapping Categories\n",
        "1. Residential Types\n",
        "  * Bungalow Styles:\n",
        "    * BUNGALOW → Bungalow\n",
        "    * MODIFIED BUNGALOW → Bungalow\n",
        "    * RAISED BUNGALOW → Bungalow\n",
        "\n",
        "  * Multi-Story Types:\n",
        "    * 2 STOREY variants → 2 Storey\n",
        "    * 3 STOREY variants → 3 Storey\n",
        "    * TWO STOREY → 2 Storey\n",
        "    * THREE STOREY → 3 Storey\n",
        "\n",
        "2. Apartment Types\n",
        "  * Low-Rise:\n",
        "    * LOW RISE APARTMENT → Apartment Low-Rise\n",
        "    * APARTMENT STYLE → Apartment Low-Rise\n",
        "    * APARTMENT → Apartment Low-Rise\n",
        "  \n",
        "  * High-Rise:\n",
        "    * HIGH RISE APARTMENT → Apartment High-Rise\n",
        "    * HIGH-RISE → Apartment High-Rise\n",
        "\n",
        "3. Special Categories\n",
        "  * Single Level:\n",
        "    * SINGLE LEVEL → Single Level Unit\n",
        "    * ONE STOREY → Single Level Unit\n",
        "    * MAIN FLOOR → Single Level Unit\n",
        "  \n",
        "  * Split/Bi-Level:\n",
        "    * BI-LEVEL variants → Bi-level\n",
        "    * 4 LEVEL SPLIT variants → 4 Level Split\n",
        "\n",
        "### Standardization Process\n",
        "1. Data Preparation\n",
        "  * Creates copy of data\n",
        "  * Preserves original values\n",
        "  * Converts to uppercase\n",
        "  * Removes whitespace\n",
        "\n",
        "2. Style Processing\n",
        "  * Direct mapping lookup\n",
        "  * Pattern matching fallback\n",
        "  * Tracks unmapped styles\n",
        "  * Handles null values\n",
        "\n",
        "3. Quality Monitoring\n",
        "  * Counts standardized rows\n",
        "  * Tracks unmapped styles\n",
        "  * Reports success rates\n",
        "  * Shows distribution\n",
        "\n",
        "### Results Tracking\n",
        "The code reports:\n",
        "* Original vs new unique styles\n",
        "* Standardization success rate\n",
        "* Distribution of new categories\n",
        "* Unmapped style details\n",
        "\n",
        "## Process Protection\n",
        "* Backs up original data\n",
        "* Validates input column\n",
        "* Creates working copy\n",
        "* Error recovery system\n",
        "* Detailed logging\n",
        "\n",
        "[Code block follows]"
      ],
      "metadata": {
        "id": "Cuy874-OVq3k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "McSCQ3qKgUrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214bcfbb-ac04-4567-955f-b22fde4761f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting property style standardization...\n",
            "Created backup of original style column\n",
            "Created working copy of DataFrame\n",
            "\n",
            "Property Style Standardization:\n",
            "--------------------------------------------------\n",
            "Original unique styles: 279\n",
            "Standardized unique styles: 11\n",
            "\n",
            "Row Standardization Summary:\n",
            "Total rows: 393,664\n",
            "Rows standardized: 366,123\n",
            "Standardization percentage: 93.0%\n",
            "\n",
            "New Style Distribution:\n",
            "2 Storey: 187,838 (47.7%)\n",
            "Bungalow: 70,301 (17.9%)\n",
            "Single Level Unit: 34,444 (8.7%)\n",
            "Bi-level: 27,272 (6.9%)\n",
            "Other: 21,750 (5.5%)\n",
            "4 Level Split: 16,570 (4.2%)\n",
            "Apartment Low-Rise: 15,440 (3.9%)\n",
            "3 Storey: 8,964 (2.3%)\n",
            "Apartment High-Rise: 4,364 (1.1%)\n",
            "Mobile Modular Home: 930 (0.2%)\n",
            "\n",
            "Unmapped Style Details (>1 occurrence):\n",
            "Low-Rise(1-4): 9,088 occurrences (2.3%)\n",
            "3 Level Split: 2,661 occurrences (0.7%)\n",
            "1 and Half Storey: 2,547 occurrences (0.6%)\n",
            "5 Level Split: 1,532 occurrences (0.4%)\n",
            "Townhouse: 1,500 occurrences (0.4%)\n",
            "Multi Level Unit: 1,439 occurrences (0.4%)\n",
            "2 and Half Storey: 795 occurrences (0.2%)\n",
            "Loft/Bachelor/Studio: 753 occurrences (0.2%)\n",
            "Side by Side: 470 occurrences (0.1%)\n",
            "Penthouse: 211 occurrences (0.1%)\n",
            "3 Level Split, Side by Side: 94 occurrences (0.0%)\n",
            "Stacked Townhouse: 92 occurrences (0.0%)\n",
            "Split Level: 79 occurrences (0.0%)\n",
            "1 and Half Storey, Side by Side: 49 occurrences (0.0%)\n",
            "Side by Side, Villa: 48 occurrences (0.0%)\n",
            "Mobile: 39 occurrences (0.0%)\n",
            "2 and Half Storey, Side by Side: 37 occurrences (0.0%)\n",
            "5 Level Split, Side by Side: 29 occurrences (0.0%)\n",
            "Side by Side, Townhouse: 28 occurrences (0.0%)\n",
            "4 Storey: 26 occurrences (0.0%)\n",
            "Not Applicable: 18 occurrences (0.0%)\n",
            "Townhouse, Up/Down: 17 occurrences (0.0%)\n",
            "3 Level Split, Back Split: 16 occurrences (0.0%)\n",
            "Side by Side, Split Level: 15 occurrences (0.0%)\n",
            "Up/Down: 12 occurrences (0.0%)\n",
            "Low-Rise(1-4), Multi Level Unit: 11 occurrences (0.0%)\n",
            "Villa: 10 occurrences (0.0%)\n",
            "Back Split: 9 occurrences (0.0%)\n",
            "5 Storey: 8 occurrences (0.0%)\n",
            "Acreage with Residence: 8 occurrences (0.0%)\n",
            "5 Level Split, Townhouse: 7 occurrences (0.0%)\n",
            "Multi Level Unit, Penthouse: 7 occurrences (0.0%)\n",
            "Loft/Bachelor/Studio, Multi Level Unit: 7 occurrences (0.0%)\n",
            "Acreage with Residence, 1 and Half Storey: 6 occurrences (0.0%)\n",
            "Other: 6 occurrences (0.0%)\n",
            "Low-Rise(1-4), Side by Side: 5 occurrences (0.0%)\n",
            "Low-Rise(1-4), Penthouse: 5 occurrences (0.0%)\n",
            "3 Level Split, Split Level: 4 occurrences (0.0%)\n",
            "1 and Half Storey, Townhouse: 4 occurrences (0.0%)\n",
            "1 and Half Storey, Up/Down: 3 occurrences (0.0%)\n",
            "1 and Half Storey, Back Split: 3 occurrences (0.0%)\n",
            "Loft/Bachelor/Studio, Low-Rise(1-4): 3 occurrences (0.0%)\n",
            "Split Level, Townhouse: 3 occurrences (0.0%)\n",
            "1 and Half Storey, Acreage with Residence: 2 occurrences (0.0%)\n",
            "Back Split, Split Level: 2 occurrences (0.0%)\n",
            "5 Level Split, Up/Down: 2 occurrences (0.0%)\n",
            "Loft/Bachelor/Studio, Low-Rise(1-4), Multi Level Unit: 2 occurrences (0.0%)\n",
            "5 Level Split, Split Level: 2 occurrences (0.0%)\n",
            "3 Level Split, Up/Down: 2 occurrences (0.0%)\n",
            "Loft/Bachelor/Studio, Penthouse: 2 occurrences (0.0%)\n",
            "\n",
            "Style standardization completed successfully! 🎉\n",
            "Step 19 completed: Property style standardization\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# PROPERTY STYLE STANDARDIZATION\n",
        "# =================================================================================\n",
        "\n",
        "def create_style_mapping():\n",
        "    \"\"\"\n",
        "    Create a comprehensive mapping of style variations to standardized categories.\n",
        "\n",
        "    Returns:\n",
        "        dict: Mapping of style variations to standard styles\n",
        "    \"\"\"\n",
        "    return {\n",
        "        # Bungalow variations\n",
        "        'BUNGALOW': 'Bungalow',\n",
        "        'MODIFIED BUNGALOW': 'Bungalow',\n",
        "        'RAISED BUNGALOW': 'Bungalow',\n",
        "\n",
        "        # 2 Storey variations\n",
        "        '2 STOREY': '2 Storey',\n",
        "        'TWO STOREY': '2 Storey',\n",
        "        '2-STOREY': '2 Storey',\n",
        "        '2 1/2 STOREY': '2 Storey',\n",
        "\n",
        "        # Single Level Unit variations\n",
        "        'SINGLE LEVEL': 'Single Level Unit',\n",
        "        'ONE STOREY': 'Single Level Unit',\n",
        "        'MAIN FLOOR': 'Single Level Unit',\n",
        "\n",
        "        # Bi-level variations\n",
        "        'BI-LEVEL': 'Bi-level',\n",
        "        'BILEVEL': 'Bi-level',\n",
        "\n",
        "        # 4 Level Split variations\n",
        "        '4 LEVEL SPLIT': '4 Level Split',\n",
        "        'FOUR LEVEL SPLIT': '4 Level Split',\n",
        "\n",
        "        # Apartment Low-Rise variations\n",
        "        'LOW RISE APARTMENT': 'Apartment Low-Rise',\n",
        "        'APARTMENT STYLE': 'Apartment Low-Rise',\n",
        "        'APARTMENT': 'Apartment Low-Rise',\n",
        "\n",
        "        # 3 Storey variations\n",
        "        '3 STOREY': '3 Storey',\n",
        "        'THREE STOREY': '3 Storey',\n",
        "\n",
        "        # Apartment High-Rise variations\n",
        "        'HIGH RISE APARTMENT': 'Apartment High-Rise',\n",
        "        'HIGH-RISE': 'Apartment High-Rise',\n",
        "\n",
        "        # Mobile/Modular variations\n",
        "        'MOBILE HOME': 'Mobile Modular Home',\n",
        "        'MODULAR': 'Mobile Modular Home'\n",
        "    }\n",
        "\n",
        "def standardize_property_styles(df, style_mapping=None):\n",
        "    \"\"\"\n",
        "    Standardize property style descriptions to consistent categories.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): Input DataFrame containing property data\n",
        "        style_mapping (dict, optional): Custom mapping of style variations.\n",
        "                                       If None, uses default mapping.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (DataFrame with standardized styles, dict of standardization summary)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Input validation\n",
        "        if 'Style' not in df.columns:\n",
        "            raise ValueError(\"Required column 'Style' not found in DataFrame\")\n",
        "\n",
        "        # Create a copy of the DataFrame to avoid modifying the original\n",
        "        df_clean = df.copy()\n",
        "        logger.log_info(\"Created working copy of DataFrame\")\n",
        "\n",
        "        # Create style mapping if not provided\n",
        "        if style_mapping is None:\n",
        "            style_mapping = create_style_mapping()\n",
        "\n",
        "        # Preserve original style column\n",
        "        df_clean['Original_Style'] = df_clean['Style']\n",
        "\n",
        "        # Track uncategorized styles\n",
        "        unmapped_styles = {}\n",
        "\n",
        "        # Track standardized rows\n",
        "        total_rows = len(df_clean)\n",
        "        standardized_rows = 0\n",
        "\n",
        "        def _standardize_single_style(style):\n",
        "            \"\"\"\n",
        "            Standardize a single style value.\n",
        "\n",
        "            Args:\n",
        "                style: Original style value\n",
        "\n",
        "            Returns:\n",
        "                str: Standardized style\n",
        "            \"\"\"\n",
        "            nonlocal standardized_rows\n",
        "\n",
        "            # Handle null values\n",
        "            if pd.isna(style):\n",
        "                return style\n",
        "\n",
        "            # Convert to uppercase for consistent matching\n",
        "            style_upper = str(style).upper().strip()\n",
        "\n",
        "            # First try direct mapping\n",
        "            if style_upper in style_mapping:\n",
        "                standardized_rows += 1\n",
        "                return style_mapping[style_upper]\n",
        "\n",
        "            # Try pattern matching\n",
        "            for key_pattern, standard_style in style_mapping.items():\n",
        "                if key_pattern in style_upper:\n",
        "                    standardized_rows += 1\n",
        "                    return standard_style\n",
        "\n",
        "            # Track unmapped styles\n",
        "            if style not in unmapped_styles:\n",
        "                unmapped_styles[style] = 0\n",
        "            unmapped_styles[style] += 1\n",
        "\n",
        "            return 'Other'\n",
        "\n",
        "        # Apply standardization\n",
        "        df_clean['Style'] = df_clean['Style'].apply(_standardize_single_style)\n",
        "\n",
        "        # Prepare summary statistics\n",
        "        original_unique_styles = len(df['Style'].unique())\n",
        "        new_unique_styles = len(df_clean['Style'].unique())\n",
        "\n",
        "        # Log the standardization process\n",
        "        logger.log_info(\"\\nProperty Style Standardization:\")\n",
        "        logger.log_info(\"-\" * 50)\n",
        "        logger.log_info(f\"Original unique styles: {original_unique_styles:,}\")\n",
        "        logger.log_info(f\"Standardized unique styles: {new_unique_styles:,}\")\n",
        "\n",
        "        # Calculate row standardization details\n",
        "        standardization_percentage = (standardized_rows / total_rows) * 100\n",
        "        logger.log_info(\"\\nRow Standardization Summary:\")\n",
        "        logger.log_info(f\"Total rows: {total_rows:,}\")\n",
        "        logger.log_info(f\"Rows standardized: {standardized_rows:,}\")\n",
        "        logger.log_info(f\"Standardization percentage: {standardization_percentage:.1f}%\")\n",
        "\n",
        "        # Log style distribution\n",
        "        style_counts = df_clean['Style'].value_counts()\n",
        "        total_properties = len(df_clean)\n",
        "\n",
        "        logger.log_info(\"\\nNew Style Distribution:\")\n",
        "        for style, count in style_counts.items():\n",
        "            percentage = (count / total_properties) * 100\n",
        "            logger.log_info(f\"{style}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "        # Log unmapped styles with more than one occurrence\n",
        "        if unmapped_styles:\n",
        "            filtered_unmapped = {\n",
        "                style: count for style, count in unmapped_styles.items() if count > 1\n",
        "            }\n",
        "\n",
        "            if filtered_unmapped:\n",
        "                logger.log_info(\"\\nUnmapped Style Details (>1 occurrence):\")\n",
        "                # Sort unmapped styles by frequency\n",
        "                sorted_unmapped = sorted(filtered_unmapped.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                for style, count in sorted_unmapped:\n",
        "                    percentage = (count / total_properties) * 100\n",
        "                    logger.log_info(f\"{style}: {count:,} occurrences ({percentage:.1f}%)\")\n",
        "            else:\n",
        "                logger.log_info(\"\\nNo unmapped styles with more than one occurrence.\")\n",
        "\n",
        "        # Prepare summary dictionary\n",
        "        summary = {\n",
        "            'original_unique_styles': original_unique_styles,\n",
        "            'new_unique_styles': new_unique_styles,\n",
        "            'total_rows': total_rows,\n",
        "            'standardized_rows': standardized_rows,\n",
        "            'standardization_percentage': standardization_percentage,\n",
        "            'unmapped_styles': unmapped_styles\n",
        "        }\n",
        "\n",
        "        return df_clean, summary\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in standardize_property_styles: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the standardization\n",
        "try:\n",
        "    logger.log_info(\"Starting property style standardization...\")\n",
        "\n",
        "    # Create backup of original style column\n",
        "    style_backup = combined_data['Style'].copy()\n",
        "    logger.log_info(\"Created backup of original style column\")\n",
        "\n",
        "    # Run the standardization\n",
        "    combined_data, style_stats = standardize_property_styles(combined_data)\n",
        "\n",
        "    logger.log_info(\"\\nStyle standardization completed successfully! 🎉\")\n",
        "\n",
        "    # Log step completion\n",
        "    logger.log_step_complete(\"Property style standardization\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during style standardization:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")\n",
        "\n",
        "    # Restore original style column if there's an error\n",
        "    combined_data['Style'] = style_backup\n",
        "    logger.log_info(\"\\nRestored original style column due to error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Enhancement: Advanced Style Classification with Machine Learning\n",
        "\n",
        "## Overview\n",
        "This code block implements a machine learning approach to classify property styles that weren't handled by the basic mapping system. It combines multiple property features to make intelligent predictions about uncertain style classifications.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Data Preparation\n",
        "1. Feature Creation\n",
        "   * Combines multiple property characteristics:\n",
        "     * Original Style description\n",
        "     * Structure Type\n",
        "     * Property Sub-Type\n",
        "   * Handles missing values with placeholders\n",
        "   * Standardizes text formatting\n",
        "   * Creates rich combined features\n",
        "\n",
        "2. Training Data Organization\n",
        "   * Separates confident vs uncertain examples\n",
        "   * Tracks data completeness\n",
        "   * Reports training data statistics\n",
        "   * Prepares text for vectorization\n",
        "\n",
        "### Machine Learning Components\n",
        "1. Text Processing\n",
        "   * Uses TF-IDF Vectorization\n",
        "   * Creates word n-grams (1-3 words)\n",
        "   * Maximum 1500 features\n",
        "   * Removes English stop words\n",
        "\n",
        "2. Model Configuration\n",
        "   * Random Forest Classifier\n",
        "   * 100 decision trees\n",
        "   * Minimum 5 samples per leaf\n",
        "   * Uses all CPU cores\n",
        "   * 80/20 train/test split\n",
        "\n",
        "3. Prediction System\n",
        "   * Confidence threshold of 0.7\n",
        "   * Tracks prediction probabilities\n",
        "   * Identifies high/low confidence predictions\n",
        "   * Analyzes uncertain cases\n",
        "\n",
        "### Quality Controls\n",
        "* Tracks confidence levels\n",
        "* Analyzes low confidence patterns\n",
        "* Reports model performance\n",
        "* Shows final distribution\n",
        "* Counts remaining unclassified\n",
        "\n",
        "## Process Protection\n",
        "* Creates data backup\n",
        "* Validates inputs\n",
        "* Handles missing values\n",
        "* Provides detailed logging\n",
        "* Error recovery system\n",
        "\n",
        "## Future Enhancement Note\n",
        "This code block currently works alongside the basic style mapping block. A future enhancement opportunity exists to:\n",
        "1. Merge both approaches into a single, comprehensive system\n",
        "2. Use basic mapping as first pass\n",
        "3. Apply ML classification only for unmapped styles\n",
        "4. Create unified confidence scoring\n",
        "5. Streamline the logging and reporting\n",
        "\n",
        "[Code block follows]"
      ],
      "metadata": {
        "id": "qoMr3ssnX3bh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yfOCJ0Z3mpYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d7b164-46db-43a7-98a1-f5e8432f9220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting advanced style classification...\n",
            "Created working copy of DataFrame for style classification\n",
            "\n",
            "Initial Missing Value Check:\n",
            "Missing Original_Style values: 5,791\n",
            "Missing Style values: 5,791\n",
            "Missing Structure_Type values: 3,683\n",
            "Missing Property_Sub_Type values: 54\n",
            "\n",
            "Training Data Summary:\n",
            "Total properties: 393,664\n",
            "Confident examples: 366,123\n",
            "Uncertain examples: 27,541\n",
            "\n",
            "Preparing training data with enhanced features...\n",
            "Created 559 features from combined property descriptions\n",
            "\n",
            "Training the classifier...\n",
            "\n",
            "Model Performance:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "           2 Storey       0.95      1.00      0.98     37461\n",
            "           3 Storey       0.00      0.00      0.00      1825\n",
            "      4 Level Split       1.00      1.00      1.00      3233\n",
            "Apartment High-Rise       1.00      1.00      1.00       828\n",
            " Apartment Low-Rise       1.00      1.00      1.00      3052\n",
            "           Bi-level       1.00      1.00      1.00      5549\n",
            "           Bungalow       1.00      1.00      1.00     14250\n",
            "Mobile Modular Home       1.00      1.00      1.00       197\n",
            "  Single Level Unit       1.00      1.00      1.00      6830\n",
            "\n",
            "           accuracy                           0.98     73225\n",
            "          macro avg       0.88      0.89      0.89     73225\n",
            "       weighted avg       0.95      0.98      0.96     73225\n",
            "\n",
            "\n",
            "Making predictions for uncertain styles...\n",
            "\n",
            "Prediction Results:\n",
            "Total uncertain styles: 27,541\n",
            "High confidence predictions: 9,665\n",
            "Low confidence predictions: 17,876\n",
            "\n",
            "Updating dataset with predictions...\n",
            "\n",
            "Analysis of 17876 Low Confidence Predictions:\n",
            "\n",
            "Common patterns in low confidence cases:\n",
            "                                                                     Predicted_Style  Confidence\n",
            "Original_Style                                                                                  \n",
            "MULTI LEVEL UNIT, PENTHOUSE LOW RISE (2-4 STORIES) APARTMENT      Apartment Low-Rise       0.693\n",
            "2 AND HALF STOREY, SIDE BY SIDE FIVE PLUS ROW/TOWNHOUSE                     2 Storey       0.690\n",
            "2 AND HALF STOREY FIVE PLUS ROW/TOWNHOUSE                                   2 Storey       0.690\n",
            "1 AND HALF STOREY FIVE PLUS ROW/TOWNHOUSE                                   2 Storey       0.690\n",
            "1 AND HALF STOREY, SIDE BY SIDE FIVE PLUS ROW/TOWNHOUSE                     2 Storey       0.690\n",
            "LOW-RISE(1-4), MULTI LEVEL UNIT LOW RISE (2-4 STORIES) APARTMENT  Apartment Low-Rise       0.680\n",
            "3 LEVEL SPLIT, SPLIT LEVEL, TOWNHOUSE FIVE PLUS ROW/TOWNHOUSE          4 Level Split       0.679\n",
            "5 LEVEL SPLIT, SPLIT LEVEL, TOWNHOUSE FIVE PLUS ROW/TOWNHOUSE          4 Level Split       0.679\n",
            "1 AND HALF STOREY, 3 LEVEL SPLIT HOUSE DETACHED                             2 Storey       0.679\n",
            "1 AND HALF STOREY HIGH RISE (5+ STORIES) APARTMENT                Apartment Low-Rise       0.656\n",
            "\n",
            "Final Style Distribution:\n",
            "2 Storey: 191,242 (48.6%)\n",
            "Bungalow: 70,301 (17.9%)\n",
            "Single Level Unit: 34,444 (8.7%)\n",
            "Bi-level: 27,272 (6.9%)\n",
            "4 Level Split: 20,915 (5.3%)\n",
            "Apartment Low-Rise: 17,356 (4.4%)\n",
            "Other: 12,085 (3.1%)\n",
            "3 Storey: 8,964 (2.3%)\n",
            "Apartment High-Rise: 4,364 (1.1%)\n",
            "Mobile Modular Home: 930 (0.2%)\n",
            "\n",
            "Remaining unclassified properties: 12,085\n",
            "Step 20 completed: Advanced style classification\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# =================================================================================\n",
        "# ADVANCED STYLE CLASSIFICATION USING MACHINE LEARNING\n",
        "# =================================================================================\n",
        "\n",
        "def prepare_style_classification_data(df):\n",
        "    \"\"\"\n",
        "    Prepare data for style classification, handling missing values and creating features.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): Input DataFrame containing property data\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Prepared DataFrame for classification\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a working copy to avoid modifying original data\n",
        "        working_df = df.copy()\n",
        "        logger.log_info(\"Created working copy of DataFrame for style classification\")\n",
        "\n",
        "        # Columns to check for missing values\n",
        "        columns_to_check = ['Original_Style', 'Style', 'Structure_Type', 'Property_Sub_Type']\n",
        "\n",
        "        # Log initial missing value information\n",
        "        logger.log_info(\"\\nInitial Missing Value Check:\")\n",
        "        for col in columns_to_check:\n",
        "            missing = working_df[col].isna().sum()\n",
        "            logger.log_info(f\"Missing {col} values: {missing:,}\")\n",
        "\n",
        "        # Handle missing values with informative placeholders\n",
        "        working_df['Original_Style'] = working_df['Original_Style'].fillna('Unknown Style')\n",
        "        working_df['Style'] = working_df['Style'].fillna('Other')\n",
        "        working_df['Structure_Type'] = working_df['Structure_Type'].fillna('Unknown Structure')\n",
        "        working_df['Property_Sub_Type'] = working_df['Property_Sub_Type'].fillna('Unknown Type')\n",
        "\n",
        "        # Create combined features for richer property descriptions\n",
        "        working_df['Combined_Features'] = (\n",
        "            working_df['Original_Style'].astype(str) + ' ' +\n",
        "            working_df['Structure_Type'].astype(str) + ' ' +\n",
        "            working_df['Property_Sub_Type'].astype(str)\n",
        "        )\n",
        "\n",
        "        # Standardize text data\n",
        "        working_df['Combined_Features'] = (working_df['Combined_Features']\n",
        "                                         .str.upper()\n",
        "                                         .str.strip())\n",
        "\n",
        "        # Separate known and unknown style properties\n",
        "        confident_mask = working_df['Style'] != 'Other'\n",
        "\n",
        "        # Log training data summary\n",
        "        logger.log_info(\"\\nTraining Data Summary:\")\n",
        "        logger.log_info(f\"Total properties: {len(working_df):,}\")\n",
        "        logger.log_info(f\"Confident examples: {confident_mask.sum():,}\")\n",
        "        logger.log_info(f\"Uncertain examples: {(~confident_mask).sum():,}\")\n",
        "\n",
        "        return working_df, confident_mask\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in prepare_style_classification_data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def train_style_classifier(df):\n",
        "    \"\"\"\n",
        "    Train a machine learning model to categorize property styles using multiple features.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): Input DataFrame containing property data\n",
        "\n",
        "    Returns:\n",
        "        tuple: Trained model, vectorizer, prediction results, and confident prediction mask\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Prepare data for classification\n",
        "        working_df, confident_mask = prepare_style_classification_data(df)\n",
        "\n",
        "        # Separate confident and uncertain styles\n",
        "        confident_styles = working_df[confident_mask].copy()\n",
        "        uncertain_styles = working_df[~confident_mask].copy()\n",
        "\n",
        "        # Create text vectorizer with enhanced features\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            analyzer='word',\n",
        "            ngram_range=(1, 3),\n",
        "            max_features=1500,\n",
        "            stop_words='english'\n",
        "        )\n",
        "\n",
        "        # Prepare training data\n",
        "        logger.log_info(\"\\nPreparing training data with enhanced features...\")\n",
        "        X = vectorizer.fit_transform(confident_styles['Combined_Features'])\n",
        "        y = confident_styles['Style'].values\n",
        "\n",
        "        logger.log_info(f\"Created {X.shape[1]} features from combined property descriptions\")\n",
        "\n",
        "        # Split data for training and testing\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        logger.log_info(\"\\nTraining the classifier...\")\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            random_state=42,\n",
        "            min_samples_leaf=5,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate model performance\n",
        "        y_pred = model.predict(X_test)\n",
        "        logger.log_info(\"\\nModel Performance:\")\n",
        "        performance_report = classification_report(y_test, y_pred)\n",
        "        logger.log_info(performance_report)\n",
        "\n",
        "        # Make predictions for uncertain styles\n",
        "        logger.log_info(\"\\nMaking predictions for uncertain styles...\")\n",
        "        uncertain_vectors = vectorizer.transform(uncertain_styles['Combined_Features'])\n",
        "        predictions = model.predict(uncertain_vectors)\n",
        "        prediction_probs = model.predict_proba(uncertain_vectors)\n",
        "\n",
        "        # Identify confident predictions\n",
        "        confidence_threshold = 0.7\n",
        "        confident_predictions = prediction_probs.max(axis=1) >= confidence_threshold\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results = pd.DataFrame({\n",
        "            'Original_Style': uncertain_styles['Combined_Features'].values,\n",
        "            'Predicted_Style': predictions,\n",
        "            'Confidence': prediction_probs.max(axis=1)\n",
        "        })\n",
        "\n",
        "        # Log prediction results\n",
        "        logger.log_info(f\"\\nPrediction Results:\")\n",
        "        logger.log_info(f\"Total uncertain styles: {len(uncertain_styles):,}\")\n",
        "        logger.log_info(f\"High confidence predictions: {confident_predictions.sum():,}\")\n",
        "        logger.log_info(f\"Low confidence predictions: {(~confident_predictions).sum():,}\")\n",
        "\n",
        "        return model, vectorizer, results, confident_predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in train_style_classifier: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def analyze_low_confidence_cases(predictions, confidence_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Analyze predictions that fell below the confidence threshold.\n",
        "\n",
        "    Args:\n",
        "        predictions (pd.DataFrame): Prediction results\n",
        "        confidence_threshold (float): Threshold for low confidence (default: 0.7)\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Analysis of low confidence prediction patterns\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Filter low confidence predictions\n",
        "        low_confidence = predictions[predictions['Confidence'] < confidence_threshold].copy()\n",
        "\n",
        "        logger.log_info(f\"\\nAnalysis of {len(low_confidence)} Low Confidence Predictions:\")\n",
        "\n",
        "        # Group by original description to see patterns\n",
        "        pattern_analysis = low_confidence.groupby('Original_Style').agg({\n",
        "            'Predicted_Style': lambda x: x.mode().iloc[0],\n",
        "            'Confidence': 'mean'\n",
        "        }).sort_values('Confidence', ascending=False)\n",
        "\n",
        "        logger.log_info(\"\\nCommon patterns in low confidence cases:\")\n",
        "        logger.log_info(pattern_analysis.head(10).to_string())\n",
        "\n",
        "        return pattern_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_low_confidence_cases: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Main execution block\n",
        "try:\n",
        "    logger.log_info(\"Starting advanced style classification...\")\n",
        "\n",
        "    # Run classification\n",
        "    model, vectorizer, predictions, confident_mask = train_style_classifier(combined_data)\n",
        "\n",
        "    # Update dataset with predictions\n",
        "    logger.log_info(\"\\nUpdating dataset with predictions...\")\n",
        "    uncertain_indices = combined_data[combined_data['Style'] == 'Other'].index\n",
        "    prediction_updates = pd.Series(\n",
        "        predictions[confident_mask]['Predicted_Style'].values,\n",
        "        index=uncertain_indices[:len(predictions[confident_mask])]\n",
        "    )\n",
        "\n",
        "    # Apply the predictions\n",
        "    combined_data.loc[prediction_updates.index, 'Style'] = prediction_updates\n",
        "\n",
        "    # Analyze low confidence cases\n",
        "    low_confidence_analysis = analyze_low_confidence_cases(predictions)\n",
        "\n",
        "    # Show final distribution\n",
        "    logger.log_info(\"\\nFinal Style Distribution:\")\n",
        "    style_counts = combined_data['Style'].value_counts()\n",
        "    total = len(combined_data)\n",
        "\n",
        "    for style, count in style_counts.items():\n",
        "        percentage = (count / total) * 100\n",
        "        logger.log_info(f\"{style}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Show remaining unclassified properties\n",
        "    remaining_other = (combined_data['Style'] == 'Other').sum()\n",
        "    logger.log_info(f\"\\nRemaining unclassified properties: {remaining_other:,}\")\n",
        "\n",
        "    # Log step completionq\n",
        "    logger.log_step_complete(\"Advanced style classification\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(\"\\nError during advanced style classification:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3uiQYYynDbt"
      },
      "source": [
        "# Data Analysis: Condo Property Assessment\n",
        "\n",
        "## Overview\n",
        "This code block performs a detailed analysis of condo-related information in our dataset. It examines condo types, naming patterns, fees, and development frequency, helping us understand the completeness and quality of our condo data.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Analysis Components\n",
        "1. Property Type Analysis\n",
        "   * Counts condo vs non-condo properties\n",
        "   * Identifies unique condo types\n",
        "   * Handles null values explicitly\n",
        "   * Reports distribution of types\n",
        "\n",
        "2. Condo Name Analysis\n",
        "   * Examines placeholder names:\n",
        "     * \"Z-name Not Listed\"\n",
        "     * \"No Name\"\n",
        "     * \"name not\" variations\n",
        "   * Counts regular named condos\n",
        "   * Tracks naming patterns\n",
        "   * Identifies missing names\n",
        "\n",
        "3. Fee Analysis\n",
        "   * Analyzes condo fee presence\n",
        "   * Breaks down by name type:\n",
        "     * Properties with fees\n",
        "     * Properties without fees\n",
        "   * Cross-references with naming patterns\n",
        "\n",
        "4. Development Analysis\n",
        "   * Identifies least common developments\n",
        "   * Excludes placeholder names\n",
        "   * Shows bottom 20 by frequency\n",
        "   * Helps identify rare properties\n",
        "\n",
        "### Quality Checks\n",
        "The code verifies:\n",
        "* Required column presence\n",
        "* Total count reconciliation\n",
        "* Category summation\n",
        "* Data completeness\n",
        "* Naming consistency\n",
        "\n",
        "### Results Tracking\n",
        "1. Property Statistics:\n",
        "   * Total property count\n",
        "   * Condo vs non-condo ratio\n",
        "   * Named vs unnamed condos\n",
        "   * Fee presence analysis\n",
        "\n",
        "2. Data Quality Metrics:\n",
        "   * Placeholder name usage\n",
        "   * Missing fee patterns\n",
        "   * Development frequency\n",
        "   * Category consistency\n",
        "\n",
        "## Process Protection\n",
        "* Validates required columns\n",
        "* Handles null values safely\n",
        "* Performs verification checks\n",
        "* Provides detailed logging\n",
        "* Includes error recovery\n",
        "\n",
        "## Future Enhancement Note\n",
        "This analysis provides valuable insights for potential future cleaning tasks:\n",
        "1. Standardizing condo names\n",
        "2. Addressing missing fees\n",
        "3. Validating property types\n",
        "4. Consolidating developments\n",
        "\n",
        "[Code block follows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mhpXPS_Jl_SM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f80f687-70b3-458d-a37e-f2db0396030a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting comprehensive condo analysis...\n",
            "Starting detailed condo property analysis...\n",
            "\n",
            "Analyzing property types...\n",
            "\n",
            "Unique Condo Types found:\n",
            "Bare Land: 18,629\n",
            "Bare Land Condo: 490\n",
            "Conventional: 100,457\n",
            "Conventional Condo: 2,997\n",
            "NaN: 48\n",
            "Not a Condo: 271,043\n",
            "\n",
            "Analyzing condo names...\n",
            "\n",
            "Analyzing condo fees...\n",
            "\n",
            "Identifying least common condo developments...\n",
            "\n",
            "Detailed Analysis Results:\n",
            "==================================================\n",
            "\n",
            "Property Counts:\n",
            "------------------------------\n",
            "Total properties in dataset: 393,664\n",
            "Non-condo properties: 271,043\n",
            "Total condo properties: 122,621\n",
            "\n",
            "Condo Name Breakdown:\n",
            "------------------------------\n",
            "Condos with regular names: 64,043\n",
            "Condos with 'Z-name Not Listed': 28,989\n",
            "Condos with 'No Name': 559\n",
            "Condos with 'name not': 29,030\n",
            "\n",
            "Condo Fee Analysis:\n",
            "------------------------------\n",
            "\n",
            "Z-name Not Listed:\n",
            "  With condo fees: 28,971\n",
            "  Without condo fees: 18\n",
            "\n",
            "No Name:\n",
            "  With condo fees: 558\n",
            "  Without condo fees: 1\n",
            "\n",
            "Name Not:\n",
            "  With condo fees: 29,012\n",
            "  Without condo fees: 18\n",
            "\n",
            "Least Common Condo Developments:\n",
            "------------------------------\n",
            "Forest Court: 1 properties\n",
            "Chinook Iii: 1 properties\n",
            "Huntsville Manor: 1 properties\n",
            "Argentry: 1 properties\n",
            "The Imperial: 1 properties\n",
            "Glenbrook Cres: 1 properties\n",
            "Silverdale: 1 properties\n",
            "Eagle Crossing: 1 properties\n",
            "Hearthstone Manor: 1 properties\n",
            "Auburn Bay by Stonecroft: 1 properties\n",
            "Millrise Manor: 1 properties\n",
            "Bowside Manor: 1 properties\n",
            "Maple Terrace: 1 properties\n",
            "Mission Crossing: 1 properties\n",
            "Wedgewood Gardens: 1 properties\n",
            "The Royal Crossing: 1 properties\n",
            "Oakmount Court: 1 properties\n",
            "Centr 733: 1 properties\n",
            "Manors: 1 properties\n",
            "Cambridge Gate: 1 properties\n",
            "Step 21 completed: Condo property analysis\n",
            "\n",
            "Analysis completed successfully! 🎉\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# CONDO DATA ANALYSIS - DETAILED BREAKDOWN AND STATISTICS\n",
        "# =================================================================================\n",
        "\n",
        "def analyze_condo_properties(df):\n",
        "    \"\"\"\n",
        "    Performs detailed analysis of condo properties, focusing on:\n",
        "    1. Property type counts (condo vs non-condo)\n",
        "    2. Analysis of placeholder condo names\n",
        "    3. Condo fee presence analysis\n",
        "    4. Identification of least common condo developments\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame containing MLS data\n",
        "\n",
        "    Returns:\n",
        "        dict: Comprehensive analysis results\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting detailed condo property analysis...\")\n",
        "\n",
        "        # First verify we have all required columns\n",
        "        required_columns = ['Condo_Type', 'Condo_Name', 'Condo_Fee']\n",
        "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {', '.join(missing_cols)}\")\n",
        "\n",
        "        # Initialize results dictionary\n",
        "        results = {}\n",
        "\n",
        "        # PART 1: Analyze Property Types\n",
        "        # ----------------------------------------\n",
        "        logger.log_info(\"\\nAnalyzing property types...\")\n",
        "\n",
        "        # Look at all unique Condo_Type values and handle NaN values properly\n",
        "        # Convert to list and handle NaN values explicitly\n",
        "        unique_types = df['Condo_Type'].unique()\n",
        "        # Convert any NaN values to string 'NaN' for display\n",
        "        unique_types = ['NaN' if pd.isna(x) else str(x) for x in unique_types]\n",
        "        unique_types.sort()  # Now we can safely sort strings\n",
        "\n",
        "        logger.log_info(\"\\nUnique Condo Types found:\")\n",
        "        for ctype in unique_types:\n",
        "            if ctype == 'NaN':\n",
        "                # Count NaN values in original column\n",
        "                count = df['Condo_Type'].isna().sum()\n",
        "            else:\n",
        "                # Count specific type\n",
        "                count = len(df[df['Condo_Type'] == ctype])\n",
        "            logger.log_info(f\"{ctype}: {count:,}\")\n",
        "\n",
        "        # Basic property counts\n",
        "        total_properties = len(df)\n",
        "        non_condo_count = len(df[df['Condo_Type'] == 'Not a Condo'])\n",
        "        total_condos = len(df[df['Condo_Type'] != 'Not a Condo'])\n",
        "\n",
        "        # PART 2: Analyze Condo Names\n",
        "        # ----------------------------------------\n",
        "        logger.log_info(\"\\nAnalyzing condo names...\")\n",
        "\n",
        "        # Count different types of placeholder names\n",
        "        # Note: We only look at actual condo properties here\n",
        "        condo_data = df[df['Condo_Type'] != 'Not a Condo'].copy()\n",
        "\n",
        "        # Handle potential NaN values in Condo_Name\n",
        "        condo_data['Condo_Name'] = condo_data['Condo_Name'].fillna('')\n",
        "\n",
        "        zname_mask = condo_data['Condo_Name'] == 'Z-name Not Listed'\n",
        "        no_name_mask = condo_data['Condo_Name'] == 'No Name'\n",
        "        name_not_mask = condo_data['Condo_Name'].str.contains(\n",
        "            'name not',\n",
        "            case=False,\n",
        "            na=False\n",
        "        )\n",
        "\n",
        "        zname_count = zname_mask.sum()\n",
        "        no_name_count = no_name_mask.sum()\n",
        "        name_not_count = name_not_mask.sum()\n",
        "\n",
        "        # Calculate condos with regular names (not placeholders)\n",
        "        regular_named_condos = total_condos - (zname_count + no_name_count + name_not_count)\n",
        "\n",
        "        # PART 3: Analyze Condo Fees\n",
        "        # ----------------------------------------\n",
        "        logger.log_info(\"\\nAnalyzing condo fees...\")\n",
        "\n",
        "        # Helper function to analyze fees for a specific subset of properties\n",
        "        def analyze_fees(data_subset):\n",
        "            has_fee = data_subset['Condo_Fee'].notna().sum()\n",
        "            no_fee = data_subset['Condo_Fee'].isna().sum()\n",
        "            return {'has_fee': has_fee, 'no_fee': no_fee}\n",
        "\n",
        "        # Analyze fees for each placeholder type\n",
        "        fee_analysis = {\n",
        "            'Z-name Not Listed': analyze_fees(condo_data[zname_mask]),\n",
        "            'No Name': analyze_fees(condo_data[no_name_mask]),\n",
        "            'Name Not': analyze_fees(condo_data[name_not_mask])\n",
        "        }\n",
        "\n",
        "        # PART 4: Find Least Common Condo Developments\n",
        "        # ----------------------------------------\n",
        "        logger.log_info(\"\\nIdentifying least common condo developments...\")\n",
        "\n",
        "        # Get counts for actual condo names (excluding placeholders)\n",
        "        valid_condos = condo_data[\n",
        "            ~(zname_mask | no_name_mask | name_not_mask) &\n",
        "            (condo_data['Condo_Name'] != '')  # Exclude empty strings\n",
        "        ]\n",
        "\n",
        "        condo_counts = valid_condos['Condo_Name'].value_counts()\n",
        "        bottom_20 = condo_counts.nsmallest(20)\n",
        "\n",
        "        # PART 5: Store Results\n",
        "        # ----------------------------------------\n",
        "        results.update({\n",
        "            'property_counts': {\n",
        "                'total_properties': total_properties,\n",
        "                'non_condo_count': non_condo_count,\n",
        "                'total_condos': total_condos,\n",
        "                'regular_named_condos': regular_named_condos\n",
        "            },\n",
        "            'placeholder_counts': {\n",
        "                'zname_count': zname_count,\n",
        "                'no_name_count': no_name_count,\n",
        "                'name_not_count': name_not_count\n",
        "            },\n",
        "            'fee_analysis': fee_analysis,\n",
        "            'least_common_condos': bottom_20.to_dict()\n",
        "        })\n",
        "\n",
        "        # PART 6: Log Detailed Results\n",
        "        # ----------------------------------------\n",
        "        logger.log_info(\"\\nDetailed Analysis Results:\")\n",
        "        logger.log_info(\"=\" * 50)\n",
        "\n",
        "        # Property counts\n",
        "        logger.log_info(\"\\nProperty Counts:\")\n",
        "        logger.log_info(\"-\" * 30)\n",
        "        logger.log_info(f\"Total properties in dataset: {total_properties:,}\")\n",
        "        logger.log_info(f\"Non-condo properties: {non_condo_count:,}\")\n",
        "        logger.log_info(f\"Total condo properties: {total_condos:,}\")\n",
        "\n",
        "        # Condo name breakdown\n",
        "        logger.log_info(\"\\nCondo Name Breakdown:\")\n",
        "        logger.log_info(\"-\" * 30)\n",
        "        logger.log_info(f\"Condos with regular names: {regular_named_condos:,}\")\n",
        "        logger.log_info(f\"Condos with 'Z-name Not Listed': {zname_count:,}\")\n",
        "        logger.log_info(f\"Condos with 'No Name': {no_name_count:,}\")\n",
        "        logger.log_info(f\"Condos with 'name not': {name_not_count:,}\")\n",
        "\n",
        "        # Condo fee analysis\n",
        "        logger.log_info(\"\\nCondo Fee Analysis:\")\n",
        "        logger.log_info(\"-\" * 30)\n",
        "        for placeholder, stats in fee_analysis.items():\n",
        "            logger.log_info(f\"\\n{placeholder}:\")\n",
        "            logger.log_info(f\"  With condo fees: {stats['has_fee']:,}\")\n",
        "            logger.log_info(f\"  Without condo fees: {stats['no_fee']:,}\")\n",
        "\n",
        "        # Least common condos\n",
        "        logger.log_info(\"\\nLeast Common Condo Developments:\")\n",
        "        logger.log_info(\"-\" * 30)\n",
        "        for condo_name, count in bottom_20.items():\n",
        "            logger.log_info(f\"{condo_name}: {count:,} properties\")\n",
        "\n",
        "        # Verification checks\n",
        "        total_accounted = (regular_named_condos + zname_count +\n",
        "                          no_name_count + name_not_count)\n",
        "\n",
        "        if total_accounted != total_condos:\n",
        "            logger.log_error(\"\\nWARNING: Condo counts don't match total!\")\n",
        "            logger.log_error(f\"Total condos: {total_condos:,}\")\n",
        "            logger.log_error(f\"Sum of categories: {total_accounted:,}\")\n",
        "            logger.log_error(f\"Difference: {total_condos - total_accounted:,}\")\n",
        "\n",
        "        logger.log_step_complete(\"Condo property analysis\")\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"\\nError during condo analysis:\")\n",
        "        logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "        logger.log_error(f\"Details: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the analysis\n",
        "try:\n",
        "    logger.log_info(\"Starting comprehensive condo analysis...\")\n",
        "\n",
        "    # Run the analysis\n",
        "    condo_analysis_results = analyze_condo_properties(combined_data)\n",
        "\n",
        "    logger.log_info(\"\\nAnalysis completed successfully! 🎉\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nError during execution:\")\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\")\n",
        "    logger.log_error(f\"Details: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8jTbRb5liZv"
      },
      "source": [
        "# Machine Learning for Condo Name Prediction\n",
        "## Pipeline Status Check\n",
        "\n",
        "### Data Input Issue\n",
        "❗ **Important Pipeline Issue**: The code is using `combined_data_clean`, but we want to maintain consistency with our main `combined_data` variable. Let me explain how we should modify this code and why it's important.\n",
        "\n",
        "Let me walk you through this advanced machine learning analysis step by step, explaining how it helps us clean our real estate data.\n",
        "\n",
        "### What This Code Does and Why It Matters\n",
        "\n",
        "Think of this code like a sophisticated real estate expert who has learned patterns about how condos are typically named. Just as an experienced agent might say \"Based on this location and year built, this is probably part of the Riverside Gardens complex,\" our machine learning model makes educated guesses about missing condo names using patterns it learns from our existing data.\n",
        "\n",
        "The code consists of several specialized functions working together:\n",
        "\n",
        "1. **Data Preparation** (`prepare_condo_data_cpu`):\n",
        "   - This function is like a data chef, preparing our raw real estate information into a format our machine learning model can understand\n",
        "   - It carefully handles different types of information: numbers (like year built), locations (using latitude/longitude), and categories (like postal codes)\n",
        "   - It's particularly careful with missing values, using smart strategies to fill in gaps without compromising data quality\n",
        "\n",
        "2. **Confidence Analysis** (`analyze_confidence_patterns`):\n",
        "   - This acts like a quality control inspector, checking how sure we are about each prediction\n",
        "   - It looks for patterns in our confidence levels: Are we more certain about newer buildings? Do some areas have more reliable predictions than others?\n",
        "\n",
        "3. **Model Training** (`train_model_with_feature_selection`):\n",
        "   - This is where the actual learning happens, like training our real estate expert\n",
        "   - It uses a two-step process to focus on the most important patterns:\n",
        "     * First, it identifies which pieces of information are most helpful for predicting condo names\n",
        "     * Then, it builds a final model focusing on just these important features\n",
        "\n",
        "4. **Main Prediction Pipeline** (`predict_condo_names`):\n",
        "   - This orchestrates the entire process, like a project manager ensuring everything works together smoothly\n",
        "   - It only makes predictions when it's reasonably confident (default 70% confidence threshold)\n",
        "   - It carefully tracks which names were predicted versus original\n",
        "\n",
        "### Pipeline Correction\n",
        "To maintain our data pipeline consistency, we should modify the final line to:\n",
        "\n",
        "```python\n",
        "# Run the enhanced prediction pipeline with our main pipeline variable\n",
        "enhanced_data, final_model, feature_importance = predict_condo_names(\n",
        "    combined_data,  # Using our main pipeline variable\n",
        "    confidence_threshold=0.7,\n",
        "    feature_threshold=0.90\n",
        ")\n",
        "\n",
        "# Save results back to our main pipeline variable\n",
        "combined_data = enhanced_data\n",
        "```\n",
        "\n",
        "### Why This Step Matters\n",
        "This machine learning step is crucial because:\n",
        "1. It helps fill in missing condo names in a systematic, data-driven way\n",
        "2. It only makes predictions when it's reasonably confident\n",
        "3. It maintains data quality by tracking which names are original versus predicted\n",
        "4. It provides insights into prediction reliability across different areas and property types\n",
        "\n",
        "Would you like me to explain any part of this process in more detail? For instance, I could elaborate on how the model makes its predictions or how we determine confidence levels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IQ-eZv2OKmS"
      },
      "source": [
        "### **Condo Name Analysis and Standardization**\n",
        "\n",
        "This code block focuses on cleaning and analyzing condo names to identify potential duplicates, variations, and standardization opportunities. Here's what it does:\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - Imports necessary libraries (`pandas`, `numpy`, `difflib.SequenceMatcher`).\n",
        "   - Defines a function `load_initial_data` to clean and prepare the dataset:\n",
        "     - Creates a copy of the input DataFrame to avoid modifying the original.\n",
        "     - Checks for the presence of the `Condo_Name` column.\n",
        "     - Cleans condo names by removing leading/trailing whitespace.\n",
        "     - Logs basic statistics (total records, unique names, missing names).\n",
        "\n",
        "2. **Condo Name Analysis**:\n",
        "   - Defines a function `analyze_condo_names` to identify similar condo names:\n",
        "     - Drops null values and converts condo names to strings.\n",
        "     - Compares each condo name with every other name using `SequenceMatcher` to calculate similarity ratios.\n",
        "     - Identifies potential duplicates with similarity ratios > 0.8 but < 1.0.\n",
        "     - Logs and sorts potential duplicates by similarity.\n",
        "\n",
        "3. **Standardization Suggestions**:\n",
        "   - Proposes sets of names to standardize (e.g., spelling variations like \"Dorchester Square\" vs. \"Dorchestor Square\").\n",
        "   - Identifies names to keep distinct (e.g., \"Copperfield Park\" vs. \"Copperfield Park II\").\n",
        "   - Logs proposed standardizations and distinct pairs.\n",
        "\n",
        "4. **Execution**:\n",
        "   - Loads and prepares the input DataFrame (`combined_data`).\n",
        "   - Runs the condo name analysis and logs the results.\n",
        "\n",
        "### **Key Features**:\n",
        "- **Data Cleaning**: Ensures condo names are clean and consistent.\n",
        "- **Duplicate Detection**: Identifies potential duplicates and variations using string similarity.\n",
        "- **Standardization**: Provides suggestions for standardizing condo names and keeping distinct names separate.\n",
        "- **Logging**: Tracks the process and outputs detailed results for review.\n",
        "\n",
        "### **Future Enhancements**:\n",
        "- Automate the standardization process based on similarity thresholds.\n",
        "- Expand the analysis to include additional columns (e.g., addresses, fees).\n",
        "- Integrate with a database for real-time updates and corrections."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CONDO NAME ANALYSIS AND VARIATION DETECTION\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This code block analyzes condo names in the dataset to:\n",
        "1. Identify potential variations and duplicates in condo names\n",
        "2. Find similar names that should be standardized\n",
        "3. Track distinct names that should remain separate\n",
        "4. Prepare data for subsequent standardization steps\n",
        "\n",
        "The analysis uses string similarity matching to find potential variations\n",
        "of the same condo name, helping us clean and standardize our data.\n",
        "\"\"\"\n",
        "\n",
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# Step 2: Load and prepare initial data\n",
        "def load_initial_data(df):\n",
        "    \"\"\"\n",
        "    Loads and prepares the initial dataset for condo name analysis.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas DataFrame\n",
        "        The raw input DataFrame containing our condo data\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas DataFrame\n",
        "        Cleaned DataFrame ready for analysis\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Loading and preparing initial data...\")\n",
        "    logger.log_info(\"=\" * 50)\n",
        "\n",
        "    # Make a copy to avoid modifying the original\n",
        "    initial_data = df.copy()\n",
        "\n",
        "    # Verify we have the required column\n",
        "    if 'Condo_Name' not in initial_data.columns:\n",
        "        logger.log_error(\"DataFrame must contain a 'Condo_Name' column\")\n",
        "        raise ValueError(\"DataFrame must contain a 'Condo_Name' column\")\n",
        "\n",
        "    # Basic data cleaning\n",
        "    logger.log_info(\"\\nCleaning data...\")\n",
        "    # Remove any leading/trailing whitespace from condo names\n",
        "    initial_data['Condo_Name'] = initial_data['Condo_Name'].str.strip()\n",
        "\n",
        "    # Print basic statistics\n",
        "    total_records = len(initial_data)\n",
        "    unique_names = initial_data['Condo_Name'].nunique()\n",
        "    null_names = initial_data['Condo_Name'].isna().sum()\n",
        "\n",
        "    logger.log_info(f\"\\nInitial data statistics:\")\n",
        "    logger.log_info(f\"Total records: {total_records:,}\")\n",
        "    logger.log_info(f\"Unique condo names: {unique_names:,}\")\n",
        "    logger.log_info(f\"Records with missing names: {null_names:,}\")\n",
        "\n",
        "    logger.log_step_complete(\"Data loading and preparation\")\n",
        "    return initial_data\n",
        "\n",
        "# Step 3: Now we can use our analyze_condo_names function\n",
        "def analyze_condo_names(df):\n",
        "    \"\"\"\n",
        "    Analyzes condo names to identify potential duplicates and variations.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas DataFrame\n",
        "        DataFrame containing condo information with a 'Condo_Name' column\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing analysis results and suggested corrections\n",
        "    \"\"\"\n",
        "    logger.log_info(\"\\nAnalyzing condo names...\")\n",
        "    logger.log_info(\"=\" * 50)\n",
        "\n",
        "    # Remove any null values and convert to string type\n",
        "    valid_names = df['Condo_Name'].dropna().astype(str)\n",
        "\n",
        "    # Get unique condo names\n",
        "    condo_names = sorted(valid_names.unique())\n",
        "    logger.log_info(f\"Total unique condo names: {len(condo_names)}\")\n",
        "\n",
        "    # Store potential matches\n",
        "    potential_duplicates = []\n",
        "\n",
        "    # Compare each name with every other name\n",
        "    logger.log_info(\"\\nLooking for similar names...\")\n",
        "    for i, name1 in enumerate(condo_names):\n",
        "        for name2 in condo_names[i+1:]:\n",
        "            # Skip empty strings or whitespace-only strings\n",
        "            if not name1.strip() or not name2.strip():\n",
        "                continue\n",
        "\n",
        "            # Calculate similarity ratio\n",
        "            ratio = SequenceMatcher(None,\n",
        "                                  name1.lower(),\n",
        "                                  name2.lower()).ratio()\n",
        "\n",
        "            # If names are very similar but not identical\n",
        "            if ratio > 0.8 and ratio < 1.0:\n",
        "                potential_duplicates.append({\n",
        "                    'name1': name1,\n",
        "                    'name2': name2,\n",
        "                    'similarity': ratio\n",
        "                })\n",
        "\n",
        "    # Sort by similarity\n",
        "    potential_duplicates.sort(key=lambda x: x['similarity'], reverse=True)\n",
        "\n",
        "    logger.log_info(\"\\nPotential name variations found:\")\n",
        "    logger.log_info(\"-\" * 30)\n",
        "    for dup in potential_duplicates:\n",
        "        logger.log_info(f\"\\nSimilarity: {dup['similarity']:.3f}\")\n",
        "        logger.log_info(f\"Name 1: {dup['name1']}\")\n",
        "        logger.log_info(f\"Name 2: {dup['name2']}\")\n",
        "\n",
        "    # Create sets of names to standardize\n",
        "    standardization_sets = [\n",
        "        # Spelling variations (same building)\n",
        "        {'Dorchester Square', 'Dorchestor Square'},\n",
        "        {'Courtyards of Garrison Woods', 'Courtyards At Garrison Woods'},\n",
        "        {'Mosaic in McKenzie Towne', 'The Mosaic In Mckenzie Town'},\n",
        "    ]\n",
        "\n",
        "    # Create sets of names to keep distinct\n",
        "    distinct_pairs = [\n",
        "        ('Copperfield Park', 'Copperfield Park II'),\n",
        "        ('KeyNote', 'KeyNote 2'),\n",
        "        ('Bridlecrest Pointe', 'Bridleview Pointe'),\n",
        "        ('Tarjan Place', 'Tarjan Pointe'),\n",
        "    ]\n",
        "\n",
        "    logger.log_info(\"\\nProposed standardizations:\")\n",
        "    logger.log_info(\"-\" * 30)\n",
        "    for name_set in standardization_sets:\n",
        "        logger.log_info(f\"\\nNames to standardize to '{list(name_set)[0]}':\")\n",
        "        for name in list(name_set)[1:]:\n",
        "            logger.log_info(f\"- {name}\")\n",
        "\n",
        "    logger.log_info(\"\\nNames to keep distinct:\")\n",
        "    logger.log_info(\"-\" * 30)\n",
        "    for name1, name2 in distinct_pairs:\n",
        "        logger.log_info(f\"\\n{name1} vs {name2}\")\n",
        "\n",
        "    logger.log_step_complete(\"Condo name analysis\")\n",
        "    return {\n",
        "        'potential_duplicates': potential_duplicates,\n",
        "        'standardization_sets': standardization_sets,\n",
        "        'distinct_pairs': distinct_pairs,\n",
        "        'total_unique_names': len(condo_names)\n",
        "    }\n",
        "\n",
        "# Step 4: Now we can run our analysis properly\n",
        "# First, load and prepare the data\n",
        "# Assuming your original DataFrame is called 'combined_data'\n",
        "training_data = load_initial_data(combined_data)\n",
        "\n",
        "# Then run the analysis\n",
        "name_analysis = analyze_condo_names(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnJMoIbKXxvf",
        "outputId": "015d2077-c1f7-4f20-f4b2-7097a9e105b1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing initial data...\n",
            "==================================================\n",
            "\n",
            "Cleaning data...\n",
            "\n",
            "Initial data statistics:\n",
            "Total records: 393,664\n",
            "Unique condo names: 1,665\n",
            "Records with missing names: 285,307\n",
            "Step 22 completed: Data loading and preparation\n",
            "\n",
            "Analyzing condo names...\n",
            "==================================================\n",
            "Total unique condo names: 1665\n",
            "\n",
            "Looking for similar names...\n",
            "\n",
            "Potential name variations found:\n",
            "------------------------------\n",
            "\n",
            "Similarity: 0.980\n",
            "Name 1: Montage in Mackenzie Towne\n",
            "Name 2: Montage in McKenzie Towne\n",
            "\n",
            "Similarity: 0.976\n",
            "Name 1: Point Mckay Phase II\n",
            "Name 2: Point Mckay Phase III\n",
            "\n",
            "Similarity: 0.974\n",
            "Name 1: Copperfield Park II\n",
            "Name 2: Copperfield Park III\n",
            "\n",
            "Similarity: 0.974\n",
            "Name 1: Point Mckay Phase I\n",
            "Name 2: Point Mckay Phase II\n",
            "\n",
            "Similarity: 0.974\n",
            "Name 1: Rundleson Village I\n",
            "Name 2: Rundleson Village II\n",
            "\n",
            "Similarity: 0.974\n",
            "Name 1: Whitehorn Village I\n",
            "Name 2: Whitehorn Village II\n",
            "\n",
            "Similarity: 0.973\n",
            "Name 1: Chateau Strathcona\n",
            "Name 2: Chateaux Strathcona\n",
            "\n",
            "Similarity: 0.973\n",
            "Name 1: Hanson Creek Manor\n",
            "Name 2: Hanson Creek Manors\n",
            "\n",
            "Similarity: 0.973\n",
            "Name 1: High Pointe Estates\n",
            "Name 2: Highpointe Estates\n",
            "\n",
            "Similarity: 0.973\n",
            "Name 1: Riverside Towers I\n",
            "Name 2: Riverside Towers II\n",
            "\n",
            "Similarity: 0.973\n",
            "Name 1: The Fairway Vistas\n",
            "Name 2: The Fairways Vistas\n",
            "\n",
            "Similarity: 0.971\n",
            "Name 1: Devonshire Estate\n",
            "Name 2: Devonshire Estates\n",
            "\n",
            "Similarity: 0.971\n",
            "Name 1: Fairway Pavilions\n",
            "Name 2: Fairways Pavilions\n",
            "\n",
            "Similarity: 0.971\n",
            "Name 1: Five West Phase I\n",
            "Name 2: Five West Phase II\n",
            "\n",
            "Similarity: 0.970\n",
            "Name 1: Calvanna Village\n",
            "Name 2: Calvannah Village\n",
            "\n",
            "Similarity: 0.970\n",
            "Name 1: Hampstead Estates\n",
            "Name 2: Hamstead Estates\n",
            "\n",
            "Similarity: 0.970\n",
            "Name 1: Huntsview Village\n",
            "Name 2: Huntview Village\n",
            "\n",
            "Similarity: 0.970\n",
            "Name 1: Imperial Place I\n",
            "Name 2: Imperial Place II\n",
            "\n",
            "Similarity: 0.970\n",
            "Name 1: Mackenzie Village\n",
            "Name 2: McKenzie Village\n",
            "\n",
            "Similarity: 0.970\n",
            "Name 1: Oak Hampton Court\n",
            "Name 2: Oakhampton Court\n",
            "\n",
            "Similarity: 0.968\n",
            "Name 1: Chrystal Heights\n",
            "Name 2: Crystal Heights\n",
            "\n",
            "Similarity: 0.968\n",
            "Name 1: Discovery Point\n",
            "Name 2: Discovery Pointe\n",
            "\n",
            "Similarity: 0.968\n",
            "Name 1: Sierra's West I\n",
            "Name 2: Sierra's West Ii\n",
            "\n",
            "Similarity: 0.966\n",
            "Name 1: Midpark Garden\n",
            "Name 2: Midpark Gardens\n",
            "\n",
            "Similarity: 0.963\n",
            "Name 1: Horizon Village - Coach Hill\n",
            "Name 2: Horizon Village Coach Hill\n",
            "\n",
            "Similarity: 0.963\n",
            "Name 1: The Pavilions\n",
            "Name 2: The Pavillions\n",
            "\n",
            "Similarity: 0.960\n",
            "Name 1: Bella Rosa I\n",
            "Name 2: Bella Rosa II\n",
            "\n",
            "Similarity: 0.960\n",
            "Name 1: Holly Spring\n",
            "Name 2: Holly Springs\n",
            "\n",
            "Similarity: 0.960\n",
            "Name 1: Refrew House\n",
            "Name 2: Renfrew House\n",
            "\n",
            "Similarity: 0.960\n",
            "Name 1: Sierra Grand\n",
            "Name 2: Sierra Grande\n",
            "\n",
            "Similarity: 0.958\n",
            "Name 1: Laural House At The Park\n",
            "Name 2: Laurel House At The Park\n",
            "\n",
            "Similarity: 0.957\n",
            "Name 1: Patina Park\n",
            "Name 2: Pattina Park\n",
            "\n",
            "Similarity: 0.957\n",
            "Name 1: Pattina Park\n",
            "Name 2: Pattna Park\n",
            "\n",
            "Similarity: 0.957\n",
            "Name 1: Pontefino I\n",
            "Name 2: Pontefino II\n",
            "\n",
            "Similarity: 0.957\n",
            "Name 1: The Terrace\n",
            "Name 2: The Terraces\n",
            "\n",
            "Similarity: 0.957\n",
            "Name 1: The Windsor\n",
            "Name 2: The Windsors\n",
            "\n",
            "Similarity: 0.957\n",
            "Name 1: Wood Meadows\n",
            "Name 2: Woodmeadows\n",
            "\n",
            "Similarity: 0.952\n",
            "Name 1: Cheasapeake\n",
            "Name 2: Chesapeake\n",
            "\n",
            "Similarity: 0.952\n",
            "Name 1: Chinook Ii\n",
            "Name 2: Chinook Iii\n",
            "\n",
            "Similarity: 0.952\n",
            "Name 1: Park Point\n",
            "Name 2: Park Pointe\n",
            "\n",
            "Similarity: 0.952\n",
            "Name 1: Riverrun I\n",
            "Name 2: Riverrun II\n",
            "\n",
            "Similarity: 0.952\n",
            "Name 1: Spirit Of Marda Loop\n",
            "Name 2: Spirit Of Marda Loop 2\n",
            "\n",
            "Similarity: 0.952\n",
            "Name 1: Sun Village\n",
            "Name 2: Sunvillage\n",
            "\n",
            "Similarity: 0.950\n",
            "Name 1: Point Mckay Phase I\n",
            "Name 2: Point Mckay Phase III\n",
            "\n",
            "Similarity: 0.950\n",
            "Name 1: Rocky Ridge Chateaus\n",
            "Name 2: Rocky Ridge Chateaux\n",
            "\n",
            "Similarity: 0.947\n",
            "Name 1: Alpine Park Phase 1\n",
            "Name 2: Alpine Park Phase 2\n",
            "\n",
            "Similarity: 0.947\n",
            "Name 1: Centr 733\n",
            "Name 2: Centro 733\n",
            "\n",
            "Similarity: 0.947\n",
            "Name 1: Iron Horse\n",
            "Name 2: Ironhorse\n",
            "\n",
            "Similarity: 0.947\n",
            "Name 1: Kensington Mews Ph1\n",
            "Name 2: Kensington Mews Ph2\n",
            "\n",
            "Similarity: 0.947\n",
            "Name 1: Royal Oak\n",
            "Name 2: Royal Oaks\n",
            "\n",
            "Similarity: 0.947\n",
            "Name 1: Wingate of Pumphill\n",
            "Name 2: Wyngate of Pumphill\n",
            "\n",
            "Similarity: 0.944\n",
            "Name 1: Whitehorn Village\n",
            "Name 2: Whitehorn Village I\n",
            "\n",
            "Similarity: 0.944\n",
            "Name 1: Z - Name Not Listed\n",
            "Name 2: Z-Name Not Listed\n",
            "\n",
            "Similarity: 0.944\n",
            "Name 1: Z - Name Not Listed\n",
            "Name 2: Z-name Not Listed\n",
            "\n",
            "Similarity: 0.941\n",
            "Name 1: Dorchester Square\n",
            "Name 2: Dorchestor Square\n",
            "\n",
            "Similarity: 0.941\n",
            "Name 1: Hunterview Village\n",
            "Name 2: Huntview Village\n",
            "\n",
            "Similarity: 0.941\n",
            "Name 1: West Edge\n",
            "Name 2: Westedge\n",
            "\n",
            "Similarity: 0.938\n",
            "Name 1: Lakeview Green 1\n",
            "Name 2: Lakeview Green 2\n",
            "\n",
            "Similarity: 0.938\n",
            "Name 1: Lakeview Green 1\n",
            "Name 2: Lakeview Green 3\n",
            "\n",
            "Similarity: 0.938\n",
            "Name 1: Lakeview Green 2\n",
            "Name 2: Lakeview Green 3\n",
            "\n",
            "Similarity: 0.936\n",
            "Name 1: Gateway Garrison Green\n",
            "Name 2: Gateway of Garrison Green\n",
            "\n",
            "Similarity: 0.933\n",
            "Name 1: Falcon Terrace\n",
            "Name 2: Falconer Terrace\n",
            "\n",
            "Similarity: 0.933\n",
            "Name 1: Georgian Villa\n",
            "Name 2: Georgian Village\n",
            "\n",
            "Similarity: 0.929\n",
            "Name 1: Courtyards At Garrison Woods\n",
            "Name 2: Courtyards of Garrison Woods\n",
            "\n",
            "Similarity: 0.927\n",
            "Name 1: Outlook @ Waterfront\n",
            "Name 2: Outlook at Waterfront\n",
            "\n",
            "Similarity: 0.927\n",
            "Name 1: Valley Park Estates\n",
            "Name 2: YV_Valley Park Estates\n",
            "\n",
            "Similarity: 0.923\n",
            "Name 1: Foxboro House\n",
            "Name 2: Roxboro House\n",
            "\n",
            "Similarity: 0.923\n",
            "Name 1: Rundle Villa\n",
            "Name 2: Rundle Village\n",
            "\n",
            "Similarity: 0.923\n",
            "Name 1: Savana\n",
            "Name 2: Savanna\n",
            "\n",
            "Similarity: 0.923\n",
            "Name 1: The Manhattan\n",
            "Name 2: The Manhatten\n",
            "\n",
            "Similarity: 0.923\n",
            "Name 1: The Vista At Richmond Hill\n",
            "Name 2: The Vista In Richmond Hill\n",
            "\n",
            "Similarity: 0.920\n",
            "Name 1: The Waterford of Bridgeland\n",
            "Name 2: Waterford of Bridgeland\n",
            "\n",
            "Similarity: 0.919\n",
            "Name 1: Bonavista Estates\n",
            "Name 2: Bonavista Estates II\n",
            "\n",
            "Similarity: 0.919\n",
            "Name 1: Whitehorn Village\n",
            "Name 2: Whitehorn Village II\n",
            "\n",
            "Similarity: 0.917\n",
            "Name 1: Bella Rosa 1\n",
            "Name 2: Bella Rosa I\n",
            "\n",
            "Similarity: 0.914\n",
            "Name 1: Berkshire Village\n",
            "Name 2: Brookshire Village\n",
            "\n",
            "Similarity: 0.914\n",
            "Name 1: Briar Oak Estates\n",
            "Name 2: Briar Park Estates\n",
            "\n",
            "Similarity: 0.914\n",
            "Name 1: Brookshire Village\n",
            "Name 2: Brookside Village\n",
            "\n",
            "Similarity: 0.914\n",
            "Name 1: Christie Garden Est\n",
            "Name 2: Christie Gardens\n",
            "\n",
            "Similarity: 0.914\n",
            "Name 1: Copperfield Park\n",
            "Name 2: Copperfield Park II\n",
            "\n",
            "Similarity: 0.914\n",
            "Name 1: Devonshire Village\n",
            "Name 2: Devonshire Villas\n",
            "\n",
            "Similarity: 0.914\n",
            "Name 1: Glenbrooke Village\n",
            "Name 2: Penbrooke Village\n",
            "\n",
            "Similarity: 0.914\n",
            "Name 1: Hunterview Village\n",
            "Name 2: Huntsview Village\n",
            "\n",
            "Similarity: 0.914\n",
            "Name 1: Landmark Estates\n",
            "Name 2: Landmark Estates II\n",
            "\n",
            "Similarity: 0.914\n",
            "Name 1: Royal Oak Estates\n",
            "Name 2: Royal Park Estates\n",
            "\n",
            "Similarity: 0.909\n",
            "Name 1: Abbeydale Village\n",
            "Name 2: Abbeydale Villas\n",
            "\n",
            "Similarity: 0.909\n",
            "Name 1: Hardwood Estates\n",
            "Name 2: Heartwood Estates\n",
            "\n",
            "Similarity: 0.909\n",
            "Name 1: Patina Park\n",
            "Name 2: Pattna Park\n",
            "\n",
            "Similarity: 0.906\n",
            "Name 1: Residence Of Mount Royal\n",
            "Name 2: The Residences of Mount Royal\n",
            "\n",
            "Similarity: 0.905\n",
            "Name 1: The Waterford of Erlton\n",
            "Name 2: Waterford of Erlton\n",
            "\n",
            "Similarity: 0.903\n",
            "Name 1: Arlington House\n",
            "Name 2: Harrington House\n",
            "\n",
            "Similarity: 0.903\n",
            "Name 1: Heritage Village\n",
            "Name 2: Heritage Villas\n",
            "\n",
            "Similarity: 0.903\n",
            "Name 1: Highwood Gardens\n",
            "Name 2: Highwood Greens\n",
            "\n",
            "Similarity: 0.903\n",
            "Name 1: Mountainview Parc\n",
            "Name 2: Mountview Parc\n",
            "\n",
            "Similarity: 0.902\n",
            "Name 1: Mosaic in McKenzie Towne\n",
            "Name 2: The Mosaic In McKenzie Town\n",
            "\n",
            "Similarity: 0.902\n",
            "Name 1: Mosaic in McKenzie Towne\n",
            "Name 2: The Mosaic In Mckenzie Town\n",
            "\n",
            "Similarity: 0.900\n",
            "Name 1: Parke Place\n",
            "Name 2: Parkplace\n",
            "\n",
            "Similarity: 0.900\n",
            "Name 1: Pontefino\n",
            "Name 2: Pontefino I\n",
            "\n",
            "Similarity: 0.897\n",
            "Name 1: Hampton Court\n",
            "Name 2: Oakhampton Court\n",
            "\n",
            "Similarity: 0.897\n",
            "Name 1: Regency Gardens\n",
            "Name 2: Regent Gardens\n",
            "\n",
            "Similarity: 0.897\n",
            "Name 1: Varsity Village\n",
            "Name 2: Varsity Villas\n",
            "\n",
            "Similarity: 0.895\n",
            "Name 1: Fairway Pavilions\n",
            "Name 2: The Fairway Pavilions\n",
            "\n",
            "Similarity: 0.895\n",
            "Name 1: Glenbrook Village II\n",
            "Name 2: Glenbrooke Village\n",
            "\n",
            "Similarity: 0.889\n",
            "Name 1: Birchwood\n",
            "Name 2: Brichwood\n",
            "\n",
            "Similarity: 0.889\n",
            "Name 1: Copperfield Park\n",
            "Name 2: Copperfield Park III\n",
            "\n",
            "Similarity: 0.889\n",
            "Name 1: KeyNote 1\n",
            "Name 2: KeyNote 2\n",
            "\n",
            "Similarity: 0.889\n",
            "Name 1: Mount Royal Terrace\n",
            "Name 2: Mt. Royal Terrace\n",
            "\n",
            "Similarity: 0.889\n",
            "Name 1: The Royal Crossing\n",
            "Name 2: The Royal Crowning\n",
            "\n",
            "Similarity: 0.885\n",
            "Name 1: Horizon Village Strathaven\n",
            "Name 2: Horizon Village Strathcona\n",
            "\n",
            "Similarity: 0.882\n",
            "Name 1: Gladstone Gardens\n",
            "Name 2: Sandstone Gardens\n",
            "\n",
            "Similarity: 0.882\n",
            "Name 1: Glenbrook Villas\n",
            "Name 2: Glenbrooke Village\n",
            "\n",
            "Similarity: 0.882\n",
            "Name 1: Holly Dale Club\n",
            "Name 2: The Holly Dale Club\n",
            "\n",
            "Similarity: 0.880\n",
            "Name 1: Bella Rosa 1\n",
            "Name 2: Bella Rosa II\n",
            "\n",
            "Similarity: 0.880\n",
            "Name 1: Country Lane\n",
            "Name 2: Coventry Lane\n",
            "\n",
            "Similarity: 0.880\n",
            "Name 1: Glamis Garden\n",
            "Name 2: Glamis Green\n",
            "\n",
            "Similarity: 0.880\n",
            "Name 1: Legacy Park\n",
            "Name 2: Legacy Park 11\n",
            "\n",
            "Similarity: 0.880\n",
            "Name 1: Oxford Place\n",
            "Name 2: Wexford Place\n",
            "\n",
            "Similarity: 0.880\n",
            "Name 1: Parke Place\n",
            "Name 2: Parkside Place\n",
            "\n",
            "Similarity: 0.880\n",
            "Name 1: Regency Court\n",
            "Name 2: Regent Court\n",
            "\n",
            "Similarity: 0.875\n",
            "Name 1: Bankview Terrace\n",
            "Name 2: Parkview Terrace\n",
            "\n",
            "Similarity: 0.875\n",
            "Name 1: Baton Apartments\n",
            "Name 2: Ebony Apartments\n",
            "\n",
            "Similarity: 0.875\n",
            "Name 1: Chaparal Villas\n",
            "Name 2: Chaparral Village\n",
            "\n",
            "Similarity: 0.875\n",
            "Name 1: Creek Crossing\n",
            "Name 2: Creekside Crossing\n",
            "\n",
            "Similarity: 0.875\n",
            "Name 1: Ironwood Estates\n",
            "Name 2: Pinewood Estates\n",
            "\n",
            "Similarity: 0.875\n",
            "Name 1: Ironwood Estates\n",
            "Name 2: Rosewood Estates\n",
            "\n",
            "Similarity: 0.875\n",
            "Name 1: KeyNote\n",
            "Name 2: KeyNote 1\n",
            "\n",
            "Similarity: 0.875\n",
            "Name 1: KeyNote\n",
            "Name 2: KeyNote 2\n",
            "\n",
            "Similarity: 0.875\n",
            "Name 1: Mcclaren Village\n",
            "Name 2: Mclaurin Village\n",
            "\n",
            "Similarity: 0.875\n",
            "Name 1: Parkside Terrace\n",
            "Name 2: Parkview Terrace\n",
            "\n",
            "Similarity: 0.875\n",
            "Name 1: Springbank Hill\n",
            "Name 2: Springbank Villas\n",
            "\n",
            "Similarity: 0.875\n",
            "Name 1: The Vogue\n",
            "Name 2: The Vue\n",
            "\n",
            "Similarity: 0.872\n",
            "Name 1: Fairways Pavilions\n",
            "Name 2: The Fairway Pavilions\n",
            "\n",
            "Similarity: 0.872\n",
            "Name 1: Valley Park Estates\n",
            "Name 2: Village Park Estates\n",
            "\n",
            "Similarity: 0.870\n",
            "Name 1: Oxford Parc\n",
            "Name 2: Oxford Place\n",
            "\n",
            "Similarity: 0.870\n",
            "Name 1: Urban Steel\n",
            "Name 2: Urban Street\n",
            "\n",
            "Similarity: 0.867\n",
            "Name 1: Airdrie Meadows\n",
            "Name 2: Prairie Meadows\n",
            "\n",
            "Similarity: 0.867\n",
            "Name 1: Arlington Place\n",
            "Name 2: Lexington Place\n",
            "\n",
            "Similarity: 0.867\n",
            "Name 1: Coachway Gardens\n",
            "Name 2: Coachway Green\n",
            "\n",
            "Similarity: 0.867\n",
            "Name 1: Fontainebleau\n",
            "Name 2: Fontainebleau Est\n",
            "\n",
            "Similarity: 0.867\n",
            "Name 1: Hampton Court\n",
            "Name 2: Oak Hampton Court\n",
            "\n",
            "Similarity: 0.867\n",
            "Name 1: Killarney Gate\n",
            "Name 2: Killarney Grande\n",
            "\n",
            "Similarity: 0.867\n",
            "Name 1: Legacy Estates - Patterson\n",
            "Name 2: Legacy Estates - Patterson Heights\n",
            "\n",
            "Similarity: 0.867\n",
            "Name 1: Tuscarora Manor\n",
            "Name 2: Tuscororo Manor\n",
            "\n",
            "Similarity: 0.865\n",
            "Name 1: Riley Park Estates\n",
            "Name 2: Valley Park Estates\n",
            "\n",
            "Similarity: 0.865\n",
            "Name 1: Summerview Terrace\n",
            "Name 2: Summit View Terrace\n",
            "\n",
            "Similarity: 0.864\n",
            "Name 1: Green Brier On The Park\n",
            "Name 2: Greenview on the Park\n",
            "\n",
            "Similarity: 0.857\n",
            "Name 1: Acadia Place\n",
            "Name 2: Ada Place\n",
            "\n",
            "Similarity: 0.857\n",
            "Name 1: Arlington Place\n",
            "Name 2: Carlton Place\n",
            "\n",
            "Similarity: 0.857\n",
            "Name 1: College Gardens\n",
            "Name 2: College Green\n",
            "\n",
            "Similarity: 0.857\n",
            "Name 1: Killarney Courts\n",
            "Name 2: Killarney Courtyard\n",
            "\n",
            "Similarity: 0.857\n",
            "Name 1: Montage in McKenzie Towne\n",
            "Name 2: Mosaic in McKenzie Towne\n",
            "\n",
            "Similarity: 0.857\n",
            "Name 1: Park Estates\n",
            "Name 2: Parkside Estates\n",
            "\n",
            "Similarity: 0.857\n",
            "Name 1: Pinetree Village\n",
            "Name 2: Spring Tree Village\n",
            "\n",
            "Similarity: 0.857\n",
            "Name 1: Pontefino\n",
            "Name 2: Pontefino II\n",
            "\n",
            "Similarity: 0.857\n",
            "Name 1: The Edison\n",
            "Name 2: The Madison\n",
            "\n",
            "Similarity: 0.857\n",
            "Name 1: The Pillars\n",
            "Name 2: The Villas\n",
            "\n",
            "Similarity: 0.850\n",
            "Name 1: Chaparral Adult Village\n",
            "Name 2: Chaparral Village\n",
            "\n",
            "Similarity: 0.850\n",
            "Name 1: Fontainebleau Est\n",
            "Name 2: Fountaine Bleau Estates\n",
            "\n",
            "Similarity: 0.850\n",
            "Name 1: Oakfield Park Villas\n",
            "Name 2: Oakville Park Villas\n",
            "\n",
            "Similarity: 0.848\n",
            "Name 1: Braeside Village\n",
            "Name 2: Brookside Village\n",
            "\n",
            "Similarity: 0.848\n",
            "Name 1: Braeside Village\n",
            "Name 2: Creekside Village\n",
            "\n",
            "Similarity: 0.848\n",
            "Name 1: Killarney Gardens\n",
            "Name 2: Killarney Grande\n",
            "\n",
            "Similarity: 0.848\n",
            "Name 1: Pineridge Village\n",
            "Name 2: Pinetree Village\n",
            "\n",
            "Similarity: 0.848\n",
            "Name 1: Rundle Village\n",
            "Name 2: Rundleson Village I\n",
            "\n",
            "Similarity: 0.848\n",
            "Name 1: Somerset Village\n",
            "Name 2: Somervale Village\n",
            "\n",
            "Similarity: 0.846\n",
            "Name 1: Anders Village\n",
            "Name 2: Deer Village\n",
            "\n",
            "Similarity: 0.846\n",
            "Name 1: Eagle Terrace\n",
            "Name 2: Maple Terrace\n",
            "\n",
            "Similarity: 0.846\n",
            "Name 1: Eagle Terrace\n",
            "Name 2: Regal Terrace\n",
            "\n",
            "Similarity: 0.846\n",
            "Name 1: Madison Court\n",
            "Name 2: Mission Court\n",
            "\n",
            "Similarity: 0.846\n",
            "Name 1: Renaissance\n",
            "Name 2: Renaissance The\n",
            "\n",
            "Similarity: 0.846\n",
            "Name 1: Renaissance\n",
            "Name 2: The Renaissance\n",
            "\n",
            "Similarity: 0.846\n",
            "Name 1: Renaissance The\n",
            "Name 2: renaissance\n",
            "\n",
            "Similarity: 0.846\n",
            "Name 1: The Renaissance\n",
            "Name 2: renaissance\n",
            "\n",
            "Similarity: 0.846\n",
            "Name 1: Village Green\n",
            "Name 2: Village Three\n",
            "\n",
            "Similarity: 0.844\n",
            "Name 1: Estates of Gleneagles\n",
            "Name 2: The Vistas of Gleneagles\n",
            "\n",
            "Similarity: 0.844\n",
            "Name 1: The Village at West Valley\n",
            "Name 2: Village At West Vll\n",
            "\n",
            "Similarity: 0.842\n",
            "Name 1: Ada Place\n",
            "Name 2: Dawn Place\n",
            "\n",
            "Similarity: 0.842\n",
            "Name 1: Douglasdale Village\n",
            "Name 2: Douglaswood Village\n",
            "\n",
            "Similarity: 0.842\n",
            "Name 1: Riley Park Estates\n",
            "Name 2: Village Park Estates\n",
            "\n",
            "Similarity: 0.842\n",
            "Name 1: River Run\n",
            "Name 2: Riverrun I\n",
            "\n",
            "Similarity: 0.840\n",
            "Name 1: Montage in Mackenzie Towne\n",
            "Name 2: Mosaic in McKenzie Towne\n",
            "\n",
            "Similarity: 0.839\n",
            "Name 1: Anders Village\n",
            "Name 2: Sandhurst Village\n",
            "\n",
            "Similarity: 0.839\n",
            "Name 1: Arlington Place\n",
            "Name 2: Wellington Place\n",
            "\n",
            "Similarity: 0.839\n",
            "Name 1: Cedar Brae Gardens\n",
            "Name 2: Cedar Gardens\n",
            "\n",
            "Similarity: 0.839\n",
            "Name 1: Clearview Point\n",
            "Name 2: Silverview Point\n",
            "\n",
            "Similarity: 0.839\n",
            "Name 1: Evergreen Estates\n",
            "Name 2: Evergreen West\n",
            "\n",
            "Similarity: 0.839\n",
            "Name 1: Glenbow Manor\n",
            "Name 2: Glenbow Manor Apts\n",
            "\n",
            "Similarity: 0.839\n",
            "Name 1: Killarney Gardens\n",
            "Name 2: Killarney Gate\n",
            "\n",
            "Similarity: 0.839\n",
            "Name 1: Lexington Place\n",
            "Name 2: Wellington Place\n",
            "\n",
            "Similarity: 0.839\n",
            "Name 1: Prominence Park\n",
            "Name 2: Prominence Place\n",
            "\n",
            "Similarity: 0.839\n",
            "Name 1: Regal Terrace\n",
            "Name 2: Regal Terrace West\n",
            "\n",
            "Similarity: 0.839\n",
            "Name 1: Rosewood Estates\n",
            "Name 2: Skywood Estates\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Aspen Village\n",
            "Name 2: Sun Village\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Aurora Place\n",
            "Name 2: Manora Place\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Avalon Court\n",
            "Name 2: Dalton Court\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Briar Park Estates\n",
            "Name 2: Riley Park Estates\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Briar Park Estates\n",
            "Name 2: Royal Park Estates\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Calla\n",
            "Name 2: Capella\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: City View Terrace\n",
            "Name 2: Summit View Terrace\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Copperfield Village\n",
            "Name 2: Deerfield Village\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Evergreen West\n",
            "Name 2: Evergreens\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Falconridge Manor\n",
            "Name 2: Falconridge Meadows\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Glenbrook Village II\n",
            "Name 2: Glenbrook Villas\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Homesteads\n",
            "Name 2: The Homesteads\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Kensington Estates\n",
            "Name 2: Wellington Estates\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Killarney Courts\n",
            "Name 2: Killarney Glen Court\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Newport Bay\n",
            "Name 2: Newport Beach\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Park Point\n",
            "Name 2: Parkdale Point\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Riley Park Estates\n",
            "Name 2: Royal Park Estates\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Seasons of Sunnyside\n",
            "Name 2: Sol of Sunnyside\n",
            "\n",
            "Similarity: 0.833\n",
            "Name 1: Village Park\n",
            "Name 2: Vintage Park\n",
            "\n",
            "Similarity: 0.829\n",
            "Name 1: Aberdeen on the Green\n",
            "Name 2: Aberdeen on the Park\n",
            "\n",
            "Similarity: 0.829\n",
            "Name 1: Mosaic of Aspen Hills\n",
            "Name 2: Mosaic of Elgin Hill\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Bracewood Meadows\n",
            "Name 2: Wood Meadows\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Brookside Place\n",
            "Name 2: Parkside Place\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Cedarbrook Park\n",
            "Name 2: Cedarwood Park\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Cosgrove Place\n",
            "Name 2: Woodgrove Place\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Deer Village\n",
            "Name 2: Deerfield Village\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Everstone Place\n",
            "Name 2: Riverton Place\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Highland Court\n",
            "Name 2: Kingsland Court\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Hilltop Estates\n",
            "Name 2: Willow Estates\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Kentish Mews\n",
            "Name 2: Kentish Town Mews\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Landmark Estates\n",
            "Name 2: Landmark West\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Mosaic Toscana\n",
            "Name 2: Mosaic Tuscanic\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Pine Meadows\n",
            "Name 2: Pineridge Meadows\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Prestwick Park\n",
            "Name 2: Prestwick Place\n",
            "\n",
            "Similarity: 0.828\n",
            "Name 1: Walgrove Place\n",
            "Name 2: Woodgrove Place\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: Berkshire Village\n",
            "Name 2: Brookside Village\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: Briar Oak Estates\n",
            "Name 2: Royal Oak Estates\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: Brookside Village\n",
            "Name 2: Creekside Village\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: Brookside Village\n",
            "Name 2: Penbrooke Village\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: Carriage Villas\n",
            "Name 2: Cougar Ridge Villas\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: Coach Hill Manor\n",
            "Name 2: Coach Hill Meadows\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: Coral Cove Estates\n",
            "Name 2: Taracove Estates\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: Devonshire Place\n",
            "Name 2: Devonshire Village\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: Horizon Village Sandstone\n",
            "Name 2: Horizon Village Strathcona\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: Prominence Heights\n",
            "Name 2: Prominence Hills\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: Rundle Village\n",
            "Name 2: Rundleson Village II\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: Sunvillage\n",
            "Name 2: Village\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: The Avenue\n",
            "Name 2: The Vue\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: The Dunes\n",
            "Name 2: The News\n",
            "\n",
            "Similarity: 0.824\n",
            "Name 1: The Rise\n",
            "Name 2: The River\n",
            "\n",
            "Similarity: 0.821\n",
            "Name 1: Cedar Brae Apartments\n",
            "Name 2: Cedar Brae Gardens\n",
            "\n",
            "Similarity: 0.821\n",
            "Name 1: Douglas Glen Gardens\n",
            "Name 2: Douglasbank Gardens\n",
            "\n",
            "Similarity: 0.821\n",
            "Name 1: Johnstone Grand Estates\n",
            "Name 2: Johnstone Grande\n",
            "\n",
            "Similarity: 0.821\n",
            "Name 1: Manors West Park\n",
            "Name 2: The Manors Of West Park\n",
            "\n",
            "Similarity: 0.821\n",
            "Name 1: Valley Park Estates\n",
            "Name 2: Valley Ridge Estates\n",
            "\n",
            "Similarity: 0.818\n",
            "Name 1: Altador Mews\n",
            "Name 2: Tudor Mews\n",
            "\n",
            "Similarity: 0.818\n",
            "Name 1: Beechwood Condominiums\n",
            "Name 2: Briarwood Condominiums\n",
            "\n",
            "Similarity: 0.818\n",
            "Name 1: Bella Citta\n",
            "Name 2: Bella Vista\n",
            "\n",
            "Similarity: 0.818\n",
            "Name 1: Doral Manor\n",
            "Name 2: Regal Manor\n",
            "\n",
            "Similarity: 0.818\n",
            "Name 1: Fountains\n",
            "Name 2: The Fountains\n",
            "\n",
            "Similarity: 0.818\n",
            "Name 1: Manhattan\n",
            "Name 2: The Manhattan\n",
            "\n",
            "Similarity: 0.818\n",
            "Name 1: Manor House\n",
            "Name 2: Noran House\n",
            "\n",
            "Similarity: 0.818\n",
            "Name 1: Milestone\n",
            "Name 2: The Milestone\n",
            "\n",
            "Similarity: 0.818\n",
            "Name 1: Pavilions\n",
            "Name 2: The Pavilions\n",
            "\n",
            "Similarity: 0.818\n",
            "Name 1: Ravenwood\n",
            "Name 2: The Ravenwood\n",
            "\n",
            "Similarity: 0.818\n",
            "Name 1: Sage Place\n",
            "Name 2: Seanel Place\n",
            "\n",
            "Similarity: 0.818\n",
            "Name 1: The Montage\n",
            "Name 2: The Montana\n",
            "\n",
            "Similarity: 0.818\n",
            "Name 1: Villa West\n",
            "Name 2: Villa d'Este\n",
            "\n",
            "Similarity: 0.816\n",
            "Name 1: Stonecroft At Arbour Lake\n",
            "Name 2: Stonecroft at Auburn Bay\n",
            "\n",
            "Similarity: 0.816\n",
            "Name 1: The Summit of Glen Eagles\n",
            "Name 2: The Vistas of Gleneagles\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Blakiston Place\n",
            "Name 2: Leaton Place\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Brae Glen Court\n",
            "Name 2: Regent Court\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Brompton Court\n",
            "Name 2: Hampton Court\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Carlton Place\n",
            "Name 2: Cranston Place\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Cedar Villas\n",
            "Name 2: Edgepark Villas\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Chinook Estates\n",
            "Name 2: Chinook West\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Coach Hill Manor\n",
            "Name 2: Coach Manor\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: College Gardens\n",
            "Name 2: College Gate\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Country Lane\n",
            "Name 2: Country Village\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Courtyards At Garrison Woods\n",
            "Name 2: The Courtyards At Garrison\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Dana Village\n",
            "Name 2: Madigan Village\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Dana Village\n",
            "Name 2: Sabrina Village\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Glenbow Manor\n",
            "Name 2: Glenwood Manor\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Glenwood Manor\n",
            "Name 2: Redwood Manor\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Leaton Place\n",
            "Name 2: Lexington Place\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Mosaic Mirage\n",
            "Name 2: Mosaic Montage\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Pine Meadows\n",
            "Name 2: Prairie Meadows\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Point Mckay\n",
            "Name 2: Point Mckay West\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Prospect Hills\n",
            "Name 2: Prospect Rise\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Sierra Gardens\n",
            "Name 2: Sierra Grande\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: The Ellington\n",
            "Name 2: The Kensington\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: The Shy-lui\n",
            "Name 2: ui - The Shy-lui\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Thornton Court\n",
            "Name 2: Tiboron Court\n",
            "\n",
            "Similarity: 0.815\n",
            "Name 1: Valhalla Ridge\n",
            "Name 2: Valhalla View\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Arlington House\n",
            "Name 2: Livingstone House\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Braeburn Village\n",
            "Name 2: Braeglen Village\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Braeglen Village\n",
            "Name 2: Braeside Village\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Brookside Place\n",
            "Name 2: Brookside Village\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Chinook Manor\n",
            "Name 2: Chinook Winds Manor\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Churchill Estates\n",
            "Name 2: Dunhill Estates\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Dunhill Estates\n",
            "Name 2: Knob Hill Estates\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Edgedale Gardens\n",
            "Name 2: Rosedale Gardens\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Everridge Square\n",
            "Name 2: Pineridge Square\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Everstone Place\n",
            "Name 2: Hearthstone Place\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Hallmark Estates\n",
            "Name 2: Landmark Estates\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Hardwood Estates\n",
            "Name 2: Ironwood Estates\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Hardwood Estates\n",
            "Name 2: Rosewood Estates\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Hawkstone Manor\n",
            "Name 2: Hearthstone Manor\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Highwood Gardens\n",
            "Name 2: Holmwood Gardens\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Inglewood Place\n",
            "Name 2: Inglewood Village\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Killarney Corner\n",
            "Name 2: Killarney Courts\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Killarney Corner\n",
            "Name 2: Killarney Grande\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Killarney Manor\n",
            "Name 2: Killarney Meadows\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Pineridge Meadows\n",
            "Name 2: Prairie Meadows\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Pinewood Estates\n",
            "Name 2: Rosewood Estates\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Signal Hill Est\n",
            "Name 2: Signal Hill Green\n",
            "\n",
            "Similarity: 0.812\n",
            "Name 1: Woodlea Estates\n",
            "Name 2: Woodridge Estates\n",
            "\n",
            "Similarity: 0.811\n",
            "Name 1: Coach Bluff Meadows\n",
            "Name 2: Coach Hill Meadows\n",
            "\n",
            "Similarity: 0.811\n",
            "Name 1: Confederation Park\n",
            "Name 2: Confederation Villa\n",
            "\n",
            "Similarity: 0.811\n",
            "Name 1: Gladstone Gardens\n",
            "Name 2: Gladstone Grdn. Est.\n",
            "\n",
            "Similarity: 0.811\n",
            "Name 1: Glenbrook Village II\n",
            "Name 2: Penbrooke Village\n",
            "\n",
            "Similarity: 0.811\n",
            "Name 1: Mount Royal Grande\n",
            "Name 2: Mount Royal Terrace\n",
            "\n",
            "Similarity: 0.811\n",
            "Name 1: Royal Park Estates\n",
            "Name 2: Valley Park Estates\n",
            "\n",
            "Similarity: 0.810\n",
            "Name 1: Penbrooke Meadows Village\n",
            "Name 2: Penbrooke Village\n",
            "\n",
            "Similarity: 0.810\n",
            "Name 1: Village Park Estates\n",
            "Name 2: YV_Valley Park Estates\n",
            "\n",
            "Similarity: 0.809\n",
            "Name 1: Sierra's Of Heritage\n",
            "Name 2: Sierras of Heritage Village\n",
            "\n",
            "Proposed standardizations:\n",
            "------------------------------\n",
            "\n",
            "Names to standardize to 'Dorchester Square':\n",
            "- Dorchestor Square\n",
            "\n",
            "Names to standardize to 'Courtyards of Garrison Woods':\n",
            "- Courtyards At Garrison Woods\n",
            "\n",
            "Names to standardize to 'The Mosaic In Mckenzie Town':\n",
            "- Mosaic in McKenzie Towne\n",
            "\n",
            "Names to keep distinct:\n",
            "------------------------------\n",
            "\n",
            "Copperfield Park vs Copperfield Park II\n",
            "\n",
            "KeyNote vs KeyNote 2\n",
            "\n",
            "Bridlecrest Pointe vs Bridleview Pointe\n",
            "\n",
            "Tarjan Place vs Tarjan Pointe\n",
            "Step 23 completed: Condo name analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ2vt_UHOQpb"
      },
      "source": [
        "### **Condo Name Distribution Analysis**\n",
        "\n",
        "This code block analyzes the distribution of condo names in the dataset to help determine appropriate thresholds for modeling. Here's what it does:\n",
        "\n",
        "1. **Data Analysis**:\n",
        "   - Calculates basic statistics for condo names:\n",
        "     - Total unique condo names.\n",
        "     - Average, median, maximum, and minimum samples per condo.\n",
        "   - Analyzes the distribution of condo names by sample size ranges (e.g., 1, 2-5, 6-10, etc.).\n",
        "   - Computes cumulative statistics to show how many condos and samples would be retained at different minimum sample thresholds.\n",
        "\n",
        "2. **Logging**:\n",
        "   - Logs basic statistics and distribution results.\n",
        "   - Tracks cumulative statistics for different thresholds.\n",
        "   - Uses the `NotebookLogger` class for consistent and detailed logging.\n",
        "\n",
        "3. **Output**:\n",
        "   - Returns a dictionary containing:\n",
        "     - Distribution of condos by sample size ranges.\n",
        "     - Cumulative statistics for different thresholds.\n",
        "     - Counts of samples for each condo name.\n",
        "\n",
        "### **Key Features**:\n",
        "- **Distribution Analysis**: Breaks down condo names by sample size ranges to understand data distribution.\n",
        "- **Cumulative Statistics**: Helps determine appropriate thresholds for modeling by showing how many condos and samples would be retained.\n",
        "- **Logging**: Provides detailed logs for tracking the analysis process and results.\n",
        "\n",
        "### **Future Enhancements**:\n",
        "- Automate threshold selection based on analysis results.\n",
        "- Visualize the distribution and cumulative statistics using plots.\n",
        "- Integrate with a model training pipeline to dynamically adjust thresholds."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CONDO SAMPLE DISTRIBUTION ANALYSIS\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This code block analyzes the distribution of condo samples in our dataset to:\n",
        "1. Calculate basic statistics about condo frequency\n",
        "2. Analyze distribution patterns across different sample size ranges\n",
        "3. Compute cumulative statistics for various minimum sample thresholds\n",
        "4. Help determine appropriate sample size requirements for model training\n",
        "\n",
        "This analysis is crucial for:\n",
        "- Understanding how many examples we have for each condo\n",
        "- Determining minimum sample thresholds for reliable prediction\n",
        "- Identifying condos with sufficient/insufficient data\n",
        "- Supporting decisions about data requirements for machine learning\n",
        "\"\"\"\n",
        "\n",
        "def analyze_condo_distribution(training_df):\n",
        "    \"\"\"\n",
        "    Analyzes the distribution of condo names in our training data to help us\n",
        "    determine appropriate thresholds for our model.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    training_df : pandas DataFrame\n",
        "        The training dataset containing condo information\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    analysis_results : dict\n",
        "        Dictionary containing the analysis results\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Analyzing Condo Name Distribution...\")\n",
        "    logger.log_info(\"=\" * 50)\n",
        "\n",
        "    # Get counts for each condo name\n",
        "    condo_counts = training_df['Condo_Name'].value_counts()\n",
        "\n",
        "    # Calculate basic statistics\n",
        "    logger.log_info(\"\\nBasic Statistics:\")\n",
        "    logger.log_info(\"-\" * 20)\n",
        "    logger.log_info(f\"Total unique condo names: {len(condo_counts):,}\")\n",
        "    logger.log_info(f\"Average samples per condo: {condo_counts.mean():.1f}\")\n",
        "    logger.log_info(f\"Median samples per condo: {condo_counts.median():.1f}\")\n",
        "    logger.log_info(f\"Maximum samples for a condo: {condo_counts.max():,}\")\n",
        "    logger.log_info(f\"Minimum samples for a condo: {condo_counts.min():,}\")\n",
        "\n",
        "    # Analyze distribution by sample size ranges\n",
        "    logger.log_info(\"\\nDistribution by Sample Size:\")\n",
        "    logger.log_info(\"-\" * 20)\n",
        "    ranges = [1, 5, 10, 20, 50, 100, float('inf')]\n",
        "    range_labels = ['1', '2-5', '6-10', '11-20', '21-50', '51-100', '100+']\n",
        "\n",
        "    distribution = []\n",
        "    cumulative_condos = []\n",
        "    cumulative_samples = []\n",
        "\n",
        "    for i in range(len(ranges)-1):\n",
        "        mask = (condo_counts >= ranges[i]) & (condo_counts < ranges[i+1])\n",
        "        count_in_range = mask.sum()\n",
        "        total_samples = condo_counts[mask].sum()\n",
        "\n",
        "        distribution.append({\n",
        "            'Range': range_labels[i],\n",
        "            'Num_Condos': count_in_range,\n",
        "            'Total_Samples': total_samples,\n",
        "            'Pct_Condos': count_in_range / len(condo_counts) * 100,\n",
        "            'Pct_Samples': total_samples / len(training_df) * 100\n",
        "        })\n",
        "\n",
        "        # Calculate cumulative statistics for condos with at least this many samples\n",
        "        min_samples = ranges[i]\n",
        "        condos_above = (condo_counts >= min_samples).sum()\n",
        "        samples_above = condo_counts[condo_counts >= min_samples].sum()\n",
        "\n",
        "        cumulative_condos.append({\n",
        "            'Min_Samples': min_samples,\n",
        "            'Num_Condos': condos_above,\n",
        "            'Pct_Condos': condos_above / len(condo_counts) * 100,\n",
        "            'Num_Samples': samples_above,\n",
        "            'Pct_Samples': samples_above / len(training_df) * 100\n",
        "        })\n",
        "\n",
        "    # Log distribution results\n",
        "    logger.log_info(\"\\nDistribution of condos by number of samples:\")\n",
        "    for d in distribution:\n",
        "        logger.log_info(f\"\\nRange {d['Range']} samples:\")\n",
        "        logger.log_info(f\"  Number of condos: {d['Num_Condos']:,} ({d['Pct_Condos']:.1f}%)\")\n",
        "        logger.log_info(f\"  Total samples: {d['Total_Samples']:,} ({d['Pct_Samples']:.1f}%)\")\n",
        "\n",
        "    # Log cumulative statistics\n",
        "    logger.log_info(\"\\nCumulative Statistics:\")\n",
        "    logger.log_info(\"-\" * 20)\n",
        "    logger.log_info(\"If we set minimum samples threshold to:\")\n",
        "    for c in cumulative_condos:\n",
        "        logger.log_info(f\"\\n{c['Min_Samples']} samples:\")\n",
        "        logger.log_info(f\"  Would keep {c['Num_Condos']:,} condos ({c['Pct_Condos']:.1f}%)\")\n",
        "        logger.log_info(f\"  Representing {c['Num_Samples']:,} samples ({c['Pct_Samples']:.1f}%)\")\n",
        "\n",
        "    logger.log_step_complete(\"Condo distribution analysis\")\n",
        "    return {\n",
        "        'distribution': distribution,\n",
        "        'cumulative_stats': cumulative_condos,\n",
        "        'condo_counts': condo_counts\n",
        "    }\n",
        "\n",
        "# Run the analysis on our training data\n",
        "distribution_analysis = analyze_condo_distribution(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbfdTcjIcPnu",
        "outputId": "784f2cdb-f8dc-45f4-e0f3-77b529d866b0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing Condo Name Distribution...\n",
            "==================================================\n",
            "\n",
            "Basic Statistics:\n",
            "--------------------\n",
            "Total unique condo names: 1,665\n",
            "Average samples per condo: 65.1\n",
            "Median samples per condo: 29.0\n",
            "Maximum samples for a condo: 31,072\n",
            "Minimum samples for a condo: 1\n",
            "\n",
            "Distribution by Sample Size:\n",
            "--------------------\n",
            "\n",
            "Distribution of condos by number of samples:\n",
            "\n",
            "Range 1 samples:\n",
            "  Number of condos: 205 (12.3%)\n",
            "  Total samples: 459 (0.1%)\n",
            "\n",
            "Range 2-5 samples:\n",
            "  Number of condos: 182 (10.9%)\n",
            "  Total samples: 1,253 (0.3%)\n",
            "\n",
            "Range 6-10 samples:\n",
            "  Number of condos: 248 (14.9%)\n",
            "  Total samples: 3,518 (0.9%)\n",
            "\n",
            "Range 11-20 samples:\n",
            "  Number of condos: 517 (31.1%)\n",
            "  Total samples: 17,119 (4.3%)\n",
            "\n",
            "Range 21-50 samples:\n",
            "  Number of condos: 311 (18.7%)\n",
            "  Total samples: 21,457 (5.5%)\n",
            "\n",
            "Range 51-100 samples:\n",
            "  Number of condos: 202 (12.1%)\n",
            "  Total samples: 64,551 (16.4%)\n",
            "\n",
            "Cumulative Statistics:\n",
            "--------------------\n",
            "If we set minimum samples threshold to:\n",
            "\n",
            "1 samples:\n",
            "  Would keep 1,665 condos (100.0%)\n",
            "  Representing 108,357 samples (27.5%)\n",
            "\n",
            "5 samples:\n",
            "  Would keep 1,460 condos (87.7%)\n",
            "  Representing 107,898 samples (27.4%)\n",
            "\n",
            "10 samples:\n",
            "  Would keep 1,278 condos (76.8%)\n",
            "  Representing 106,645 samples (27.1%)\n",
            "\n",
            "20 samples:\n",
            "  Would keep 1,030 condos (61.9%)\n",
            "  Representing 103,127 samples (26.2%)\n",
            "\n",
            "50 samples:\n",
            "  Would keep 513 condos (30.8%)\n",
            "  Representing 86,008 samples (21.8%)\n",
            "\n",
            "100 samples:\n",
            "  Would keep 202 condos (12.1%)\n",
            "  Representing 64,551 samples (16.4%)\n",
            "Step 24 completed: Condo distribution analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s7vDEuuPcab"
      },
      "source": [
        "### **Condo Name Standardization**\n",
        "\n",
        "This code block standardizes condo names in the dataset by correcting spelling variations and formatting inconsistencies while preserving distinct buildings. Here's what it does:\n",
        "\n",
        "1. **Standardization Mapping**:\n",
        "   - Creates a dictionary (`name_standardization`) to map variant names to their standardized versions.\n",
        "   - Includes corrections for:\n",
        "     - Spelling variations (e.g., \"Chateaux Strathcona\" → \"Chateau Strathcona\").\n",
        "     - Case and formatting inconsistencies (e.g., \"renaissance\" → \"Renaissance\").\n",
        "\n",
        "2. **Name Standardization**:\n",
        "   - Applies the standardization mapping to the `Condo_Name` column in the dataset.\n",
        "   - Logs the changes made, including the number of records affected for each standardized name.\n",
        "\n",
        "3. **Summary**:\n",
        "   - Logs the total number of unique condo names before and after standardization.\n",
        "   - Tracks the standardization process using the `NotebookLogger` class.\n",
        "\n",
        "### **Key Features**:\n",
        "- **Spelling Corrections**: Fixes common spelling variations in condo names.\n",
        "- **Case Standardization**: Ensures consistent capitalization and formatting.\n",
        "- **Logging**: Provides detailed logs of changes and summary statistics.\n",
        "\n",
        "### **Future Enhancements**:\n",
        "- Automate the detection of spelling variations using string similarity algorithms.\n",
        "- Expand the standardization mapping dynamically based on new data.\n",
        "- Integrate with a data validation pipeline to ensure ongoing consistency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CONDO NAME STANDARDIZATION AND SPELLING CORRECTION\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This code block handles the standardization of condo names by:\n",
        "1. Creating a mapping of known spelling variations and formatting inconsistencies\n",
        "2. Correcting common misspellings of condo names\n",
        "3. Standardizing capitalization and formatting\n",
        "4. Tracking and logging all name changes for verification\n",
        "\n",
        "Key components:\n",
        "- create_name_standardization_map(): Defines standardization rules\n",
        "- standardize_condo_names(): Applies the rules to the dataset\n",
        "- Preserves distinct buildings while fixing variations of the same building\n",
        "\"\"\"\n",
        "\n",
        "def create_name_standardization_map():\n",
        "    \"\"\"\n",
        "    Creates a mapping dictionary for standardizing condo names.\n",
        "    This helps us correct spelling variations while keeping truly different buildings separate.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary mapping variant names to their standardized versions\n",
        "    \"\"\"\n",
        "    # Spelling variations (same building, different spellings)\n",
        "    spelling_standardization = {\n",
        "        'Chateaux Strathcona': 'Chateau Strathcona',\n",
        "        'Cheasapeake': 'Chesapeake',\n",
        "        \"Davenport's Country Lane \": \"Davenport's Country Lane\",  # Remove trailing space\n",
        "        'Hamstead Estates': 'Hampstead Estates',\n",
        "        'Hanson Creek Manors': 'Hanson Creek Manor',\n",
        "        'Laural House At The Park': 'Laurel House At The Park',\n",
        "        'Oakhampton Court': 'Oak Hampton Court',\n",
        "        'Refrew House': 'Renfrew House',\n",
        "        'The Terraces': 'The Terrace',\n",
        "        'Woodmeadows': 'Wood Meadows',\n",
        "        'Calvannah Village': 'Calvanna Village',\n",
        "        'Chrystal Heights': 'Crystal Heights',\n",
        "        'Huntview Village': 'Huntsview Village',\n",
        "        'Mackenzie Village': 'McKenzie Village',\n",
        "        'The Pavillions': 'The Pavilions',\n",
        "        'Holly Spring': 'Holly Springs',\n",
        "        'Pattina Park': 'Patina Park',\n",
        "        'Sierra Grand': 'Sierra Grande'\n",
        "    }\n",
        "\n",
        "    # Case standardization (same building, different capitalization/formatting)\n",
        "    case_standardization = {\n",
        "        'renaissance': 'Renaissance',\n",
        "        'The Mosaic In Mckenzie Town': 'The Mosaic In McKenzie Town',\n",
        "        'Caledonia on the Waterfront': 'Caledonia On The Waterfront',\n",
        "        'The Pavilions of Eau Claire': 'The Pavilions Of Eau Claire',\n",
        "        'Montage in McKenzie Towne': 'Montage In McKenzie Towne'\n",
        "    }\n",
        "\n",
        "    # Combine all standardizations\n",
        "    name_standardization = {**spelling_standardization, **case_standardization}\n",
        "\n",
        "    return name_standardization\n",
        "\n",
        "def standardize_condo_names(df):\n",
        "    \"\"\"\n",
        "    Standardizes condo names in the dataset by correcting spelling variations\n",
        "    while preserving distinct buildings.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas DataFrame\n",
        "        The dataset containing condo information\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas DataFrame\n",
        "        Dataset with standardized condo names\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Standardizing condo names...\")\n",
        "    logger.log_info(\"=\" * 50)\n",
        "\n",
        "    # Make a copy to avoid modifying the original\n",
        "    standardized_df = df.copy()\n",
        "\n",
        "    # Get our standardization mapping\n",
        "    name_map = create_name_standardization_map()\n",
        "\n",
        "    # Count occurrences before standardization\n",
        "    before_counts = standardized_df['Condo_Name'].value_counts()\n",
        "\n",
        "    # Apply standardization\n",
        "    standardized_df['Condo_Name'] = standardized_df['Condo_Name'].replace(name_map)\n",
        "\n",
        "    # Count occurrences after standardization\n",
        "    after_counts = standardized_df['Condo_Name'].value_counts()\n",
        "\n",
        "    # Log summary of changes\n",
        "    logger.log_info(\"\\nChanges made:\")\n",
        "    logger.log_info(\"-\" * 30)\n",
        "\n",
        "    for old_name, new_name in name_map.items():\n",
        "        before = before_counts.get(old_name, 0)\n",
        "        if before > 0:  # Only show names that were actually in the dataset\n",
        "            logger.log_info(f\"\\nStandardized '{old_name}' to '{new_name}'\")\n",
        "            logger.log_info(f\"- Records affected: {before}\")\n",
        "\n",
        "    logger.log_info(\"\\nSummary:\")\n",
        "    logger.log_info(f\"Total unique names before: {len(before_counts)}\")\n",
        "    logger.log_info(f\"Total unique names after: {len(after_counts)}\")\n",
        "\n",
        "    logger.log_step_complete(\"Condo name standardization\")\n",
        "    return standardized_df\n",
        "\n",
        "# Apply the standardization\n",
        "standardized_data = standardize_condo_names(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PuKV87cc0_L",
        "outputId": "1d65dad9-b7b6-42d7-fa63-150d4390b91f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardizing condo names...\n",
            "==================================================\n",
            "\n",
            "Changes made:\n",
            "------------------------------\n",
            "\n",
            "Standardized 'Chateaux Strathcona' to 'Chateau Strathcona'\n",
            "- Records affected: 15\n",
            "\n",
            "Standardized 'Cheasapeake' to 'Chesapeake'\n",
            "- Records affected: 27\n",
            "\n",
            "Standardized 'Hamstead Estates' to 'Hampstead Estates'\n",
            "- Records affected: 30\n",
            "\n",
            "Standardized 'Hanson Creek Manors' to 'Hanson Creek Manor'\n",
            "- Records affected: 9\n",
            "\n",
            "Standardized 'Laural House At The Park' to 'Laurel House At The Park'\n",
            "- Records affected: 38\n",
            "\n",
            "Standardized 'Oakhampton Court' to 'Oak Hampton Court'\n",
            "- Records affected: 14\n",
            "\n",
            "Standardized 'Refrew House' to 'Renfrew House'\n",
            "- Records affected: 16\n",
            "\n",
            "Standardized 'The Terraces' to 'The Terrace'\n",
            "- Records affected: 50\n",
            "\n",
            "Standardized 'Woodmeadows' to 'Wood Meadows'\n",
            "- Records affected: 5\n",
            "\n",
            "Standardized 'Calvannah Village' to 'Calvanna Village'\n",
            "- Records affected: 47\n",
            "\n",
            "Standardized 'Chrystal Heights' to 'Crystal Heights'\n",
            "- Records affected: 16\n",
            "\n",
            "Standardized 'Huntview Village' to 'Huntsview Village'\n",
            "- Records affected: 12\n",
            "\n",
            "Standardized 'Mackenzie Village' to 'McKenzie Village'\n",
            "- Records affected: 38\n",
            "\n",
            "Standardized 'The Pavillions' to 'The Pavilions'\n",
            "- Records affected: 98\n",
            "\n",
            "Standardized 'Holly Spring' to 'Holly Springs'\n",
            "- Records affected: 10\n",
            "\n",
            "Standardized 'Pattina Park' to 'Patina Park'\n",
            "- Records affected: 22\n",
            "\n",
            "Standardized 'Sierra Grand' to 'Sierra Grande'\n",
            "- Records affected: 12\n",
            "\n",
            "Standardized 'renaissance' to 'Renaissance'\n",
            "- Records affected: 17\n",
            "\n",
            "Standardized 'The Mosaic In Mckenzie Town' to 'The Mosaic In McKenzie Town'\n",
            "- Records affected: 111\n",
            "\n",
            "Standardized 'Caledonia on the Waterfront' to 'Caledonia On The Waterfront'\n",
            "- Records affected: 55\n",
            "\n",
            "Standardized 'The Pavilions of Eau Claire' to 'The Pavilions Of Eau Claire'\n",
            "- Records affected: 25\n",
            "\n",
            "Standardized 'Montage in McKenzie Towne' to 'Montage In McKenzie Towne'\n",
            "- Records affected: 35\n",
            "\n",
            "Summary:\n",
            "Total unique names before: 1665\n",
            "Total unique names after: 1644\n",
            "Step 25 completed: Condo name standardization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sho7Wp6EPp_r"
      },
      "source": [
        "### **Building Pattern Analysis**\n",
        "\n",
        "This code block analyzes patterns in condo names to identify sequential buildings, spelling variations, and similar but distinct buildings. Here's what it does:\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - Cleans and prepares condo names by removing null values, converting to strings, and stripping whitespace.\n",
        "   - Logs basic statistics (total properties, valid names, unique condo names).\n",
        "\n",
        "2. **Pattern Analysis**:\n",
        "   - Identifies **sequential buildings** (e.g., \"Building 1\" and \"Building 2\") using regex patterns.\n",
        "   - Detects **spelling variations** (e.g., \"Chateau\" vs. \"Chateaux\") using string similarity.\n",
        "   - Flags **similar but different buildings** (e.g., \"Oak Place\" vs. \"Oak Pointe\") based on similarity ratios and common indicators.\n",
        "\n",
        "3. **Results**:\n",
        "   - Logs sequential buildings, spelling variations, and similar but different buildings.\n",
        "   - Provides detailed insights into naming patterns and potential standardization opportunities.\n",
        "\n",
        "### **Key Features**:\n",
        "- **Sequential Building Detection**: Identifies buildings that are part of the same complex (e.g., phases or numbered buildings).\n",
        "- **Spelling Variation Detection**: Finds and logs spelling inconsistencies in condo names.\n",
        "- **Similarity Analysis**: Highlights buildings with similar names but distinct identities.\n",
        "- **Logging**: Tracks the analysis process and results using the `NotebookLogger` class.\n",
        "\n",
        "### **Future Enhancements**:\n",
        "- Automate the standardization of sequential buildings and spelling variations.\n",
        "- Expand the analysis to include additional naming patterns (e.g., abbreviations, acronyms).\n",
        "- Integrate with a data validation pipeline to ensure ongoing consistency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# BUILDING PATTERN AND SEQUENCE ANALYSIS\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This code block analyzes patterns in building names to identify:\n",
        "1. Sequential buildings (e.g., Phase 1, Phase 2, or I, II, III)\n",
        "2. Different spelling variations of the same building\n",
        "3. Similar but distinct building names\n",
        "4. Base names for building complexes\n",
        "\n",
        "Key functions:\n",
        "- is_sequential(): Detects buildings that are part of a sequence\n",
        "- Uses SequenceMatcher to find similar names with 89%+ similarity\n",
        "- Categorizes similar names into:\n",
        "  * Sequential buildings (same complex, different phases)\n",
        "  * Spelling variations (same building, different spellings)\n",
        "  * Similar but different (distinct buildings with similar names)\n",
        "\"\"\"\n",
        "\n",
        "def analyze_building_patterns(training_df):\n",
        "    \"\"\"\n",
        "    Analyzes patterns in building names to identify sequential buildings\n",
        "    and spelling variations.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    training_df : pandas DataFrame\n",
        "        The training dataset containing condo information\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing analysis results\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Analyzing building patterns...\")\n",
        "    logger.log_info(\"=\" * 50)\n",
        "\n",
        "    # Step 1: Clean and prepare condo names\n",
        "    logger.log_info(\"\\nStep 1: Preparing condo names\")\n",
        "\n",
        "    # Check if we have the required column\n",
        "    if 'Condo_Name' not in training_df.columns:\n",
        "        raise ValueError(\"DataFrame must contain a 'Condo_Name' column\")\n",
        "\n",
        "    # Remove null values and convert to string\n",
        "    valid_mask = training_df['Condo_Name'].notna()\n",
        "    clean_df = training_df[valid_mask].copy()\n",
        "\n",
        "    # Convert all names to strings and remove leading/trailing whitespace\n",
        "    clean_df['Condo_Name'] = clean_df['Condo_Name'].astype(str).str.strip()\n",
        "\n",
        "    # Get unique, non-null condo names\n",
        "    condo_names = sorted(\n",
        "        [name for name in clean_df['Condo_Name'].unique() if name and name.strip()]\n",
        "    )\n",
        "\n",
        "    logger.log_info(f\"Total properties: {len(training_df):,}\")\n",
        "    logger.log_info(f\"Properties with valid names: {len(clean_df):,}\")\n",
        "    logger.log_info(f\"Unique condo names found: {len(condo_names):,}\")\n",
        "\n",
        "    # Initialize our categorization dictionaries\n",
        "    sequential_buildings = {}  # For buildings that are part of same complex\n",
        "    spelling_variations = {}   # For same building with different spellings\n",
        "    similar_but_different = [] # For different buildings with similar names\n",
        "\n",
        "    # Common words that indicate different buildings\n",
        "    different_indicators = ['i', 'ii', 'iii', '1', '2', '3', 'phase', 'pointe', 'place']\n",
        "\n",
        "    # Function to check if names indicate sequential buildings\n",
        "    def is_sequential(name1, name2):\n",
        "        \"\"\"Helper function to check if two names represent sequential buildings\"\"\"\n",
        "        import re\n",
        "\n",
        "        # Remove special characters and convert to lowercase\n",
        "        clean1 = re.sub(r'[^\\w\\s]', '', name1.lower())\n",
        "        clean2 = re.sub(r'[^\\w\\s]', '', name2.lower())\n",
        "\n",
        "        # Check for common sequential patterns\n",
        "        sequential_patterns = [\n",
        "            (r'(\\w+.*?)\\s+(?:phase\\s+)?([123]|i{1,3})\\s*$',\n",
        "             r'(\\w+.*?)\\s+(?:phase\\s+)?([123]|i{1,3})\\s*$'),\n",
        "            (r'(\\w+.*?)\\s+([123]|i{1,3})\\s*$',\n",
        "             r'(\\w+.*?)\\s+([123]|i{1,3})\\s*$')\n",
        "        ]\n",
        "\n",
        "        for pattern1, pattern2 in sequential_patterns:\n",
        "            match1 = re.match(pattern1, clean1)\n",
        "            match2 = re.match(pattern2, clean2)\n",
        "\n",
        "            if match1 and match2 and match1.group(1) == match2.group(1):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    logger.log_info(\"\\nStep 2: Finding similar names...\")\n",
        "    # Compare each name with every other name\n",
        "    for i, name1 in enumerate(condo_names):\n",
        "        for name2 in condo_names[i+1:]:\n",
        "            ratio = SequenceMatcher(None,\n",
        "                                  name1.lower(),\n",
        "                                  name2.lower()).ratio()\n",
        "\n",
        "            # If names are very similar\n",
        "            if ratio >= 0.89:\n",
        "                # Check if these are sequential buildings\n",
        "                if is_sequential(name1, name2):\n",
        "                    base_name = re.sub(r'\\s+(?:phase\\s+)?[123i]+\\s*$', '',\n",
        "                                     name1.lower())\n",
        "                    if base_name not in sequential_buildings:\n",
        "                        sequential_buildings[base_name] = set()\n",
        "                    sequential_buildings[base_name].add(name1)\n",
        "                    sequential_buildings[base_name].add(name2)\n",
        "\n",
        "                # Check for spelling variations\n",
        "                elif ratio > 0.95 and not any(\n",
        "                    ind in name1.lower() or ind in name2.lower()\n",
        "                    for ind in different_indicators\n",
        "                ):\n",
        "                    key = min(name1, name2)  # Use alphabetically first as key\n",
        "                    if key not in spelling_variations:\n",
        "                        spelling_variations[key] = set()\n",
        "                    spelling_variations[key].add(name1)\n",
        "                    spelling_variations[key].add(name2)\n",
        "\n",
        "                # Similar but different buildings\n",
        "                else:\n",
        "                    similar_but_different.append((name1, name2, ratio))\n",
        "\n",
        "    # Log results\n",
        "    logger.log_info(\"\\nSequential Buildings Found:\")\n",
        "    logger.log_info(\"-\" * 30)\n",
        "    for base_name, variations in sequential_buildings.items():\n",
        "        logger.log_info(f\"\\nBase name: {base_name}\")\n",
        "        for v in sorted(variations):\n",
        "            logger.log_info(f\"- {v}\")\n",
        "\n",
        "    logger.log_info(\"\\nSpelling Variations Found:\")\n",
        "    logger.log_info(\"-\" * 30)\n",
        "    for standard_name, variations in spelling_variations.items():\n",
        "        logger.log_info(f\"\\nStandard name: {standard_name}\")\n",
        "        for v in sorted(variations):\n",
        "            if v != standard_name:\n",
        "                logger.log_info(f\"- {v}\")\n",
        "\n",
        "    logger.log_info(\"\\nSimilar but Different Buildings:\")\n",
        "    logger.log_info(\"-\" * 30)\n",
        "    for name1, name2, ratio in sorted(similar_but_different,\n",
        "                                    key=lambda x: x[2],\n",
        "                                    reverse=True)[:20]:\n",
        "        logger.log_info(f\"\\nSimilarity: {ratio:.3f}\")\n",
        "        logger.log_info(f\"- {name1}\")\n",
        "        logger.log_info(f\"- {name2}\")\n",
        "\n",
        "    logger.log_step_complete(\"Building pattern analysis\")\n",
        "    return {\n",
        "        'sequential_buildings': sequential_buildings,\n",
        "        'spelling_variations': spelling_variations,\n",
        "        'similar_but_different': similar_but_different\n",
        "    }\n",
        "\n",
        "# Import required libraries (if not already imported)\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "\n",
        "# Run the analysis\n",
        "building_patterns = analyze_building_patterns(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uiq0KMz0dMST",
        "outputId": "179b9eca-89a5-4051-f260-c855db8bca1d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing building patterns...\n",
            "==================================================\n",
            "\n",
            "Step 1: Preparing condo names\n",
            "Total properties: 393,664\n",
            "Properties with valid names: 108,357\n",
            "Unique condo names found: 1,665\n",
            "\n",
            "Step 2: Finding similar names...\n",
            "\n",
            "Sequential Buildings Found:\n",
            "------------------------------\n",
            "\n",
            "Base name: alpine park\n",
            "- Alpine Park Phase 1\n",
            "- Alpine Park Phase 2\n",
            "\n",
            "Base name: bella rosa\n",
            "- Bella Rosa 1\n",
            "- Bella Rosa I\n",
            "- Bella Rosa II\n",
            "\n",
            "Base name: chinook\n",
            "- Chinook Ii\n",
            "- Chinook Iii\n",
            "\n",
            "Base name: copperfield park\n",
            "- Copperfield Park II\n",
            "- Copperfield Park III\n",
            "\n",
            "Base name: five west\n",
            "- Five West Phase I\n",
            "- Five West Phase II\n",
            "\n",
            "Base name: imperial place\n",
            "- Imperial Place I\n",
            "- Imperial Place II\n",
            "\n",
            "Base name: lakeview green\n",
            "- Lakeview Green 1\n",
            "- Lakeview Green 2\n",
            "- Lakeview Green 3\n",
            "\n",
            "Base name: point mckay\n",
            "- Point Mckay Phase I\n",
            "- Point Mckay Phase II\n",
            "- Point Mckay Phase III\n",
            "\n",
            "Base name: pontefino\n",
            "- Pontefino I\n",
            "- Pontefino II\n",
            "\n",
            "Base name: riverrun\n",
            "- Riverrun I\n",
            "- Riverrun II\n",
            "\n",
            "Base name: riverside towers\n",
            "- Riverside Towers I\n",
            "- Riverside Towers II\n",
            "\n",
            "Base name: rundleson village\n",
            "- Rundleson Village I\n",
            "- Rundleson Village II\n",
            "\n",
            "Base name: sierra's west\n",
            "- Sierra's West I\n",
            "- Sierra's West Ii\n",
            "\n",
            "Base name: whitehorn village\n",
            "- Whitehorn Village I\n",
            "- Whitehorn Village II\n",
            "\n",
            "Spelling Variations Found:\n",
            "------------------------------\n",
            "\n",
            "Standard name: Chateau Strathcona\n",
            "- Chateaux Strathcona\n",
            "\n",
            "Standard name: Cheasapeake\n",
            "- Chesapeake\n",
            "\n",
            "Standard name: Hampstead Estates\n",
            "- Hamstead Estates\n",
            "\n",
            "Standard name: Hanson Creek Manor\n",
            "- Hanson Creek Manors\n",
            "\n",
            "Standard name: Laural House At The Park\n",
            "- Laurel House At The Park\n",
            "\n",
            "Standard name: Oak Hampton Court\n",
            "- Oakhampton Court\n",
            "\n",
            "Standard name: Refrew House\n",
            "- Renfrew House\n",
            "\n",
            "Standard name: The Terrace\n",
            "- The Terraces\n",
            "\n",
            "Standard name: WEST\n",
            "- West\n",
            "\n",
            "Standard name: Wood Meadows\n",
            "- Woodmeadows\n",
            "\n",
            "Similar but Different Buildings:\n",
            "------------------------------\n",
            "\n",
            "Similarity: 1.000\n",
            "- Caledonia On The Waterfront\n",
            "- Caledonia on the Waterfront\n",
            "\n",
            "Similarity: 1.000\n",
            "- Renaissance\n",
            "- renaissance\n",
            "\n",
            "Similarity: 1.000\n",
            "- The Mosaic In McKenzie Town\n",
            "- The Mosaic In Mckenzie Town\n",
            "\n",
            "Similarity: 1.000\n",
            "- The Pavilions Of Eau Claire\n",
            "- The Pavilions of Eau Claire\n",
            "\n",
            "Similarity: 1.000\n",
            "- Z-Name Not Listed\n",
            "- Z-name Not Listed\n",
            "\n",
            "Similarity: 0.980\n",
            "- Montage in Mackenzie Towne\n",
            "- Montage in McKenzie Towne\n",
            "\n",
            "Similarity: 0.973\n",
            "- High Pointe Estates\n",
            "- Highpointe Estates\n",
            "\n",
            "Similarity: 0.973\n",
            "- The Fairway Vistas\n",
            "- The Fairways Vistas\n",
            "\n",
            "Similarity: 0.971\n",
            "- Devonshire Estate\n",
            "- Devonshire Estates\n",
            "\n",
            "Similarity: 0.971\n",
            "- Fairway Pavilions\n",
            "- Fairways Pavilions\n",
            "\n",
            "Similarity: 0.970\n",
            "- Calvanna Village\n",
            "- Calvannah Village\n",
            "\n",
            "Similarity: 0.970\n",
            "- Huntsview Village\n",
            "- Huntview Village\n",
            "\n",
            "Similarity: 0.970\n",
            "- Mackenzie Village\n",
            "- McKenzie Village\n",
            "\n",
            "Similarity: 0.968\n",
            "- Chrystal Heights\n",
            "- Crystal Heights\n",
            "\n",
            "Similarity: 0.968\n",
            "- Discovery Point\n",
            "- Discovery Pointe\n",
            "\n",
            "Similarity: 0.966\n",
            "- Midpark Garden\n",
            "- Midpark Gardens\n",
            "\n",
            "Similarity: 0.963\n",
            "- Horizon Village - Coach Hill\n",
            "- Horizon Village Coach Hill\n",
            "\n",
            "Similarity: 0.963\n",
            "- The Pavilions\n",
            "- The Pavillions\n",
            "\n",
            "Similarity: 0.960\n",
            "- Holly Spring\n",
            "- Holly Springs\n",
            "\n",
            "Similarity: 0.960\n",
            "- Sierra Grand\n",
            "- Sierra Grande\n",
            "Step 26 completed: Building pattern analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpSg0bVDP0mK"
      },
      "source": [
        "# Condo Complex Grouping System\n",
        "## Overview\n",
        "This code block creates and manages a grouping system for related condo buildings within larger complexes. It's crucial for proper real estate analysis as it helps identify units that are part of the same development, even if they have slightly different names (e.g., phases, numbered buildings).\n",
        "\n",
        "## Technical Details\n",
        "### Input\n",
        "* DataFrame containing standardized condo data\n",
        "* Required column: 'Condo_Name'\n",
        "* Current scale: Handles entire MLS dataset\n",
        "\n",
        "### Output\n",
        "* Original DataFrame with new 'Condo_Complex' column\n",
        "* Complex statistics in log file\n",
        "* Detailed building distribution analysis\n",
        "\n",
        "### Key Operations\n",
        "1. Input Validation\n",
        "   * Verifies DataFrame format\n",
        "   * Checks for required columns\n",
        "   * Handles null values safely\n",
        "\n",
        "2. Complex Mapping\n",
        "   * Uses dictionary to map buildings to complexes\n",
        "   * Handles case sensitivity\n",
        "   * Preserves original names when no mapping exists\n",
        "\n",
        "3. Analysis Generation\n",
        "   * Calculates complex-level statistics\n",
        "   * Analyzes building distributions\n",
        "   * Computes unit percentages per building\n",
        "\n",
        "## Code Review Findings\n",
        "1. Potential Issues:\n",
        "   * The complex_mapping dictionary is hardcoded - consider moving to external configuration\n",
        "   * No validation for duplicate mappings\n",
        "   * Missing handling for edge cases (e.g., similar names but different complexes)\n",
        "\n",
        "2. Suggested Improvements:\n",
        "   * Add validation for complex names before mapping\n",
        "   * Implement fuzzy matching for similar names\n",
        "   * Add option to export mapping dictionary for review\n",
        "   * Consider adding validation for geographical proximity\n",
        "\n",
        "## Pipeline Implications\n",
        "This standardization step is crucial because:\n",
        "1. It affects all downstream complex-level analyses\n",
        "2. Impacts Power BI visualizations grouping\n",
        "3. Influences statistical calculations by complex\n",
        "\n",
        "## Next Steps\n",
        "After running this code:\n",
        "1. Verify complex groupings are accurate\n",
        "2. Review any unmapped buildings\n",
        "3. Consider geographical validation\n",
        "4. Update Power BI relationships\n",
        "\n",
        "###### Console Output Guide\n",
        "Watch for these key messages:\n",
        "```markdown\n",
        "* Number of null condo names\n",
        "* Multi-building complex details\n",
        "* Summary statistics\n",
        "* Average units per building/complex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "YoSbWczAgwlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dae25ca-ef5f-48fb-fd6a-5ee85c1e273b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting complex grouping process...\n",
            "Starting Condo Complex grouping process...\n",
            "Found 285307 null condo names - will be excluded from grouping\n",
            "\n",
            "Analyzing complex groupings...\n",
            "\n",
            "Multi-building Complex Details:\n",
            "------------------------------\n",
            "\n",
            "Complex: Alpine Park\n",
            "Buildings:\n",
            "- Alpine Park Phase 1: 6 units (85.7% of complex)\n",
            "- Alpine Park Phase 2: 1 units (14.3% of complex)\n",
            "Total units in complex: 7\n",
            "\n",
            "Complex: Bella Rosa\n",
            "Buildings:\n",
            "- Bella Rosa 1: 10 units (76.9% of complex)\n",
            "- Bella Rosa I: 1 units (7.7% of complex)\n",
            "- Bella Rosa II: 2 units (15.4% of complex)\n",
            "Total units in complex: 13\n",
            "\n",
            "Complex: Chinook\n",
            "Buildings:\n",
            "- Chinook Ii: 1 units (50.0% of complex)\n",
            "- Chinook Iii: 1 units (50.0% of complex)\n",
            "Total units in complex: 2\n",
            "\n",
            "Complex: Copperfield Park\n",
            "Buildings:\n",
            "- Copperfield Park: 292 units (57.6% of complex)\n",
            "- Copperfield Park II: 117 units (23.1% of complex)\n",
            "- Copperfield Park III: 98 units (19.3% of complex)\n",
            "Total units in complex: 507\n",
            "\n",
            "Complex: Five West\n",
            "Buildings:\n",
            "- Five West Phase I: 115 units (48.7% of complex)\n",
            "- Five West Phase II: 121 units (51.3% of complex)\n",
            "Total units in complex: 236\n",
            "\n",
            "Complex: Imperial Place\n",
            "Buildings:\n",
            "- Imperial Place I: 7 units (46.7% of complex)\n",
            "- Imperial Place II: 8 units (53.3% of complex)\n",
            "Total units in complex: 15\n",
            "\n",
            "Complex: Lakeview Green\n",
            "Buildings:\n",
            "- Lakeview Green 1: 64 units (31.2% of complex)\n",
            "- Lakeview Green 2: 66 units (32.2% of complex)\n",
            "- Lakeview Green 3: 75 units (36.6% of complex)\n",
            "Total units in complex: 205\n",
            "\n",
            "Complex: Point Mckay\n",
            "Buildings:\n",
            "- Point Mckay: 112 units (33.0% of complex)\n",
            "- Point Mckay Phase I: 73 units (21.5% of complex)\n",
            "- Point Mckay Phase II: 42 units (12.4% of complex)\n",
            "- Point Mckay Phase III: 49 units (14.5% of complex)\n",
            "- Point Mckay West: 63 units (18.6% of complex)\n",
            "Total units in complex: 339\n",
            "\n",
            "Summary Statistics:\n",
            "------------------------------\n",
            "Total complexes: 1,630\n",
            "Total buildings: 1,644\n",
            "Complexes with multiple buildings: 8\n",
            "Average units per building: 239.5\n",
            "Average units per complex: 241.5\n",
            "Step 27 completed: Complex grouping creation\n",
            "\n",
            "Complex grouping completed successfully! 🎉\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# CONDO COMPLEX GROUPING AND RELATIONSHIP MAPPING\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This code block creates and manages condo complex relationships by:\n",
        "1. Creating a mapping between individual buildings and their parent complexes\n",
        "2. Adding a new 'Condo_Complex' column to track these relationships\n",
        "3. Analyzing the distribution of buildings within complexes\n",
        "4. Providing detailed statistics about complex sizes and compositions\n",
        "\n",
        "Key Features:\n",
        "- Maintains a dictionary of known building-to-complex relationships\n",
        "- Uses case-insensitive matching to handle formatting variations\n",
        "- Tracks multi-building complexes and their unit distributions\n",
        "- Calculates complex-level statistics\n",
        "\"\"\"\n",
        "\n",
        "def create_complex_column(df):\n",
        "    \"\"\"\n",
        "    Creates a new column 'Condo_Complex' that groups related buildings together.\n",
        "    This function standardizes complex names and provides detailed statistics\n",
        "    about building distributions within complexes.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas DataFrame\n",
        "        The dataset containing standardized condo names\n",
        "        Must contain 'Condo_Name' column\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas DataFrame\n",
        "        Dataset with new 'Condo_Complex' column\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting Condo Complex grouping process...\")\n",
        "\n",
        "        # Validate input\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
        "\n",
        "        if 'Condo_Name' not in df.columns:\n",
        "            raise ValueError(\"DataFrame must contain 'Condo_Name' column\")\n",
        "\n",
        "        # Make a copy to avoid modifying the original\n",
        "        complex_df = df.copy()\n",
        "\n",
        "        # Handle any null values in Condo_Name\n",
        "        null_names = complex_df['Condo_Name'].isna().sum()\n",
        "        if null_names > 0:\n",
        "            logger.log_info(f\"Found {null_names} null condo names - will be excluded from grouping\")\n",
        "\n",
        "        # Dictionary to map buildings to their complexes\n",
        "        complex_mapping = {\n",
        "            # Alpine Park complex\n",
        "            'Alpine Park Phase 1': 'Alpine Park',\n",
        "            'Alpine Park Phase 2': 'Alpine Park',\n",
        "\n",
        "            # Bella Rosa complex\n",
        "            'Bella Rosa': 'Bella Rosa',\n",
        "            'Bella Rosa 1': 'Bella Rosa',\n",
        "            'Bella Rosa I': 'Bella Rosa',\n",
        "            'Bella Rosa II': 'Bella Rosa',\n",
        "\n",
        "            # Chinook complex\n",
        "            'Chinook': 'Chinook',\n",
        "            'Chinook Ii': 'Chinook',\n",
        "            'Chinook Iii': 'Chinook',\n",
        "\n",
        "            # Copperfield Park complex\n",
        "            'Copperfield Park': 'Copperfield Park',\n",
        "            'Copperfield Park II': 'Copperfield Park',\n",
        "            'Copperfield Park III': 'Copperfield Park',\n",
        "\n",
        "            # Five West complex\n",
        "            'Five West': 'Five West',\n",
        "            'Five West Phase I': 'Five West',\n",
        "            'Five West Phase II': 'Five West',\n",
        "\n",
        "            # Imperial Place complex\n",
        "            'Imperial Place': 'Imperial Place',\n",
        "            'Imperial Place I': 'Imperial Place',\n",
        "            'Imperial Place II': 'Imperial Place',\n",
        "\n",
        "            # Lakeview Green complex\n",
        "            'Lakeview Green': 'Lakeview Green',\n",
        "            'Lakeview Green 1': 'Lakeview Green',\n",
        "            'Lakeview Green 2': 'Lakeview Green',\n",
        "            'Lakeview Green 3': 'Lakeview Green',\n",
        "\n",
        "            # Point Mckay complex\n",
        "            'Point Mckay': 'Point Mckay',\n",
        "            'Point Mckay West': 'Point Mckay',\n",
        "            'Point Mckay Phase I': 'Point Mckay',\n",
        "            'Point Mckay Phase II': 'Point Mckay',\n",
        "            'Point Mckay Phase III': 'Point Mckay',\n",
        "\n",
        "            # Add other complexes...\n",
        "        }\n",
        "\n",
        "        # Create case-insensitive mapping\n",
        "        case_insensitive_mapping = {\n",
        "            k.lower(): v for k, v in complex_mapping.items()\n",
        "        }\n",
        "\n",
        "        # Create the new column with case-insensitive matching\n",
        "        complex_df['Condo_Complex'] = complex_df['Condo_Name'].apply(\n",
        "            lambda x: case_insensitive_mapping.get(\n",
        "                str(x).lower(),\n",
        "                x if pd.notna(x) else None\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Analyze the groupings\n",
        "        logger.log_info(\"\\nAnalyzing complex groupings...\")\n",
        "\n",
        "        # Get complexes with multiple buildings\n",
        "        complex_buildings = {}\n",
        "        valid_complexes = complex_df[complex_df['Condo_Complex'].notna()]\n",
        "\n",
        "        for complex_name in sorted(valid_complexes['Condo_Complex'].unique()):\n",
        "            complex_mask = valid_complexes['Condo_Complex'] == complex_name\n",
        "            buildings = valid_complexes[complex_mask]['Condo_Name'].unique()\n",
        "\n",
        "            if len(buildings) > 1:  # If complex has multiple buildings\n",
        "                complex_buildings[complex_name] = buildings\n",
        "\n",
        "        # Log details for complexes with multiple buildings\n",
        "        if complex_buildings:\n",
        "            logger.log_info(\"\\nMulti-building Complex Details:\")\n",
        "            logger.log_info(\"-\" * 30)\n",
        "\n",
        "            for complex_name, buildings in sorted(complex_buildings.items()):\n",
        "                logger.log_info(f\"\\nComplex: {complex_name}\")\n",
        "                logger.log_info(\"Buildings:\")\n",
        "\n",
        "                # Get total units for the complex\n",
        "                complex_mask = complex_df['Condo_Complex'] == complex_name\n",
        "                total_complex_units = complex_mask.sum()\n",
        "\n",
        "                # Log details for each building\n",
        "                for building in sorted(buildings):\n",
        "                    building_mask = complex_df['Condo_Name'] == building\n",
        "                    count = building_mask.sum()\n",
        "                    percentage = (count / total_complex_units) * 100\n",
        "                    logger.log_info(\n",
        "                        f\"- {building}: {count:,} units \"\n",
        "                        f\"({percentage:.1f}% of complex)\"\n",
        "                    )\n",
        "\n",
        "                logger.log_info(f\"Total units in complex: {total_complex_units:,}\")\n",
        "\n",
        "        # Calculate and log statistics\n",
        "        total_complexes = complex_df['Condo_Complex'].nunique()\n",
        "        total_buildings = complex_df['Condo_Name'].nunique()\n",
        "        multi_building_complexes = len(complex_buildings)\n",
        "\n",
        "        logger.log_info(\"\\nSummary Statistics:\")\n",
        "        logger.log_info(\"-\" * 30)\n",
        "        logger.log_info(f\"Total complexes: {total_complexes:,}\")\n",
        "        logger.log_info(f\"Total buildings: {total_buildings:,}\")\n",
        "        logger.log_info(\n",
        "            f\"Complexes with multiple buildings: {multi_building_complexes:,}\"\n",
        "        )\n",
        "\n",
        "        # Calculate averages\n",
        "        total_units = len(complex_df)\n",
        "        avg_units_per_building = total_units / total_buildings\n",
        "        avg_units_per_complex = total_units / total_complexes\n",
        "\n",
        "        logger.log_info(\n",
        "            f\"Average units per building: {avg_units_per_building:.1f}\"\n",
        "        )\n",
        "        logger.log_info(\n",
        "            f\"Average units per complex: {avg_units_per_complex:.1f}\"\n",
        "        )\n",
        "\n",
        "        logger.log_step_complete(\"Complex grouping creation\")\n",
        "        return complex_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in create_complex_column: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the complex grouping\n",
        "try:\n",
        "    logger.log_info(\"Starting complex grouping process...\")\n",
        "\n",
        "    # Create complex groupings\n",
        "    complex_data = create_complex_column(standardized_data)\n",
        "\n",
        "    logger.log_info(\"\\nComplex grouping completed successfully! 🎉\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"Error during execution: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDnc4ITOP_BT"
      },
      "source": [
        "# Training Data Filtering and Validation System\n",
        "## Overview\n",
        "This code block prepares data for machine learning by ensuring we have enough samples of each condo for reliable analysis. It's like ensuring we have enough examples of each condo type to make accurate predictions, similar to how a real estate agent needs multiple comparable properties for accurate valuations.\n",
        "\n",
        "## Technical Details\n",
        "### Input\n",
        "* DataFrame containing condo information\n",
        "* Required column: 'Condo_Name'\n",
        "* Configuration parameter: min_samples (default=10)\n",
        "* Current scale: Works with full MLS dataset\n",
        "\n",
        "### Output\n",
        "* filtered_df: Clean DataFrame containing only condos with sufficient samples\n",
        "* excluded_condos: Series containing information about excluded properties\n",
        "* Detailed statistics in log file\n",
        "\n",
        "### Key Operations\n",
        "1. Input Validation\n",
        "   * Verifies DataFrame format and required columns\n",
        "   * Validates min_samples parameter\n",
        "   * Handles edge cases safely\n",
        "\n",
        "2. Data Filtering\n",
        "   * Counts samples per condo\n",
        "   * Applies minimum sample threshold\n",
        "   * Creates clean filtered dataset\n",
        "\n",
        "3. Statistical Analysis\n",
        "   * Calculates retention rates\n",
        "   * Analyzes excluded data\n",
        "   * Provides detailed distribution reports\n",
        "\n",
        "## Code Review Findings\n",
        "1. Potential Improvements:\n",
        "   * Add validation for data quality (not just quantity)\n",
        "   * Consider adding a warning threshold for borderline cases\n",
        "   * Add option to export excluded condos list\n",
        "   * Consider adding data balance checks\n",
        "\n",
        "2. Safety Features Present:\n",
        "   * Input validation\n",
        "   * Error handling\n",
        "   * Data copying\n",
        "   * Detailed logging\n",
        "\n",
        "## Pipeline Implications\n",
        "This filtering step is crucial because:\n",
        "1. It affects machine learning model quality\n",
        "2. Impacts prediction reliability\n",
        "3. Influences which condos we can analyze\n",
        "\n",
        "## Next Steps\n",
        "After running this filter:\n",
        "1. Review excluded condos for patterns\n",
        "2. Consider adjusting min_samples threshold\n",
        "3. Analyze data balance\n",
        "4. Prepare filtered data for model training\n",
        "\n",
        "###### Console Output Guide\n",
        "Key information to watch for:\n",
        "```markdown\n",
        "* Initial data statistics\n",
        "* Filtering results\n",
        "* Retention percentages\n",
        "* Distribution of excluded condos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "FOpzrjMLjV3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce8beee7-b6f1-4f7c-935f-8d51e6f5c05d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training data preparation...\n",
            "Starting training data filtering process...\n",
            "Minimum samples threshold: 10\n",
            "\n",
            "Initial data statistics:\n",
            "Total properties: 393,664\n",
            "Unique condos: 1,665\n",
            "\n",
            "Filtering Results:\n",
            "------------------------------\n",
            "Condo counts:\n",
            "- Original condos: 1,665\n",
            "- Condos after filtering: 1,278\n",
            "- Condos excluded: 387\n",
            "\n",
            "Sample counts:\n",
            "- Original samples: 393,664\n",
            "- Samples after filtering: 106,645\n",
            "- Samples excluded: 287,019\n",
            "\n",
            "Percentages:\n",
            "- Condos retained: 76.8%\n",
            "- Samples retained: 27.1%\n",
            "\n",
            "Distribution of excluded condos:\n",
            "------------------------------\n",
            "Condos with 1 sample: 73\n",
            "Condos with 2 samples: 49\n",
            "Condos with 3 samples: 44\n",
            "Condos with 4 samples: 39\n",
            "Condos with 5 samples: 33\n",
            "Condos with 6 samples: 50\n",
            "Condos with 7 samples: 37\n",
            "Condos with 8 samples: 29\n",
            "Condos with 9 samples: 33\n",
            "Step 28 completed: Training data filtering\n",
            "\n",
            "Training data preparation completed successfully! 🎉\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# TRAINING DATA FILTERING AND SAMPLE VALIDATION\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This code block prepares training data for machine learning by:\n",
        "1. Filtering out condos with insufficient samples (less than minimum threshold)\n",
        "2. Validating data quality and sample sizes\n",
        "3. Tracking excluded condos for analysis\n",
        "4. Providing detailed statistics about data retention\n",
        "\n",
        "Key features:\n",
        "- Uses minimum sample threshold (default: 10 samples)\n",
        "- Maintains data integrity with extensive validation\n",
        "- Provides detailed logging of filtering results\n",
        "- Analyzes distribution of excluded samples\n",
        "\"\"\"\n",
        "\n",
        "def prepare_filtered_training_data(training_df, min_samples=10):\n",
        "    \"\"\"\n",
        "    Prepares training data by filtering out condos with insufficient samples for reliable analysis.\n",
        "    This helps ensure we have enough data points for each condo in our training set.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    training_df : pandas DataFrame\n",
        "        The original training dataset containing condo information\n",
        "        Must include 'Condo_Name' column\n",
        "    min_samples : int, default=10\n",
        "        Minimum number of samples required for a condo to be included\n",
        "        Condos with fewer samples will be excluded\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple:\n",
        "        filtered_df : pandas DataFrame\n",
        "            The filtered training dataset containing only condos with sufficient samples\n",
        "        excluded_condos : pandas Series\n",
        "            Information about excluded condos and their sample counts\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting training data filtering process...\")\n",
        "        logger.log_info(f\"Minimum samples threshold: {min_samples}\")\n",
        "\n",
        "        # Step 1: Validate input data\n",
        "        # ---------------------------\n",
        "        if not isinstance(training_df, pd.DataFrame):\n",
        "            raise ValueError(\"training_df must be a pandas DataFrame\")\n",
        "\n",
        "        if 'Condo_Name' not in training_df.columns:\n",
        "            raise ValueError(\"DataFrame must contain 'Condo_Name' column\")\n",
        "\n",
        "        if not isinstance(min_samples, int) or min_samples < 1:\n",
        "            raise ValueError(\"min_samples must be a positive integer\")\n",
        "\n",
        "        # Step 2: Calculate sample counts per condo\n",
        "        # ----------------------------------------\n",
        "        # Count how many times each condo appears in the dataset\n",
        "        condo_counts = training_df['Condo_Name'].value_counts()\n",
        "\n",
        "        logger.log_info(\"\\nInitial data statistics:\")\n",
        "        logger.log_info(f\"Total properties: {len(training_df):,}\")\n",
        "        logger.log_info(f\"Unique condos: {len(condo_counts):,}\")\n",
        "\n",
        "        # Step 3: Filter the data\n",
        "        # ----------------------\n",
        "        # Get names of condos that meet our threshold\n",
        "        valid_condos = condo_counts[condo_counts >= min_samples].index\n",
        "\n",
        "        # Create mask for valid condos and apply it\n",
        "        valid_mask = training_df['Condo_Name'].isin(valid_condos)\n",
        "        filtered_df = training_df[valid_mask].copy()\n",
        "\n",
        "        # Get information about excluded condos\n",
        "        excluded_condos = condo_counts[condo_counts < min_samples]\n",
        "\n",
        "        # Step 4: Calculate and log statistics\n",
        "        # ----------------------------------\n",
        "        # Basic counts\n",
        "        original_condos = len(condo_counts)\n",
        "        remaining_condos = len(valid_condos)\n",
        "        excluded_count = len(excluded_condos)\n",
        "\n",
        "        # Sample counts\n",
        "        original_samples = len(training_df)\n",
        "        remaining_samples = len(filtered_df)\n",
        "        excluded_samples = original_samples - remaining_samples\n",
        "\n",
        "        # Log the results\n",
        "        logger.log_info(\"\\nFiltering Results:\")\n",
        "        logger.log_info(\"-\" * 30)\n",
        "        logger.log_info(\"Condo counts:\")\n",
        "        logger.log_info(f\"- Original condos: {original_condos:,}\")\n",
        "        logger.log_info(f\"- Condos after filtering: {remaining_condos:,}\")\n",
        "        logger.log_info(f\"- Condos excluded: {excluded_count:,}\")\n",
        "\n",
        "        logger.log_info(\"\\nSample counts:\")\n",
        "        logger.log_info(f\"- Original samples: {original_samples:,}\")\n",
        "        logger.log_info(f\"- Samples after filtering: {remaining_samples:,}\")\n",
        "        logger.log_info(f\"- Samples excluded: {excluded_samples:,}\")\n",
        "\n",
        "        # Calculate percentages for better context\n",
        "        logger.log_info(\"\\nPercentages:\")\n",
        "        logger.log_info(\n",
        "            f\"- Condos retained: {(remaining_condos/original_condos)*100:.1f}%\"\n",
        "        )\n",
        "        logger.log_info(\n",
        "            f\"- Samples retained: {(remaining_samples/original_samples)*100:.1f}%\"\n",
        "        )\n",
        "\n",
        "        # Step 5: Analyze excluded condos\n",
        "        # ------------------------------\n",
        "        logger.log_info(\"\\nDistribution of excluded condos:\")\n",
        "        logger.log_info(\"-\" * 30)\n",
        "\n",
        "        # Group excluded condos by their sample count\n",
        "        excluded_dist = excluded_condos.value_counts().sort_index()\n",
        "\n",
        "        for samples, count in excluded_dist.items():\n",
        "            logger.log_info(\n",
        "                f\"Condos with {samples} sample{'s' if samples > 1 else ''}: \"\n",
        "                f\"{count:,}\"\n",
        "            )\n",
        "\n",
        "        logger.log_step_complete(\"Training data filtering\")\n",
        "\n",
        "        return filtered_df, excluded_condos\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in prepare_filtered_training_data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the filtering process\n",
        "try:\n",
        "    logger.log_info(\"Starting training data preparation...\")\n",
        "\n",
        "    # Set minimum samples threshold\n",
        "    MIN_SAMPLES = 10  # We can adjust this value based on our needs\n",
        "\n",
        "    # Run the filtering\n",
        "    filtered_training_data, excluded = prepare_filtered_training_data(\n",
        "        training_data,\n",
        "        min_samples=MIN_SAMPLES\n",
        "    )\n",
        "\n",
        "    logger.log_info(\"\\nTraining data preparation completed successfully! 🎉\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"Error during execution: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03GLkxR7QFIk"
      },
      "source": [
        "# Machine Learning Feature Preparation System\n",
        "## Overview\n",
        "This code block transforms raw condo data into a format suitable for machine learning by creating a feature matrix and preparing target variables. Think of it as converting our real estate data into a format that a machine learning model can understand and learn from.\n",
        "\n",
        "## Technical Details\n",
        "### Input\n",
        "* DataFrame containing standardized condo data\n",
        "* Required columns:\n",
        "  - 'Condo_Name' (target variable)\n",
        "  - 'Year_Built' (numerical feature)\n",
        "  - 'Latitude', 'Longitude' (location features)\n",
        "  - 'Postal_Code', 'Subdivision_Name', 'Condo_Complex' (categorical features)\n",
        "* Configuration: min_samples threshold (default=10)\n",
        "\n",
        "### Output\n",
        "* X: Feature matrix (dummy variables for model training)\n",
        "* y: Target variable series (Condo_Name predictions)\n",
        "* feature_info: Dictionary containing feature statistics and memory usage\n",
        "\n",
        "### Key Operations\n",
        "1. Data Validation and Filtering\n",
        "   * Verifies all required columns exist\n",
        "   * Filters out condos with insufficient samples\n",
        "   * Creates working copy to preserve original data\n",
        "\n",
        "2. Numerical Feature Processing\n",
        "   * Converts Year_Built to numeric format\n",
        "   * Handles missing values using median imputation\n",
        "   * Validates data ranges\n",
        "\n",
        "3. Location Feature Engineering\n",
        "   * Creates location bins for nearby properties\n",
        "   * Generates unique location identifiers\n",
        "   * Handles missing coordinate data\n",
        "\n",
        "4. Feature Matrix Creation\n",
        "   * Generates dummy variables for categorical features\n",
        "   * Uses sparse matrices to optimize memory\n",
        "   * Creates comprehensive feature information dictionary\n",
        "\n",
        "## Code Review Findings\n",
        "1. Potential Improvements:\n",
        "   * Consider adding feature scaling/normalization\n",
        "   * Implement feature selection for high-cardinality categories\n",
        "   * Add correlation analysis between features\n",
        "   * Consider adding cross-validation setup\n",
        "\n",
        "2. Memory Optimization Opportunities:\n",
        "   * Monitor memory usage for large datasets\n",
        "   * Consider chunking for very large datasets\n",
        "   * Implement feature selection to reduce dimensionality\n",
        "\n",
        "## Pipeline Implications\n",
        "This preparation step is crucial because:\n",
        "1. Feature quality directly impacts model performance\n",
        "2. Memory usage affects scalability\n",
        "3. Proper encoding ensures model understands the data\n",
        "\n",
        "## Next Steps\n",
        "After this preparation:\n",
        "1. Review feature importance\n",
        "2. Consider feature selection\n",
        "3. Evaluate memory usage\n",
        "4. Begin model training\n",
        "\n",
        "###### Console Output Guide\n",
        "Watch for these key messages:\n",
        "```markdown\n",
        "* Missing required columns\n",
        "* Sample size filtering results\n",
        "* Feature creation statistics\n",
        "* Memory usage warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5uLn96EUlj01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39998911-d02d-4ffb-b1ac-424510e46c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training data preparation process...\n",
            "Starting training data preparation...\n",
            "\n",
            "Step 1: Analyzing sample sizes\n",
            "Total unique condos: 1,644\n",
            "Condos with 10+ samples: 1,260\n",
            "Condos excluded: 384\n",
            "Working dataset size: 106,668 records\n",
            "\n",
            "Step 2: Processing numerical features\n",
            "Processing Year_Built...\n",
            "Year range: 1904 to 2026\n",
            "\n",
            "Step 3: Processing location features\n",
            "Created 10,590 unique location bins\n",
            "\n",
            "Step 4: Creating feature matrix\n",
            "\n",
            "Processing Year_Built...\n",
            "Created 109 features from Year_Built\n",
            "\n",
            "Processing Postal_Code...\n",
            "Created 11,078 features from Postal_Code\n",
            "\n",
            "Processing Subdivision_Name...\n",
            "Warning: 13 missing values in Subdivision_Name\n",
            "Created 364 features from Subdivision_Name\n",
            "\n",
            "Processing location_id...\n",
            "Created 10,590 features from location_id\n",
            "\n",
            "Processing Condo_Complex...\n",
            "Created 1,251 features from Condo_Complex\n",
            "\n",
            "Final Dataset Statistics:\n",
            "------------------------------\n",
            "Feature matrix shape: (106668, 23392)\n",
            "Number of target classes: 1,260\n",
            "Memory usage: 3.36 MB\n",
            "Step 29 completed: Training data preparation\n",
            "\n",
            "Feature Information:\n",
            "--------------------------------------------------\n",
            "\n",
            "Year_Built:\n",
            "- Unique values: 109\n",
            "- Features created: 109\n",
            "\n",
            "Postal_Code:\n",
            "- Unique values: 11,078\n",
            "- Features created: 11,078\n",
            "\n",
            "Subdivision_Name:\n",
            "- Unique values: 364\n",
            "- Features created: 364\n",
            "- Missing values: 13\n",
            "\n",
            "location_id:\n",
            "- Unique values: 10,590\n",
            "- Features created: 10,590\n",
            "\n",
            "Condo_Complex:\n",
            "- Unique values: 1,251\n",
            "- Features created: 1,251\n",
            "\n",
            "Train/Validation Split:\n",
            "Training samples: 85,334\n",
            "Validation samples: 21,334\n",
            "\n",
            "Data preparation completed successfully! 🎉\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# FEATURE ENGINEERING AND TRAINING MATRIX PREPARATION\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This code block prepares the final feature matrix for machine learning by:\n",
        "1. Creating and processing features from various condo characteristics\n",
        "2. Converting categorical variables into dummy variables\n",
        "3. Handling missing values and data type conversions\n",
        "4. Creating a location-based binning system\n",
        "5. Preparing both feature matrix (X) and target variable (y)\n",
        "\n",
        "Data Processing Steps:\n",
        "1. Input validation and filtering\n",
        "2. Numerical feature processing (Year_Built)\n",
        "3. Location feature creation (coordinate binning)\n",
        "4. Categorical feature encoding (dummy variables)\n",
        "5. Final matrix assembly and train/test split\n",
        "\n",
        "Key Features:\n",
        "- Uses sparse matrices to save memory\n",
        "- Creates location bins for geographic features\n",
        "- Handles missing values appropriately\n",
        "- Provides detailed logging of feature creation\n",
        "\"\"\"\n",
        "\n",
        "def prepare_training_data(df, min_samples=10):\n",
        "    \"\"\"\n",
        "    Prepares data for training the condo name prediction model by creating\n",
        "    a feature matrix from various condo characteristics.\n",
        "\n",
        "    This function:\n",
        "    1. Filters out condos with insufficient samples\n",
        "    2. Processes numerical features (Year_Built)\n",
        "    3. Creates location-based features\n",
        "    4. Generates dummy variables for categorical features\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas DataFrame\n",
        "        Dataset containing standardized condo information\n",
        "        Required columns: ['Condo_Name', 'Year_Built', 'Latitude', 'Longitude',\n",
        "                         'Postal_Code', 'Subdivision_Name', 'Condo_Complex']\n",
        "    min_samples : int, default=10\n",
        "        Minimum number of samples required for a condo to be included\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X : pandas DataFrame\n",
        "        Feature matrix with dummy variables\n",
        "    y : pandas Series\n",
        "        Target variable (Condo_Name to predict)\n",
        "    feature_info : dict\n",
        "        Information about generated features\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting training data preparation...\")\n",
        "\n",
        "        # Step 1: Input Validation\n",
        "        # ------------------------\n",
        "        required_columns = [\n",
        "            'Condo_Name', 'Year_Built', 'Latitude', 'Longitude',\n",
        "            'Postal_Code', 'Subdivision_Name', 'Condo_Complex'\n",
        "        ]\n",
        "\n",
        "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {', '.join(missing_cols)}\")\n",
        "\n",
        "        # Step 2: Filter Condos with Sufficient Samples\n",
        "        # -------------------------------------------\n",
        "        logger.log_info(\"\\nStep 1: Analyzing sample sizes\")\n",
        "        condo_counts = df['Condo_Name'].value_counts()\n",
        "        valid_condos = condo_counts[condo_counts >= min_samples].index\n",
        "\n",
        "        logger.log_info(f\"Total unique condos: {len(condo_counts):,}\")\n",
        "        logger.log_info(f\"Condos with {min_samples}+ samples: {len(valid_condos):,}\")\n",
        "        logger.log_info(\n",
        "            f\"Condos excluded: {len(condo_counts) - len(valid_condos):,}\"\n",
        "        )\n",
        "\n",
        "        # Create working dataset with only valid condos\n",
        "        working_df = df[df['Condo_Name'].isin(valid_condos)].copy()\n",
        "        logger.log_info(f\"Working dataset size: {len(working_df):,} records\")\n",
        "\n",
        "        # Step 3: Process Numerical Features\n",
        "        # --------------------------------\n",
        "        logger.log_info(\"\\nStep 2: Processing numerical features\")\n",
        "\n",
        "        # Handle Year_Built\n",
        "        logger.log_info(\"Processing Year_Built...\")\n",
        "        working_df['Year_Built'] = pd.to_numeric(\n",
        "            working_df['Year_Built'],\n",
        "            errors='coerce'\n",
        "        )\n",
        "\n",
        "        # Calculate and fill missing years\n",
        "        year_median = working_df['Year_Built'].median()\n",
        "        working_df['Year_Built'] = (\n",
        "            working_df['Year_Built']\n",
        "            .fillna(year_median)\n",
        "            .astype('int32')\n",
        "        )\n",
        "\n",
        "        logger.log_info(\n",
        "            f\"Year range: {working_df['Year_Built'].min()} \"\n",
        "            f\"to {working_df['Year_Built'].max()}\"\n",
        "        )\n",
        "\n",
        "        # Step 4: Process Location Features\n",
        "        # -------------------------------\n",
        "        logger.log_info(\"\\nStep 3: Processing location features\")\n",
        "\n",
        "        # Validate coordinate data\n",
        "        if working_df['Latitude'].isna().any() or working_df['Longitude'].isna().any():\n",
        "            logger.log_error(\"Warning: Missing coordinate data detected\")\n",
        "\n",
        "        # Create location bins (round to ~100m precision)\n",
        "        working_df['lat_bin'] = working_df['Latitude'].round(3)\n",
        "        working_df['lon_bin'] = working_df['Longitude'].round(3)\n",
        "\n",
        "        # Create unique location identifier\n",
        "        working_df['location_id'] = (\n",
        "            working_df['lat_bin'].astype(str) + '_' +\n",
        "            working_df['lon_bin'].astype(str)\n",
        "        )\n",
        "\n",
        "        logger.log_info(\n",
        "            f\"Created {working_df['location_id'].nunique():,} \"\n",
        "            \"unique location bins\"\n",
        "        )\n",
        "\n",
        "        # Step 5: Create Feature Matrix\n",
        "        # ---------------------------\n",
        "        logger.log_info(\"\\nStep 4: Creating feature matrix\")\n",
        "\n",
        "        # Define feature columns\n",
        "        feature_columns = [\n",
        "            'Year_Built',\n",
        "            'Postal_Code',\n",
        "            'Subdivision_Name',\n",
        "            'location_id',\n",
        "            'Condo_Complex'\n",
        "        ]\n",
        "\n",
        "        # Initialize feature collection\n",
        "        feature_frames = []\n",
        "        feature_info = {\n",
        "            'columns': {},\n",
        "            'total_features': 0,\n",
        "            'memory_usage': 0\n",
        "        }\n",
        "\n",
        "        # Create dummy variables for each feature\n",
        "        for column in feature_columns:\n",
        "            logger.log_info(f\"\\nProcessing {column}...\")\n",
        "\n",
        "            # Check for missing values\n",
        "            missing = working_df[column].isna().sum()\n",
        "            if missing > 0:\n",
        "                logger.log_info(\n",
        "                    f\"Warning: {missing:,} missing values in {column}\"\n",
        "                )\n",
        "\n",
        "            # Create dummy variables\n",
        "            dummies = pd.get_dummies(\n",
        "                working_df[column],\n",
        "                prefix=column,\n",
        "                sparse=True  # Use sparse matrices to save memory\n",
        "            )\n",
        "\n",
        "            # Add to collection\n",
        "            feature_frames.append(dummies)\n",
        "\n",
        "            # Store feature information\n",
        "            feature_info['columns'][column] = {\n",
        "                'unique_values': working_df[column].nunique(),\n",
        "                'features_created': len(dummies.columns),\n",
        "                'missing_values': missing\n",
        "            }\n",
        "\n",
        "            logger.log_info(\n",
        "                f\"Created {len(dummies.columns):,} features from {column}\"\n",
        "            )\n",
        "\n",
        "        # Combine all features\n",
        "        X = pd.concat(feature_frames, axis=1)\n",
        "\n",
        "        # Create target variable\n",
        "        y = working_df['Condo_Name']\n",
        "\n",
        "        # Update feature information\n",
        "        feature_info['total_features'] = X.shape[1]\n",
        "        feature_info['memory_usage'] = X.memory_usage().sum() / 1024**2  # MB\n",
        "\n",
        "        # Log final statistics\n",
        "        logger.log_info(\"\\nFinal Dataset Statistics:\")\n",
        "        logger.log_info(\"-\" * 30)\n",
        "        logger.log_info(f\"Feature matrix shape: {X.shape}\")\n",
        "        logger.log_info(f\"Number of target classes: {len(y.unique()):,}\")\n",
        "        logger.log_info(f\"Memory usage: {feature_info['memory_usage']:.2f} MB\")\n",
        "\n",
        "        logger.log_step_complete(\"Training data preparation\")\n",
        "\n",
        "        return X, y, feature_info\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in prepare_training_data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the data preparation\n",
        "try:\n",
        "    logger.log_info(\"Starting training data preparation process...\")\n",
        "\n",
        "    # Prepare training data\n",
        "    X, y, feature_info = prepare_training_data(complex_data)\n",
        "\n",
        "    # Print detailed feature information\n",
        "    logger.log_info(\"\\nFeature Information:\")\n",
        "    logger.log_info(\"-\" * 50)\n",
        "\n",
        "    for column, info in feature_info['columns'].items():\n",
        "        logger.log_info(f\"\\n{column}:\")\n",
        "        logger.log_info(f\"- Unique values: {info['unique_values']:,}\")\n",
        "        logger.log_info(f\"- Features created: {info['features_created']:,}\")\n",
        "        if info['missing_values'] > 0:\n",
        "            logger.log_info(f\"- Missing values: {info['missing_values']:,}\")\n",
        "\n",
        "    # Create train/validation split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,      # Use 20% of data for validation\n",
        "        random_state=42,    # Set seed for reproducibility\n",
        "        stratify=y          # Ensure balanced split of target classes\n",
        "    )\n",
        "\n",
        "    logger.log_info(\"\\nTrain/Validation Split:\")\n",
        "    logger.log_info(f\"Training samples: {len(X_train):,}\")\n",
        "    logger.log_info(f\"Validation samples: {len(X_val):,}\")\n",
        "\n",
        "    logger.log_info(\"\\nData preparation completed successfully! 🎉\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"Error during execution: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0Emvw4HQkdu"
      },
      "source": [
        "# Train/Validation Split System\n",
        "## Overview\n",
        "This code block creates a balanced split of our condo data into training and validation sets. It's crucial for ensuring our machine learning model can generalize well to new data while maintaining proper representation of all condo types.\n",
        "\n",
        "## Technical Details\n",
        "### Input\n",
        "* X: Feature matrix created in previous step\n",
        "* y: Target variable (Condo_Name)\n",
        "* test_size: Validation set proportion (default 20%)\n",
        "* random_state: Seed for reproducibility (default 42)\n",
        "* Current scale: Works with prepared feature matrix\n",
        "\n",
        "### Output\n",
        "* X_train, y_train: Training data (80% by default)\n",
        "* X_val, y_val: Validation data (20% by default)\n",
        "* Distribution verification statistics\n",
        "\n",
        "### Key Operations\n",
        "1. Sample Size Validation\n",
        "   * Checks minimum samples per condo class\n",
        "   * Warns if any condos have insufficient samples (<5)\n",
        "   * Reports distribution statistics\n",
        "\n",
        "2. Stratified Splitting\n",
        "   * Ensures proportional representation of each condo\n",
        "   * Maintains class balance between sets\n",
        "   * Preserves relationships between features and targets\n",
        "\n",
        "3. Distribution Verification\n",
        "   * Confirms all condos appear in both sets\n",
        "   * Verifies balanced distribution\n",
        "   * Reports detailed statistics\n",
        "\n",
        "## Code Review Findings\n",
        "1. Areas for Improvement:\n",
        "   * Switch from print() to logger for consistency\n",
        "   * Add input validation for test_size\n",
        "   * Consider adding cross-validation option\n",
        "   * Add data balance metrics\n",
        "\n",
        "2. Strong Points:\n",
        "   * Good stratification implementation\n",
        "   * Thorough distribution checking\n",
        "   * Clear warning system\n",
        "\n",
        "## Pipeline Implications\n",
        "This splitting step is crucial because:\n",
        "1. Affects model evaluation accuracy\n",
        "2. Impacts our ability to detect overfitting\n",
        "3. Influences final model performance\n",
        "\n",
        "## Next Steps\n",
        "After creating this split:\n",
        "1. Verify class distributions\n",
        "2. Consider cross-validation for small classes\n",
        "3. Begin model training\n",
        "4. Monitor for data leakage\n",
        "\n",
        "###### Console Output Guide\n",
        "Key information to watch for:\n",
        "```markdown\n",
        "* Minimum samples warning (<5 samples)\n",
        "* Split sizes and proportions\n",
        "* Distribution consistency check\n",
        "* Missing condo warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_validation_split(X, y, test_size=0.2, random_state=42, min_class_size=5):\n",
        "    \"\"\"\n",
        "    Creates a stratified train/validation split with enhanced class distribution analysis.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : pandas DataFrame\n",
        "        Feature matrix\n",
        "    y : pandas Series\n",
        "        Target variable (condo names)\n",
        "    test_size : float, default=0.2\n",
        "        Proportion of data to use for validation (between 0 and 1)\n",
        "    random_state : int, default=42\n",
        "        Random seed for reproducibility\n",
        "    min_class_size : int, default=5\n",
        "        Minimum samples per class for warning threshold\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple:\n",
        "        X_train : pandas DataFrame - Training features\n",
        "        X_val : pandas DataFrame - Validation features\n",
        "        y_train : pandas Series - Training targets\n",
        "        y_val : pandas Series - Validation targets\n",
        "        metrics : dict - Distribution and balance metrics\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting train/validation split process with enhanced analysis...\")\n",
        "\n",
        "        # Input Validation\n",
        "        if not isinstance(test_size, float) or not 0 < test_size < 1:\n",
        "            raise ValueError(\"test_size must be a float between 0 and 1\")\n",
        "        if not isinstance(random_state, int) or random_state < 0:\n",
        "            raise ValueError(\"random_state must be a non-negative integer\")\n",
        "        if len(X) != len(y):\n",
        "            raise ValueError(\"X and y must have the same length\")\n",
        "        if len(X) == 0:\n",
        "            raise ValueError(\"Input data is empty\")\n",
        "\n",
        "        # Detailed Class Distribution Analysis\n",
        "        # ----------------------------------\n",
        "        value_counts = y.value_counts()\n",
        "\n",
        "        # Calculate basic statistics\n",
        "        class_stats = {\n",
        "            'total_samples': len(y),\n",
        "            'total_classes': len(value_counts),\n",
        "            'min_samples': value_counts.min(),\n",
        "            'max_samples': value_counts.max(),\n",
        "            'median_samples': value_counts.median(),\n",
        "            'mean_samples': value_counts.mean(),\n",
        "            'std_samples': value_counts.std()\n",
        "        }\n",
        "\n",
        "        # Create class size bins\n",
        "        size_bins = [0, 10, 50, 100, 500, 1000, float('inf')]\n",
        "        size_labels = ['1-10', '11-50', '51-100', '101-500', '501-1000', '1000+']\n",
        "\n",
        "        # Group classes by size\n",
        "        class_sizes = pd.cut(\n",
        "            value_counts,\n",
        "            bins=size_bins,\n",
        "            labels=size_labels,\n",
        "            right=False\n",
        "        ).value_counts().sort_index()\n",
        "\n",
        "        # Calculate class percentiles\n",
        "        percentiles = {\n",
        "            'p10': value_counts.quantile(0.1),\n",
        "            'p25': value_counts.quantile(0.25),\n",
        "            'p50': value_counts.quantile(0.5),\n",
        "            'p75': value_counts.quantile(0.75),\n",
        "            'p90': value_counts.quantile(0.9)\n",
        "        }\n",
        "\n",
        "        # Find largest classes\n",
        "        top_classes = value_counts.head(5)\n",
        "        smallest_classes = value_counts.tail(5)\n",
        "\n",
        "        # Log Distribution Analysis\n",
        "        # -----------------------\n",
        "        logger.log_info(\"\\nDetailed Class Distribution Analysis:\")\n",
        "        logger.log_info(\"=\" * 50)\n",
        "        logger.log_info(f\"Total Dataset Statistics:\")\n",
        "        logger.log_info(f\"- Total samples: {class_stats['total_samples']:,}\")\n",
        "        logger.log_info(f\"- Total unique classes: {class_stats['total_classes']:,}\")\n",
        "\n",
        "        logger.log_info(\"\\nSample Distribution:\")\n",
        "        logger.log_info(f\"- Minimum samples per class: {class_stats['min_samples']:,}\")\n",
        "        logger.log_info(f\"- Maximum samples per class: {class_stats['max_samples']:,}\")\n",
        "        logger.log_info(f\"- Median samples per class: {class_stats['median_samples']:.1f}\")\n",
        "        logger.log_info(f\"- Mean samples per class: {class_stats['mean_samples']:.1f}\")\n",
        "        logger.log_info(f\"- Standard deviation: {class_stats['std_samples']:.1f}\")\n",
        "\n",
        "        logger.log_info(\"\\nClass Size Distribution:\")\n",
        "        for size_range, count in class_sizes.items():\n",
        "            logger.log_info(f\"- {size_range} samples: {count:,} classes\")\n",
        "\n",
        "        logger.log_info(\"\\nPercentile Analysis:\")\n",
        "        logger.log_info(f\"- 10th percentile: {percentiles['p10']:.1f} samples\")\n",
        "        logger.log_info(f\"- 25th percentile: {percentiles['p25']:.1f} samples\")\n",
        "        logger.log_info(f\"- 50th percentile: {percentiles['p50']:.1f} samples\")\n",
        "        logger.log_info(f\"- 75th percentile: {percentiles['p75']:.1f} samples\")\n",
        "        logger.log_info(f\"- 90th percentile: {percentiles['p90']:.1f} samples\")\n",
        "\n",
        "        logger.log_info(\"\\nLargest Classes:\")\n",
        "        for name, count in top_classes.items():\n",
        "            logger.log_info(f\"- {name}: {count:,} samples ({count/len(y)*100:.1f}% of dataset)\")\n",
        "\n",
        "        logger.log_info(\"\\nSmallest Classes:\")\n",
        "        for name, count in smallest_classes.items():\n",
        "            logger.log_info(f\"- {name}: {count:,} samples\")\n",
        "\n",
        "        # Calculate Imbalance Metrics\n",
        "        # -------------------------\n",
        "        imbalance_ratio = class_stats['max_samples'] / class_stats['min_samples']\n",
        "        gini_coefficient = 1 - ((value_counts / value_counts.sum())**2).sum()\n",
        "\n",
        "        logger.log_info(\"\\nImbalance Metrics:\")\n",
        "        logger.log_info(f\"- Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
        "        logger.log_info(f\"- Gini coefficient: {gini_coefficient:.3f}\")\n",
        "\n",
        "        # Perform Split\n",
        "        # ------------\n",
        "        logger.log_info(\"\\nCreating stratified split...\")\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X, y,\n",
        "            test_size=test_size,\n",
        "            random_state=random_state,\n",
        "            stratify=y\n",
        "        )\n",
        "\n",
        "        # Verify Split Distribution\n",
        "        # -----------------------\n",
        "        train_dist = y_train.value_counts()\n",
        "        val_dist = y_val.value_counts()\n",
        "\n",
        "        # Calculate split metrics\n",
        "        metrics = {\n",
        "            'class_stats': class_stats,\n",
        "            'class_sizes': class_sizes.to_dict(),\n",
        "            'percentiles': percentiles,\n",
        "            'imbalance_metrics': {\n",
        "                'imbalance_ratio': imbalance_ratio,\n",
        "                'gini_coefficient': gini_coefficient\n",
        "            },\n",
        "            'split_metrics': {\n",
        "                'train_size': len(X_train),\n",
        "                'val_size': len(X_val),\n",
        "                'train_classes': len(train_dist),\n",
        "                'val_classes': len(val_dist)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        logger.log_info(\"\\nSplit Verification:\")\n",
        "        logger.log_info(f\"Training set: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "        logger.log_info(f\"Validation set: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "        logger.log_info(f\"Classes in training: {len(train_dist):,}\")\n",
        "        logger.log_info(f\"Classes in validation: {len(val_dist):,}\")\n",
        "\n",
        "        # Provide recommendations based on analysis\n",
        "        logger.log_info(\"\\nRecommendations based on analysis:\")\n",
        "        if imbalance_ratio > 100:\n",
        "            logger.log_info(\"- High class imbalance detected. Consider:\")\n",
        "            logger.log_info(\"  * Implementing class weights\")\n",
        "            logger.log_info(\"  * Using SMOTE or other resampling techniques\")\n",
        "            logger.log_info(\"  * Stratified K-Fold cross-validation\")\n",
        "\n",
        "        if gini_coefficient > 0.8:\n",
        "            logger.log_info(\"- Severe distribution skew detected. Consider:\")\n",
        "            logger.log_info(\"  * Grouping rare classes\")\n",
        "            logger.log_info(\"  * Creating hierarchical classification\")\n",
        "\n",
        "        logger.log_step_complete(\"Enhanced train/validation split creation\")\n",
        "        return X_train, X_val, y_train, y_val, metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in create_train_validation_split: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the split\n",
        "try:\n",
        "    logger.log_info(\"Starting enhanced train/validation split process...\")\n",
        "\n",
        "    # Create split with enhanced analysis\n",
        "    X_train, X_val, y_train, y_val, split_metrics = create_train_validation_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        min_class_size=5\n",
        "    )\n",
        "\n",
        "    logger.log_info(\"\\nSplit completed successfully! 🎉\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"Error during split execution: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ILD9juflv9G",
        "outputId": "48370930-9755-4d80-c9cb-94e9974db4c7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting enhanced train/validation split process...\n",
            "Starting train/validation split process with enhanced analysis...\n",
            "\n",
            "Detailed Class Distribution Analysis:\n",
            "==================================================\n",
            "Total Dataset Statistics:\n",
            "- Total samples: 106,668\n",
            "- Total unique classes: 1,260\n",
            "\n",
            "Sample Distribution:\n",
            "- Minimum samples per class: 10\n",
            "- Maximum samples per class: 31,072\n",
            "- Median samples per class: 41.0\n",
            "- Mean samples per class: 84.7\n",
            "- Standard deviation: 875.7\n",
            "\n",
            "Class Size Distribution:\n",
            "- 1-10 samples: 0 classes\n",
            "- 11-50 samples: 747 classes\n",
            "- 51-100 samples: 306 classes\n",
            "- 101-500 samples: 204 classes\n",
            "- 501-1000 samples: 2 classes\n",
            "- 1000+ samples: 1 classes\n",
            "\n",
            "Percentile Analysis:\n",
            "- 10th percentile: 14.0 samples\n",
            "- 25th percentile: 23.0 samples\n",
            "- 50th percentile: 41.0 samples\n",
            "- 75th percentile: 74.0 samples\n",
            "- 90th percentile: 130.1 samples\n",
            "\n",
            "Largest Classes:\n",
            "- Z-name Not Listed: 31,072 samples (29.1% of dataset)\n",
            "- No Name: 561 samples (0.5% of dataset)\n",
            "- Copperwood: 515 samples (0.5% of dataset)\n",
            "- London at Heritage Station: 464 samples (0.4% of dataset)\n",
            "- Gateway Southcentre: 452 samples (0.4% of dataset)\n",
            "\n",
            "Smallest Classes:\n",
            "- Rundleson Village II: 10 samples\n",
            "- Templewoods: 10 samples\n",
            "- Bella Rosa 1: 10 samples\n",
            "- The Rennaisance Of Mt Douglas: 10 samples\n",
            "- The Fairway Pavilions: 10 samples\n",
            "\n",
            "Imbalance Metrics:\n",
            "- Imbalance ratio: 3107.2:1\n",
            "- Gini coefficient: 0.914\n",
            "\n",
            "Creating stratified split...\n",
            "\n",
            "Split Verification:\n",
            "Training set: 85,334 samples (80.0%)\n",
            "Validation set: 21,334 samples (20.0%)\n",
            "Classes in training: 1,260\n",
            "Classes in validation: 1,260\n",
            "\n",
            "Recommendations based on analysis:\n",
            "- High class imbalance detected. Consider:\n",
            "  * Implementing class weights\n",
            "  * Using SMOTE or other resampling techniques\n",
            "  * Stratified K-Fold cross-validation\n",
            "- Severe distribution skew detected. Consider:\n",
            "  * Grouping rare classes\n",
            "  * Creating hierarchical classification\n",
            "Step 30 completed: Enhanced train/validation split creation\n",
            "\n",
            "Split completed successfully! 🎉\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SYreHo_QqK6"
      },
      "source": [
        "# Train Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2M6xXVXMBb5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a0bc3c-1074-448c-df38-f8752dcf28f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting baseline model training process...\n",
            "Starting baseline Random Forest model training...\n",
            "\n",
            "Dataset Information:\n",
            "Training samples: 85,334\n",
            "Validation samples: 21,334\n",
            "Number of features: 23,392\n",
            "Number of classes: 1,260\n",
            "\n",
            "Configuring Random Forest model...\n",
            "\n",
            "Training model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  4.0min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 250.24 seconds\n",
            "\n",
            "Making predictions on validation set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.3s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    5.1s finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.3s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    5.2s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model performance...\n",
            "\n",
            "Overall Accuracy: 0.9972\n",
            "\n",
            "Accuracy at confidence thresholds:\n",
            "\n",
            "Threshold 0.5:\n",
            "- Accuracy: 0.9984\n",
            "- Coverage: 99.51%\n",
            "- Count: 21,230 predictions\n",
            "\n",
            "Threshold 0.7:\n",
            "- Accuracy: 0.9994\n",
            "- Coverage: 98.12%\n",
            "- Count: 20,933 predictions\n",
            "\n",
            "Threshold 0.9:\n",
            "- Accuracy: 0.9998\n",
            "- Coverage: 93.27%\n",
            "- Count: 19,898 predictions\n",
            "\n",
            "Analyzing error patterns...\n",
            "\n",
            "Top 10 Most Common Misclassifications:\n",
            "\n",
            "1. Wrong Prediction:\n",
            "   Actual: Copperfield Park\n",
            "   Predicted: Copperfield Park II\n",
            "   Count: 9\n",
            "\n",
            "2. Wrong Prediction:\n",
            "   Actual: Copperfield Park II\n",
            "   Predicted: Copperfield Park\n",
            "   Count: 8\n",
            "\n",
            "3. Wrong Prediction:\n",
            "   Actual: Copperfield Park\n",
            "   Predicted: Copperfield Park III\n",
            "   Count: 6\n",
            "\n",
            "4. Wrong Prediction:\n",
            "   Actual: Point Mckay\n",
            "   Predicted: Point Mckay Phase I\n",
            "   Count: 2\n",
            "\n",
            "5. Wrong Prediction:\n",
            "   Actual: Point Mckay\n",
            "   Predicted: Point Mckay Phase II\n",
            "   Count: 2\n",
            "\n",
            "6. Wrong Prediction:\n",
            "   Actual: Barclay Estates\n",
            "   Predicted: Riverwest\n",
            "   Count: 1\n",
            "\n",
            "7. Wrong Prediction:\n",
            "   Actual: Bella Rosa 1\n",
            "   Predicted: Brookview Estates\n",
            "   Count: 1\n",
            "\n",
            "8. Wrong Prediction:\n",
            "   Actual: Canyon Glen Townhouses\n",
            "   Predicted: The Meadows\n",
            "   Count: 1\n",
            "\n",
            "9. Wrong Prediction:\n",
            "   Actual: 916 Memorial\n",
            "   Predicted: Sunnyside\n",
            "   Count: 1\n",
            "\n",
            "10. Wrong Prediction:\n",
            "   Actual: Estate\n",
            "   Predicted: The Estate\n",
            "   Count: 1\n",
            "Step 31 completed: Baseline model training\n",
            "\n",
            "Baseline model training completed successfully! 🎉\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        " def train_baseline_model(X_train, X_val, y_train, y_val):\n",
        "    \"\"\"\n",
        "    Trains a baseline Random Forest model for condo name prediction and\n",
        "    evaluates its performance through multiple metrics.\n",
        "\n",
        "    This function:\n",
        "    1. Configures and trains a Random Forest classifier\n",
        "    2. Makes predictions on validation data\n",
        "    3. Analyzes model performance at different confidence thresholds\n",
        "    4. Examines error patterns to understand model weaknesses\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : pandas DataFrame\n",
        "        Training feature matrix\n",
        "    X_val : pandas DataFrame\n",
        "        Validation feature matrix\n",
        "    y_train : pandas Series\n",
        "        Training target values (condo names)\n",
        "    y_val : pandas Series\n",
        "        Validation target values (condo names)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple:\n",
        "        model : RandomForestClassifier\n",
        "            Trained model ready for predictions\n",
        "        performance_metrics : dict\n",
        "            Dictionary containing various performance metrics\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from sklearn.ensemble import RandomForestClassifier\n",
        "        from sklearn.metrics import accuracy_score, classification_report\n",
        "        import time\n",
        "\n",
        "        logger.log_info(\"Starting baseline Random Forest model training...\")\n",
        "\n",
        "        # Step 1: Input Validation\n",
        "        # -----------------------\n",
        "        # Verify we have matching dimensions\n",
        "        if len(X_train) != len(y_train) or len(X_val) != len(y_val):\n",
        "            raise ValueError(\"Feature and target arrays must have matching lengths\")\n",
        "\n",
        "        # Verify we have the same features in train and validation\n",
        "        if not all(X_train.columns == X_val.columns):\n",
        "            raise ValueError(\"Training and validation features must match\")\n",
        "\n",
        "        # Log dataset sizes\n",
        "        logger.log_info(\"\\nDataset Information:\")\n",
        "        logger.log_info(f\"Training samples: {len(X_train):,}\")\n",
        "        logger.log_info(f\"Validation samples: {len(X_val):,}\")\n",
        "        logger.log_info(f\"Number of features: {X_train.shape[1]:,}\")\n",
        "        logger.log_info(f\"Number of classes: {len(y_train.unique()):,}\")\n",
        "\n",
        "        # Step 2: Model Configuration\n",
        "        # -------------------------\n",
        "        logger.log_info(\"\\nConfiguring Random Forest model...\")\n",
        "\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=100,     # Start with 100 trees\n",
        "            max_depth=None,       # Let trees grow fully\n",
        "            min_samples_split=2,  # Default value\n",
        "            min_samples_leaf=1,   # Default value\n",
        "            n_jobs=-1,           # Use all CPU cores\n",
        "            random_state=42,      # For reproducibility\n",
        "            verbose=1            # Show progress\n",
        "        )\n",
        "\n",
        "        # Step 3: Model Training\n",
        "        # --------------------\n",
        "        logger.log_info(\"\\nTraining model...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Calculate training time\n",
        "        training_time = time.time() - start_time\n",
        "        logger.log_info(f\"Training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "        # Step 4: Make Predictions\n",
        "        # ----------------------\n",
        "        logger.log_info(\"\\nMaking predictions on validation set...\")\n",
        "\n",
        "        # Get both predictions and probabilities\n",
        "        predictions = model.predict(X_val)\n",
        "        probabilities = model.predict_proba(X_val)\n",
        "        confidence_scores = probabilities.max(axis=1)\n",
        "\n",
        "        # Step 5: Performance Evaluation\n",
        "        # ----------------------------\n",
        "        logger.log_info(\"\\nEvaluating model performance...\")\n",
        "\n",
        "        # Calculate overall accuracy\n",
        "        accuracy = accuracy_score(y_val, predictions)\n",
        "        logger.log_info(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Analyze performance at different confidence thresholds\n",
        "        confidence_thresholds = [0.5, 0.7, 0.9]\n",
        "        threshold_metrics = {}\n",
        "\n",
        "        logger.log_info(\"\\nAccuracy at confidence thresholds:\")\n",
        "        for threshold in confidence_thresholds:\n",
        "            # Get high confidence predictions\n",
        "            high_confidence_mask = confidence_scores >= threshold\n",
        "            high_confidence_count = high_confidence_mask.sum()\n",
        "\n",
        "            if high_confidence_count > 0:\n",
        "                high_conf_accuracy = accuracy_score(\n",
        "                    y_val[high_confidence_mask],\n",
        "                    predictions[high_confidence_mask]\n",
        "                )\n",
        "                coverage = high_confidence_count / len(y_val)\n",
        "\n",
        "                threshold_metrics[threshold] = {\n",
        "                    'accuracy': high_conf_accuracy,\n",
        "                    'coverage': coverage,\n",
        "                    'count': high_confidence_count\n",
        "                }\n",
        "\n",
        "                logger.log_info(f\"\\nThreshold {threshold}:\")\n",
        "                logger.log_info(f\"- Accuracy: {high_conf_accuracy:.4f}\")\n",
        "                logger.log_info(f\"- Coverage: {coverage:.2%}\")\n",
        "                logger.log_info(f\"- Count: {high_confidence_count:,} predictions\")\n",
        "\n",
        "        # Step 6: Error Analysis\n",
        "        # --------------------\n",
        "        logger.log_info(\"\\nAnalyzing error patterns...\")\n",
        "\n",
        "        # Create analysis DataFrame\n",
        "        analysis_df = pd.DataFrame({\n",
        "            'Actual': y_val,\n",
        "            'Predicted': predictions,\n",
        "            'Confidence': confidence_scores\n",
        "        })\n",
        "\n",
        "        # Analyze errors\n",
        "        errors = analysis_df[analysis_df['Actual'] != analysis_df['Predicted']]\n",
        "        error_patterns = (\n",
        "            errors.groupby(['Actual', 'Predicted'])\n",
        "            .size()\n",
        "            .sort_values(ascending=False)\n",
        "        )\n",
        "\n",
        "        # Log most common errors\n",
        "        logger.log_info(\"\\nTop 10 Most Common Misclassifications:\")\n",
        "        for i, ((actual, predicted), count) in enumerate(\n",
        "            error_patterns.head(10).items(), 1\n",
        "        ):\n",
        "            logger.log_info(f\"\\n{i}. Wrong Prediction:\")\n",
        "            logger.log_info(f\"   Actual: {actual}\")\n",
        "            logger.log_info(f\"   Predicted: {predicted}\")\n",
        "            logger.log_info(f\"   Count: {count:,}\")\n",
        "\n",
        "        # Store all metrics\n",
        "        performance_metrics = {\n",
        "            'accuracy': accuracy,\n",
        "            'training_time': training_time,\n",
        "            'confidence_scores': confidence_scores,\n",
        "            'predictions': predictions,\n",
        "            'threshold_metrics': threshold_metrics,\n",
        "            'error_patterns': error_patterns,\n",
        "            'classification_report': classification_report(\n",
        "                y_val,\n",
        "                predictions,\n",
        "                output_dict=True\n",
        "            )\n",
        "        }\n",
        "\n",
        "        logger.log_step_complete(\"Baseline model training\")\n",
        "        return model, performance_metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in train_baseline_model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the model training\n",
        "try:\n",
        "    logger.log_info(\"Starting baseline model training process...\")\n",
        "\n",
        "    # Train the model\n",
        "    baseline_model, performance_metrics = train_baseline_model(\n",
        "        X_train, X_val, y_train, y_val\n",
        "    )\n",
        "\n",
        "    logger.log_info(\"\\nBaseline model training completed successfully! 🎉\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"Error during execution: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ChydioVrmJj"
      },
      "source": [
        "# Date Analysis and Standardization\n",
        "## Pipeline Status Check\n",
        "\n",
        "### Data Input Issue\n",
        "❗ **Important Pipeline Issue**: The code is using `cleaned_data` instead of our main pipeline variable `combined_data`. Let me show you how we should modify this code to maintain our data pipeline integrity.\n",
        "\n",
        "Here's the corrected version that maintains our pipeline consistency:\n",
        "\n",
        "```python\n",
        "# First analyze the dates in our main dataset\n",
        "date_analysis = analyze_date_columns(combined_data)\n",
        "\n",
        "# Convert dates and save back to our main pipeline variable\n",
        "combined_data = convert_dates_for_power_bi(combined_data)\n",
        "```\n",
        "\n",
        "## Understanding the Date Processing\n",
        "\n",
        "This code performs two crucial operations that work together like a sophisticated date management system. Let me explain why each part matters and how they work together.\n",
        "\n",
        "### The Analysis Phase\n",
        "The `analyze_date_columns` function works like a thorough date inspector. Imagine you're examining a collection of historical documents - you want to make sure all the dates make logical sense and follow a proper timeline. This function does exactly that for our real estate data by:\n",
        "\n",
        "1. **Checking Date Types and Formats**:\n",
        "   Just as we might find dates written in different styles (like \"January 1, 2024\" or \"01/01/2024\"), this function identifies how our dates are currently stored in the database.\n",
        "\n",
        "2. **Validating Date Sequences**:\n",
        "   In real estate transactions, events must happen in a logical order - you can't purchase a house before it's listed, and you can't close before purchasing. The function verifies these logical sequences.\n",
        "\n",
        "3. **Identifying Suspicious Dates**:\n",
        "   Just as we might question a document dated \"2025\" or \"1890\" in a modern real estate transaction, this function flags dates that seem unreasonable:\n",
        "   - Future dates that haven't happened yet\n",
        "   - Very old dates that predate modern real estate practices (before 1980)\n",
        "\n",
        "### The Conversion Phase\n",
        "The `convert_dates_for_power_bi` function acts like a date standardization expert. After we understand what issues exist in our dates, this function:\n",
        "\n",
        "1. **Creates Consistent Formats**:\n",
        "   Converts all dates to a standard datetime format that Power BI can understand and work with effectively.\n",
        "\n",
        "2. **Handles Errors Gracefully**:\n",
        "   If it encounters dates it can't convert, it marks them as null rather than failing or creating invalid data.\n",
        "\n",
        "3. **Maintains Data Quality**:\n",
        "   Keeps track of how many dates couldn't be converted, helping us understand our data quality.\n",
        "\n",
        "### Why This Matters for Our Pipeline\n",
        "This date standardization step is crucial because:\n",
        "- Proper date formats enable time-based analysis in Power BI\n",
        "- Consistent date formats prevent visualization errors\n",
        "- Validated date sequences ensure accurate transaction timeline analysis\n",
        "- Clean date data enables reliable trend analysis\n",
        "\n",
        "## Next Steps\n",
        "After this standardization, our `combined_data` will have properly formatted dates ready for:\n",
        "1. Timeline visualizations in Power BI\n",
        "2. Sales trend analysis\n",
        "3. Seasonal pattern identification\n",
        "4. Transaction duration calculations\n",
        "\n",
        "Would you like me to explain any part of this process in more detail? For instance, I could elaborate on how the date validation works or why certain date formats are better for Power BI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8PLs4CcL6rAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa10232-666b-421a-abd7-6d9d87342bfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Power BI column analysis...\n",
            "Starting Power BI column analysis...\n",
            "\n",
            "Total columns analyzed: 52\n",
            "Columns needing attention: 13 (25.0%)\n",
            "\n",
            "Column Analysis Results:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Column_Name                    Data_Type      Null_Count   Null_Percentage Cardinality_Score Text_Score   Concerns                      \n",
            "                     Basement          object  82,552      21.0%             0.1              9.2               Many Nulls              \n",
            "              Basement_Finish          object       0       0.0%             0.0              6.5                     None              \n",
            "         Bedrooms_Above_Grade         float64   8,181       2.1%             0.0                -                     None              \n",
            "         Bedrooms_Below_Grade         float64 237,930      60.4%             0.0                -               Many Nulls              \n",
            "    Below_Grade_Finished_Area          object 258,520      65.7%             8.6              3.0               Many Nulls              \n",
            "                         City          object       0       0.0%             0.0              3.6                     None              \n",
            "                   Close_Date  datetime64[ns]       0       0.0%             1.4                -                     None              \n",
            "                  Close_Price         float64       4       0.0%             4.4                -                     None              \n",
            "                   Commission          object 228,220      58.0%            11.5             19.6               Many Nulls              \n",
            "                    Condo_Fee          object 266,602      67.7%             4.6              3.5               Many Nulls              \n",
            "                   Condo_Name          object 285,307      72.5%             0.4              7.7               Many Nulls              \n",
            "                   Condo_Type          object      48       0.0%             0.0              5.6                     None              \n",
            "    Cumulative_Days_On_Market          object       7       0.0%             0.7              1.0                     None              \n",
            "               Days_On_Market          object       0       0.0%             0.2              1.0                     None              \n",
            "                   Full_Baths         float64   8,628       2.2%             0.0                -                     None              \n",
            "                    Garage_YN          object     801       0.2%             0.0              1.3                     None              \n",
            "                   Is_Walkout          object       0       0.0%             0.0              1.1                     None              \n",
            "                     LINC_Num          object       0       0.0%            79.3              4.8                     None              \n",
            "                     Latitude         float64       0       0.0%            53.6                -                     None              \n",
            "        Listing_Contract_Date          object       0       0.0%             1.4              5.0                     None              \n",
            "                    Longitude         float64       0       0.0%            52.8                -                     None              \n",
            "                  Lot_Size_SF          object  89,883      22.8%             2.8              2.5               Many Nulls              \n",
            "                      MLS_Num          object       0       0.0%           100.0              4.0         High Cardinality              \n",
            "            Num_Garage_Spaces          object 294,796      74.9%             0.1              0.9               Many Nulls              \n",
            "                Occupant_Type          object   1,619       0.4%             0.0              3.1                     None              \n",
            "          Original_List_Price         float64     364       0.1%             3.1                -                     None              \n",
            "               Original_Style          object   5,791       1.5%             0.1              5.1                     None              \n",
            "                      Parking          object  15,347       3.9%             5.4             15.4                     None              \n",
            "                  Postal_Code          object       0       0.0%            10.8              3.5                     None              \n",
            "          Previous_List_Price          object  64,513      16.4%             1.5              5.5                     None              \n",
            "              Previous_Status          object 137,430      34.9%             0.0              3.3               Many Nulls              \n",
            "               Price_Per_SqFt         float64     897       0.2%            57.7                -                     None              \n",
            "            Property_Sub_Type          object      54       0.0%             0.0              5.3                     None              \n",
            "       Purchase_Contract_Date          object 291,004      73.9%             0.4              5.0               Many Nulls              \n",
            "                    RMS_Total         float64     897       0.2%             1.1                -                     None              \n",
            "            Street_Dir_Suffix          object  48,229      12.2%             0.0              1.0                     None              \n",
            "               Structure_Type          object   3,683       0.9%             0.0              4.2                     None              \n",
            "                        Style          object   5,791       1.5%             0.0              4.8                     None              \n",
            "             Subdivision_Name          object     162       0.0%             0.1              5.4                     None              \n",
            "                        Suite          object 151,445      38.5%             0.0              4.3               Many Nulls              \n",
            "         Suite_Separate_Entry          object       0       0.0%             0.0              1.0                     None              \n",
            "           Tax_Assessed_Value          object 310,908      79.0%             1.3              4.0               Many Nulls              \n",
            "                  Total_Baths         float64       3       0.0%             0.0                -                     None              \n",
            "                         Year           int32       0       0.0%             0.0                -                     None              \n",
            "                   Year_Built         float64      86       0.0%             0.0                -                     None              \n",
            "                       Zoning          object   4,738       1.2%             0.8              2.4                     None              \n",
            "               data_load_date          object       0       0.0%             0.0              5.0                     None              \n",
            "          data_load_timestamp  datetime64[us]       0       0.0%             0.0                -                     None              \n",
            "                  source_file          object       0       0.0%             0.0             10.4                     None              \n",
            "      standardized_commission          object       0       0.0%             0.0              9.1                     None              \n",
            "         standardized_parking          object  15,347       3.9%             0.7             12.5                     None              \n",
            "standardized_parking_complete          object       0       0.0%             0.7             12.2                     None              \n",
            "\n",
            "Analysis completed successfully! 🎉\n"
          ]
        }
      ],
      "source": [
        "def analyze_columns_for_power_bi(df):\n",
        "    \"\"\"\n",
        "    Analyzes DataFrame columns with a focus on Power BI compatibility.\n",
        "    Displays results in a clear tabular format.\n",
        "\n",
        "    Args:\n",
        "        df: pandas DataFrame to analyze\n",
        "\n",
        "    Returns:\n",
        "        pandas DataFrame containing column analysis\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting Power BI column analysis...\")\n",
        "\n",
        "        # Initialize list to store column information\n",
        "        column_info = []\n",
        "\n",
        "        # Calculate total rows for percentages\n",
        "        total_rows = len(df)\n",
        "\n",
        "        # Analyze each column\n",
        "        for column in sorted(df.columns):  # Sort alphabetically\n",
        "            # Basic information\n",
        "            dtype = df[column].dtype\n",
        "            null_count = df[column].isna().sum()\n",
        "            null_percentage = (null_count / total_rows) * 100\n",
        "\n",
        "            # Calculate cardinality score (0-100)\n",
        "            unique_count = df[column].nunique()\n",
        "            cardinality_score = min((unique_count / total_rows) * 100, 100)\n",
        "\n",
        "            # Calculate text complexity score (0-100)\n",
        "            text_score = 0\n",
        "            if dtype == 'object':\n",
        "                try:\n",
        "                    # Calculate average text length\n",
        "                    avg_length = (\n",
        "                        df[column]\n",
        "                        .dropna()\n",
        "                        .astype(str)\n",
        "                        .str.len()\n",
        "                        .mean()\n",
        "                    )\n",
        "\n",
        "                    # Score based on text length\n",
        "                    if avg_length <= 50:\n",
        "                        text_score = (avg_length / 50) * 25\n",
        "                    elif avg_length <= 200:\n",
        "                        text_score = 25 + ((avg_length - 50) / 150) * 25\n",
        "                    elif avg_length <= 1000:\n",
        "                        text_score = 50 + ((avg_length - 200) / 800) * 25\n",
        "                    else:\n",
        "                        text_score = 75 + min((avg_length - 1000) / 1000 * 25, 25)\n",
        "                except:\n",
        "                    text_score = 0\n",
        "\n",
        "            # Add to our collection\n",
        "            column_info.append({\n",
        "                'Column_Name': column,\n",
        "                'Data_Type': str(dtype),\n",
        "                'Null_Count': null_count,\n",
        "                'Null_Percentage': round(null_percentage, 2),\n",
        "                'Cardinality_Score': round(cardinality_score, 2),\n",
        "                'Text_Score': round(text_score, 2)\n",
        "            })\n",
        "\n",
        "        # Create DataFrame from our collection\n",
        "        analysis_df = pd.DataFrame(column_info)\n",
        "\n",
        "        # Add a Power BI concern flag\n",
        "        analysis_df['Concerns'] = analysis_df.apply(\n",
        "            lambda row: ' | '.join(filter(None, [\n",
        "                'High Cardinality' if row['Cardinality_Score'] > 80 else '',\n",
        "                'Long Text' if row['Text_Score'] > 50 else '',\n",
        "                'Many Nulls' if row['Null_Percentage'] > 20 else ''\n",
        "            ])) or 'None',\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Set display options for better formatting\n",
        "        pd.set_option('display.max_rows', None)\n",
        "        pd.set_option('display.max_columns', None)\n",
        "        pd.set_option('display.width', None)\n",
        "        pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "        # Create a formatted display version\n",
        "        display_df = analysis_df.copy()\n",
        "\n",
        "        # Format numeric columns\n",
        "        display_df['Null_Count'] = display_df['Null_Count'].apply(\n",
        "            lambda x: f\"{x:,}\"\n",
        "        )\n",
        "        display_df['Null_Percentage'] = display_df['Null_Percentage'].apply(\n",
        "            lambda x: f\"{x:.1f}%\"\n",
        "        )\n",
        "        display_df['Cardinality_Score'] = display_df['Cardinality_Score'].apply(\n",
        "            lambda x: f\"{x:.1f}\"\n",
        "        )\n",
        "        display_df['Text_Score'] = display_df['Text_Score'].apply(\n",
        "            lambda x: f\"{x:.1f}\" if x > 0 else \"-\"\n",
        "        )\n",
        "\n",
        "        # Log summary statistics\n",
        "        logger.log_info(f\"\\nTotal columns analyzed: {len(display_df)}\")\n",
        "\n",
        "        # Count columns with concerns\n",
        "        concerns = analysis_df[analysis_df['Concerns'] != 'None']\n",
        "        if len(concerns) > 0:\n",
        "            logger.log_info(\n",
        "                f\"Columns needing attention: {len(concerns)} \"\n",
        "                f\"({(len(concerns)/len(display_df))*100:.1f}%)\"\n",
        "            )\n",
        "\n",
        "        # Display the formatted table\n",
        "        logger.log_info(\"\\nColumn Analysis Results:\")\n",
        "        logger.log_info(\"-\" * 100)\n",
        "        logger.log_info(display_df.to_string(\n",
        "            index=False,\n",
        "            justify='left',\n",
        "            col_space={\n",
        "                'Column_Name': 30,\n",
        "                'Data_Type': 12,\n",
        "                'Null_Count': 12,\n",
        "                'Null_Percentage': 12,\n",
        "                'Cardinality_Score': 12,\n",
        "                'Text_Score': 12,\n",
        "                'Concerns': 30\n",
        "            }\n",
        "        ))\n",
        "\n",
        "        return analysis_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_columns_for_power_bi: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the analysis\n",
        "try:\n",
        "    logger.log_info(\"Starting Power BI column analysis...\")\n",
        "\n",
        "    # Run the analysis\n",
        "    column_analysis = analyze_columns_for_power_bi(combined_data)\n",
        "\n",
        "    logger.log_info(\"\\nAnalysis completed successfully! 🎉\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"Error during execution: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "RD1HuMKMCv-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "091ed549-da77-4dc7-fa10-a312eb5f1464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting final export process...\n",
            "Starting Power BI export preparation...\n",
            "\n",
            "Verifying data types...\n",
            "Converted Previous_List_Price from object to float64\n",
            "Converted Tax_Assessed_Value from object to float64\n",
            "Converted Condo_Fee from object to float64\n",
            "Converted RMS_Total from float64 to int64\n",
            "Converted Below_Grade_Finished_Area from object to float64\n",
            "Converted Lot_Size_SF from object to float64\n",
            "Converted Num_Garage_Spaces from object to int64\n",
            "Converted Days_On_Market from object to int64\n",
            "Converted Cumulative_Days_On_Market from object to int64\n",
            "Converted Bedrooms_Above_Grade from float64 to int64\n",
            "Converted Year_Built from float64 to int64\n",
            "Converted Listing_Contract_Date from object to datetime64[ns]\n",
            "Converted Purchase_Contract_Date from object to datetime64[ns]\n",
            "\n",
            "Export Results:\n",
            "--------------------------------------------------\n",
            "Data exported to: /content/drive/My Drive/Realtor/Data Project/power_bi_ready_data.csv\n",
            "File size: 157.30 MB\n",
            "\n",
            "Column Changes:\n",
            "Original columns: 52\n",
            "Final columns: 47\n",
            "\n",
            "Columns removed:\n",
            "- standardized_parking\n",
            "- Year\n",
            "- Price_Per_SqFt\n",
            "- Original_Style\n",
            "- Commission\n",
            "\n",
            "Export completed successfully! 🎉\n"
          ]
        }
      ],
      "source": [
        "def prepare_and_export_for_power_bi(df, output_folder, columns_to_drop=None):\n",
        "    \"\"\"\n",
        "    Prepares data for Power BI by:\n",
        "    1. Validating and converting data types\n",
        "    2. Removing specified columns\n",
        "    3. Exporting to CSV\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame to prepare and export\n",
        "        output_folder: Path where the CSV should be saved\n",
        "        columns_to_drop: List of column names to remove (optional)\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            DataFrame: Prepared data\n",
        "            dict: Summary of changes and verifications\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting Power BI export preparation...\")\n",
        "\n",
        "        # Step 1: Create working copy\n",
        "        # --------------------------\n",
        "        df_export = df.copy()\n",
        "        issues_found = []\n",
        "\n",
        "        # Step 2: Define expected column types\n",
        "        # ----------------------------------\n",
        "        column_types = {\n",
        "            # Monetary columns (should be float)\n",
        "            'Close_Price': 'float64',\n",
        "            'Original_List_Price': 'float64',\n",
        "            'Previous_List_Price': 'float64',\n",
        "            'Tax_Assessed_Value': 'float64',\n",
        "            'Condo_Fee': 'float64',\n",
        "\n",
        "            # Numeric columns (integers)\n",
        "            'RMS_Total': 'int64',\n",
        "            'Below_Grade_Finished_Area': 'float64',\n",
        "            'Lot_Size_SF': 'float64',\n",
        "            'Num_Garage_Spaces': 'int64',\n",
        "            'Days_On_Market': 'int64',\n",
        "            'Cumulative_Days_On_Market': 'int64',\n",
        "            'Bedrooms_Above_Grade': 'int64',\n",
        "            'Year_Built': 'int64',\n",
        "\n",
        "            # Date columns\n",
        "            'Close_Date': 'datetime64[ns]',\n",
        "            'Listing_Contract_Date': 'datetime64[ns]',\n",
        "            'Purchase_Contract_Date': 'datetime64[ns]',\n",
        "\n",
        "            # Categorical columns (strings)\n",
        "            'Condo_Name': 'object',\n",
        "            'Condo_Complex': 'object',\n",
        "            'Postal_Code': 'object',\n",
        "            'Subdivision_Name': 'object'\n",
        "        }\n",
        "\n",
        "        # Step 3: Verify and convert data types\n",
        "        # -----------------------------------\n",
        "        logger.log_info(\"\\nVerifying data types...\")\n",
        "\n",
        "        for column, expected_type in column_types.items():\n",
        "            if column in df_export.columns:\n",
        "                current_type = str(df_export[column].dtype)\n",
        "\n",
        "                if current_type != expected_type:\n",
        "                    try:\n",
        "                        # Handle date columns\n",
        "                        if expected_type.startswith('datetime'):\n",
        "                            df_export[column] = pd.to_datetime(\n",
        "                                df_export[column],\n",
        "                                errors='coerce'\n",
        "                            )\n",
        "                        # Handle numeric columns\n",
        "                        elif expected_type.startswith(('int', 'float')):\n",
        "                            if df_export[column].dtype == 'object':\n",
        "                                df_export[column] = (\n",
        "                                    df_export[column]\n",
        "                                    .astype(str)\n",
        "                                    .str.replace('$', '')\n",
        "                                    .str.replace(',', '')\n",
        "                                    .str.replace('−', '-')\n",
        "                                )\n",
        "                            df_export[column] = pd.to_numeric(\n",
        "                                df_export[column],\n",
        "                                errors='coerce'\n",
        "                            )\n",
        "\n",
        "                            if expected_type == 'int64':\n",
        "                                df_export[column] = df_export[column].fillna(0).astype('int64')\n",
        "\n",
        "                        logger.log_info(\n",
        "                            f\"Converted {column} from {current_type} to {expected_type}\"\n",
        "                        )\n",
        "\n",
        "                    except Exception as e:\n",
        "                        issues_found.append(\n",
        "                            f\"Error converting {column}: {str(e)}\"\n",
        "                        )\n",
        "\n",
        "        # Step 4: Remove specified columns\n",
        "        # ------------------------------\n",
        "        # Default columns to drop if none specified\n",
        "        if columns_to_drop is None:\n",
        "            columns_to_drop = [\n",
        "                'standardized_parking',\n",
        "                'Year',\n",
        "                'Price_Per_SqFt',\n",
        "                'Original_Style',\n",
        "                'Commission'\n",
        "            ]\n",
        "\n",
        "        # Track column changes\n",
        "        columns_dropped = []\n",
        "        columns_not_found = []\n",
        "\n",
        "        # Remove specified columns\n",
        "        for column in columns_to_drop:\n",
        "            if column in df_export.columns:\n",
        "                df_export = df_export.drop(columns=column)\n",
        "                columns_dropped.append(column)\n",
        "            else:\n",
        "                columns_not_found.append(column)\n",
        "\n",
        "        # Step 5: Export to CSV\n",
        "        # -------------------\n",
        "        output_path = Path(output_folder) / 'power_bi_ready_data.csv'\n",
        "        df_export.to_csv(output_path, index=False, encoding='utf-8')\n",
        "\n",
        "        # Verify export and create summary\n",
        "        if output_path.exists():\n",
        "            file_size_mb = output_path.stat().st_size / (1024 * 1024)\n",
        "\n",
        "            # Create comprehensive summary\n",
        "            summary = {\n",
        "                'original_columns': len(df.columns),\n",
        "                'final_columns': len(df_export.columns),\n",
        "                'columns_dropped': columns_dropped,\n",
        "                'columns_not_found': columns_not_found,\n",
        "                'file_size_mb': file_size_mb,\n",
        "                'export_path': str(output_path),\n",
        "                'issues_found': issues_found,\n",
        "                'data_types': df_export.dtypes.to_dict()\n",
        "            }\n",
        "\n",
        "            # Log results\n",
        "            logger.log_info(\"\\nExport Results:\")\n",
        "            logger.log_info(\"-\" * 50)\n",
        "            logger.log_info(f\"Data exported to: {output_path}\")\n",
        "            logger.log_info(f\"File size: {file_size_mb:.2f} MB\")\n",
        "\n",
        "            logger.log_info(\"\\nColumn Changes:\")\n",
        "            logger.log_info(f\"Original columns: {len(df.columns):,}\")\n",
        "            logger.log_info(f\"Final columns: {len(df_export.columns):,}\")\n",
        "\n",
        "            if columns_dropped:\n",
        "                logger.log_info(\"\\nColumns removed:\")\n",
        "                for col in columns_dropped:\n",
        "                    logger.log_info(f\"- {col}\")\n",
        "\n",
        "            if issues_found:\n",
        "                logger.log_error(\"\\nIssues found:\")\n",
        "                for issue in issues_found:\n",
        "                    logger.log_error(f\"- {issue}\")\n",
        "\n",
        "            return df_export, summary\n",
        "\n",
        "        else:\n",
        "            raise FileNotFoundError(\"Export file was not created successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in prepare_and_export_for_power_bi: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the export\n",
        "try:\n",
        "    logger.log_info(\"Starting final export process...\")\n",
        "\n",
        "    # Define output folder\n",
        "    output_folder = folder_path.parent\n",
        "\n",
        "    # Prepare and export data\n",
        "    export_df, summary = prepare_and_export_for_power_bi(\n",
        "        combined_data,\n",
        "        output_folder\n",
        "    )\n",
        "\n",
        "    if not summary['issues_found']:\n",
        "        logger.log_info(\"\\nExport completed successfully! 🎉\")\n",
        "    else:\n",
        "        logger.log_error(\"\\nExport completed with issues - please review warnings above\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"Error during execution: {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOlcx/J8oSUXN27XbCVk89l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bbde107e9b9642e690f56ecac4f21700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28ebf00879fd4666898ea2ebafa6aebf",
              "IPY_MODEL_fe185bfe809e4e14bd8e62b5e3dfc3a3",
              "IPY_MODEL_d0d9d971fc184a8ca40d7b671a6134d5"
            ],
            "layout": "IPY_MODEL_d61fca44a2f546a98d531d9a73999207"
          }
        },
        "28ebf00879fd4666898ea2ebafa6aebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf271a0012754a88b614d2030dafb14d",
            "placeholder": "​",
            "style": "IPY_MODEL_187c45d946884361a5c794373bc9cefb",
            "value": "Processing CSV files: 100%"
          }
        },
        "fe185bfe809e4e14bd8e62b5e3dfc3a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b63c1a7695c74bc4bac227ffbaa667d2",
            "max": 83,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c1cb0ab25384d898f02bfca3ac47017",
            "value": 83
          }
        },
        "d0d9d971fc184a8ca40d7b671a6134d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14d9429878fa456e90e10ff8b525187e",
            "placeholder": "​",
            "style": "IPY_MODEL_87135e7e865d429b89b17f445bf6d6fe",
            "value": " 83/83 [00:47&lt;00:00,  1.74it/s]"
          }
        },
        "d61fca44a2f546a98d531d9a73999207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf271a0012754a88b614d2030dafb14d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "187c45d946884361a5c794373bc9cefb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b63c1a7695c74bc4bac227ffbaa667d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c1cb0ab25384d898f02bfca3ac47017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14d9429878fa456e90e10ff8b525187e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87135e7e865d429b89b17f445bf6d6fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed55d093bbf546839a34258b58be4140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19994ba2eda24344805eb8682c72d0be",
              "IPY_MODEL_cc9d4a6c52bf4698b5e13207a6eb947c",
              "IPY_MODEL_e7cc087d565e42b6b9e276e7be7320a2"
            ],
            "layout": "IPY_MODEL_da2c3ccad85543e5820c6a300985a07d"
          }
        },
        "19994ba2eda24344805eb8682c72d0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9846c6ec58a8433085766427545cb79a",
            "placeholder": "​",
            "style": "IPY_MODEL_7b915589b18e47248e55d81d92e08b14",
            "value": "Cleaning numeric columns: 100%"
          }
        },
        "cc9d4a6c52bf4698b5e13207a6eb947c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b781b39254c945d9be441aaa684492b8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4a5166c8108492eaa64a2bd24d7b72d",
            "value": 2
          }
        },
        "e7cc087d565e42b6b9e276e7be7320a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0160ab2289b466ca737cac866445a82",
            "placeholder": "​",
            "style": "IPY_MODEL_d7c09597395c46d29ff31e85421e1f97",
            "value": " 2/2 [00:00&lt;00:00,  2.91it/s]"
          }
        },
        "da2c3ccad85543e5820c6a300985a07d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9846c6ec58a8433085766427545cb79a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b915589b18e47248e55d81d92e08b14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b781b39254c945d9be441aaa684492b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4a5166c8108492eaa64a2bd24d7b72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0160ab2289b466ca737cac866445a82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7c09597395c46d29ff31e85421e1f97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f174308569244417b9e38a4f4ac4a1f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7711972b8e5c43289e6db5f5771a408f",
              "IPY_MODEL_9fb57e7a18e54a6181cc9e7971384e02",
              "IPY_MODEL_088d7b2ec5e24f80b77df543fc9675d9"
            ],
            "layout": "IPY_MODEL_2c0572fc1e414d469bd4a6473bffeacd"
          }
        },
        "7711972b8e5c43289e6db5f5771a408f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_009b97be026444e4b8bd379b512d80eb",
            "placeholder": "​",
            "style": "IPY_MODEL_fdc30f0c7c784bf69028339aad71966f",
            "value": "Processing groups:  97%"
          }
        },
        "9fb57e7a18e54a6181cc9e7971384e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_380a85f2bec64265a687160765417809",
            "max": 1502,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5001ee1f4f84a15aafb3ef5675f25ed",
            "value": 1456
          }
        },
        "088d7b2ec5e24f80b77df543fc9675d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb64f9452afd4715a7f7442f4f346734",
            "placeholder": "​",
            "style": "IPY_MODEL_af2566f6a5e5422d8c09a37be75512ec",
            "value": " 1456/1502 [00:32&lt;00:00, 58.70it/s]"
          }
        },
        "2c0572fc1e414d469bd4a6473bffeacd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "009b97be026444e4b8bd379b512d80eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdc30f0c7c784bf69028339aad71966f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "380a85f2bec64265a687160765417809": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5001ee1f4f84a15aafb3ef5675f25ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb64f9452afd4715a7f7442f4f346734": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af2566f6a5e5422d8c09a37be75512ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b727ae4f3ec455889c8f288fecc9274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e98e46ae97e4c7ea76da103cd546065",
              "IPY_MODEL_89b4c2133c16492c9779ecc8ee488aa0",
              "IPY_MODEL_90efc8e7700648a8963498a30448aabf"
            ],
            "layout": "IPY_MODEL_740e4fe08cbe48ba9330b25cb96391a9"
          }
        },
        "4e98e46ae97e4c7ea76da103cd546065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbd0e746a4bc4a99a5ed9aadec85e015",
            "placeholder": "​",
            "style": "IPY_MODEL_2df88486920a465d9015510823d49385",
            "value": "Applying fixes: "
          }
        },
        "89b4c2133c16492c9779ecc8ee488aa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e55a69e02b494d0b96150071fbf47356",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2036966dee3a46d3a4985f67012ffe7e",
            "value": 1
          }
        },
        "90efc8e7700648a8963498a30448aabf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b45fe636754f4a189e03794dd7baf8fc",
            "placeholder": "​",
            "style": "IPY_MODEL_72e68d17c540450d9f5d63487fa9ca4d",
            "value": " 1/? [00:00&lt;00:00, 11.81it/s]"
          }
        },
        "740e4fe08cbe48ba9330b25cb96391a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbd0e746a4bc4a99a5ed9aadec85e015": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df88486920a465d9015510823d49385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e55a69e02b494d0b96150071fbf47356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2036966dee3a46d3a4985f67012ffe7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b45fe636754f4a189e03794dd7baf8fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72e68d17c540450d9f5d63487fa9ca4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04b97d8f36ff4957a82c0c85b4d9b82a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92b3a317ca144a5dbcc36253fcb88e6e",
              "IPY_MODEL_8d70ee752b344f26bff7d6c89b08acc0",
              "IPY_MODEL_6c4a18237c714e31a0aa9f21359c1d65"
            ],
            "layout": "IPY_MODEL_bad63c23f2bd48de8a418c40ae1d80ce"
          }
        },
        "92b3a317ca144a5dbcc36253fcb88e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5467c1fafeb42319e067e45067d194e",
            "placeholder": "​",
            "style": "IPY_MODEL_7a75cf80e81a43a69f1f02b64ecd0e49",
            "value": "100%"
          }
        },
        "8d70ee752b344f26bff7d6c89b08acc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c253bb15c43640e1baced52b194dc36d",
            "max": 393664,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2442fb2a03d0431da146a8ef3a462d01",
            "value": 393664
          }
        },
        "6c4a18237c714e31a0aa9f21359c1d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fac4eb210b9441187271e755690ccd7",
            "placeholder": "​",
            "style": "IPY_MODEL_0b45a23e631b4875a1cdb9162d0a12ff",
            "value": " 393664/393664 [00:59&lt;00:00, 7055.04it/s]"
          }
        },
        "bad63c23f2bd48de8a418c40ae1d80ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5467c1fafeb42319e067e45067d194e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a75cf80e81a43a69f1f02b64ecd0e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c253bb15c43640e1baced52b194dc36d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2442fb2a03d0431da146a8ef3a462d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fac4eb210b9441187271e755690ccd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b45a23e631b4875a1cdb9162d0a12ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c91768a07dea427e9eae26029b661b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be08d8dd490d45b1b99882e95b2576a6",
              "IPY_MODEL_d11ba07bf32f43bc9c344a180809df1c",
              "IPY_MODEL_ccafa9da58f04c838c95879d2c16deb6"
            ],
            "layout": "IPY_MODEL_beea3cbadc754516b386f5fb30fb4f9b"
          }
        },
        "be08d8dd490d45b1b99882e95b2576a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_930e36293a3c4d9cb7ec6c335095a548",
            "placeholder": "​",
            "style": "IPY_MODEL_795e6734ac9c48c79fcd91c0b8ba9846",
            "value": "Processing Calgary outliers: 100%"
          }
        },
        "d11ba07bf32f43bc9c344a180809df1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45a4fc854cad4cb5965af3897251e128",
            "max": 704,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc16ff194ba74cd4b27bbf2f3fdceffc",
            "value": 704
          }
        },
        "ccafa9da58f04c838c95879d2c16deb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f60851e687447e0bb8f2381a7af158c",
            "placeholder": "​",
            "style": "IPY_MODEL_fc6cb8e7c8f54992b04c3faa841f1df8",
            "value": " 704/704 [00:00&lt;00:00, 2070.68it/s]"
          }
        },
        "beea3cbadc754516b386f5fb30fb4f9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "930e36293a3c4d9cb7ec6c335095a548": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "795e6734ac9c48c79fcd91c0b8ba9846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45a4fc854cad4cb5965af3897251e128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc16ff194ba74cd4b27bbf2f3fdceffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f60851e687447e0bb8f2381a7af158c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc6cb8e7c8f54992b04c3faa841f1df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0af6cae12e03416a96f4cfa2d1194672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2e608f90f0541d6b16ff7cd9d82a57b",
              "IPY_MODEL_e0ec4ad4ee4c42f083bd9952728c101f",
              "IPY_MODEL_e26aed9c10654302ae3e016172421bb0"
            ],
            "layout": "IPY_MODEL_4686df10069c48c79502f1210f5bef41"
          }
        },
        "d2e608f90f0541d6b16ff7cd9d82a57b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0aa73345b3f4478be0a9961bde44f05",
            "placeholder": "​",
            "style": "IPY_MODEL_91941451e1bf4184906e48712f70d09f",
            "value": "Processing Okotoks outliers: 100%"
          }
        },
        "e0ec4ad4ee4c42f083bd9952728c101f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7d16fa724e34d8a9898a077f10fc334",
            "max": 68,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7849d39befe4cbaa56e3e1afad6f424",
            "value": 68
          }
        },
        "e26aed9c10654302ae3e016172421bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c71d97861b624630bd4ca6bd1f06efb9",
            "placeholder": "​",
            "style": "IPY_MODEL_567d8ba44acb4640a2aa14650cb37bc7",
            "value": " 68/68 [00:00&lt;00:00, 1517.91it/s]"
          }
        },
        "4686df10069c48c79502f1210f5bef41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0aa73345b3f4478be0a9961bde44f05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91941451e1bf4184906e48712f70d09f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7d16fa724e34d8a9898a077f10fc334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7849d39befe4cbaa56e3e1afad6f424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c71d97861b624630bd4ca6bd1f06efb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "567d8ba44acb4640a2aa14650cb37bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f8f89d811484d9e8dc01939936b0507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ce9cb1ebecf47abb83577a474dbc133",
              "IPY_MODEL_49ac48f7db92418f9c55da459587fc35",
              "IPY_MODEL_1ce07207849a4b01903f428665d21d57"
            ],
            "layout": "IPY_MODEL_a5cb1d5d4a144a4f84fa216e8db1ef74"
          }
        },
        "4ce9cb1ebecf47abb83577a474dbc133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0db098ec4c3245d0a921d0c73a89ba78",
            "placeholder": "​",
            "style": "IPY_MODEL_8f86c383fb1a415090de413403f651a5",
            "value": "Processing Airdrie outliers: 100%"
          }
        },
        "49ac48f7db92418f9c55da459587fc35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b63c3cf70c234e438c09af095f117e3b",
            "max": 189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3743ba26f4c54bdfad2b2d01b21d5ecb",
            "value": 189
          }
        },
        "1ce07207849a4b01903f428665d21d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fac2c722d178430691ad199dbc178436",
            "placeholder": "​",
            "style": "IPY_MODEL_4aed070d3e30403cb35e3efaaf642165",
            "value": " 189/189 [00:00&lt;00:00, 2018.91it/s]"
          }
        },
        "a5cb1d5d4a144a4f84fa216e8db1ef74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0db098ec4c3245d0a921d0c73a89ba78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f86c383fb1a415090de413403f651a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b63c3cf70c234e438c09af095f117e3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3743ba26f4c54bdfad2b2d01b21d5ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fac2c722d178430691ad199dbc178436": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aed070d3e30403cb35e3efaaf642165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b139a95017204b50858dc75c81ceef0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_163e4d9fee0543f7ac6599f392ab1766",
              "IPY_MODEL_f01e9805bd49493ebe61e7e13356fc3a",
              "IPY_MODEL_5636888b426840afa0ea2a893a742a86"
            ],
            "layout": "IPY_MODEL_4912116e7f8b4779a5cf9adb70fca343"
          }
        },
        "163e4d9fee0543f7ac6599f392ab1766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30fcd308e0ab4a84b48cf3a6af487d2b",
            "placeholder": "​",
            "style": "IPY_MODEL_3540a0c106f54a6796bad9df8c8bdae8",
            "value": "Processing Cochrane outliers: 100%"
          }
        },
        "f01e9805bd49493ebe61e7e13356fc3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bd556317869431eaff7a7f14824bf7d",
            "max": 99,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_816c9fba649a40d6a9805e13c54e486d",
            "value": 99
          }
        },
        "5636888b426840afa0ea2a893a742a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10eb3a3b96bc41f7847e9fb3e11fd104",
            "placeholder": "​",
            "style": "IPY_MODEL_0c038b7a75b24e588542d417c0851c64",
            "value": " 99/99 [00:00&lt;00:00, 1777.97it/s]"
          }
        },
        "4912116e7f8b4779a5cf9adb70fca343": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30fcd308e0ab4a84b48cf3a6af487d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3540a0c106f54a6796bad9df8c8bdae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bd556317869431eaff7a7f14824bf7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "816c9fba649a40d6a9805e13c54e486d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10eb3a3b96bc41f7847e9fb3e11fd104": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c038b7a75b24e588542d417c0851c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f947a4ef5ecf4b8cbad2e838fa760348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8e0258542244c1bab45ca25f93f0e8b",
              "IPY_MODEL_a5a66cdf6df14e3ca788ec29c6b2242a",
              "IPY_MODEL_a8305f740a074480b05deaaa18ef8f23"
            ],
            "layout": "IPY_MODEL_ac4c7c97d9304a5cad09104d40c1dea5"
          }
        },
        "f8e0258542244c1bab45ca25f93f0e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7b50062620c42e3aba48db2f49b8083",
            "placeholder": "​",
            "style": "IPY_MODEL_ee756010c69e4d6daafc3ab434ca0f19",
            "value": "Processing Red Deer outliers: 100%"
          }
        },
        "a5a66cdf6df14e3ca788ec29c6b2242a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11be7334f6f64455be2905f7b0d11a58",
            "max": 31,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8badf1afb6234bddb5b2f4e0da8b00c4",
            "value": 31
          }
        },
        "a8305f740a074480b05deaaa18ef8f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa66bc2ae01446c09c091a949d6e1d9d",
            "placeholder": "​",
            "style": "IPY_MODEL_5592e8daf90d43da903aa1742060e384",
            "value": " 31/31 [00:00&lt;00:00, 1006.47it/s]"
          }
        },
        "ac4c7c97d9304a5cad09104d40c1dea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7b50062620c42e3aba48db2f49b8083": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee756010c69e4d6daafc3ab434ca0f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11be7334f6f64455be2905f7b0d11a58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8badf1afb6234bddb5b2f4e0da8b00c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa66bc2ae01446c09c091a949d6e1d9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5592e8daf90d43da903aa1742060e384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f00865bd5954eb482b5ae1d6ae43cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78bc8fbd12194e95831f9f96a7f073a6",
              "IPY_MODEL_0a01b87abc784d56b54d5768027d0128",
              "IPY_MODEL_255aed489d9c419e8609dde3fecd7fc9"
            ],
            "layout": "IPY_MODEL_b9392babcf2b42cebc61a5eccdf50da4"
          }
        },
        "78bc8fbd12194e95831f9f96a7f073a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76ff17a3b8e04438b5f64c0e79d64cce",
            "placeholder": "​",
            "style": "IPY_MODEL_53f8be3f9f3a470698386553b47dcbb5",
            "value": "Processing High River outliers: 100%"
          }
        },
        "0a01b87abc784d56b54d5768027d0128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_444fa3874cef42549e2ae379b73b0f83",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e06ec02cc1240d780ecc50b6b99d2ad",
            "value": 25
          }
        },
        "255aed489d9c419e8609dde3fecd7fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0012cebc355942718ea8ea5bbbc1bd7d",
            "placeholder": "​",
            "style": "IPY_MODEL_6f71500f3dbc4a83acc34c1882c58318",
            "value": " 25/25 [00:00&lt;00:00, 1022.79it/s]"
          }
        },
        "b9392babcf2b42cebc61a5eccdf50da4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76ff17a3b8e04438b5f64c0e79d64cce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53f8be3f9f3a470698386553b47dcbb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "444fa3874cef42549e2ae379b73b0f83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e06ec02cc1240d780ecc50b6b99d2ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0012cebc355942718ea8ea5bbbc1bd7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f71500f3dbc4a83acc34c1882c58318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}