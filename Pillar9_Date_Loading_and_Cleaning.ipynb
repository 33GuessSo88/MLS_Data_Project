{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/33GuessSo88/MLS_Data_Project/blob/main/Pillar9_Date_Loading_and_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRzUYj7o_MQc"
      },
      "source": [
        "# TO DO\n",
        "\n",
        "1.   Commission needs some work, there are still obvious commission structres in the 'other' category that need to be dealt with.\n",
        "2.   Coordinate cleaning, unable to clean 6, why?\n",
        "3. RMS why does it only fix one data point, I thought there were 4. also stops at 97%, why. need to observe more examples of suspicious values.\n",
        "4. Style analysis needs work, way too much reporting.\n",
        "5. fix the restart thing when changing runtime.\n",
        "6. there are 4 rows with no close price. need to address this.\n",
        "7. why are there so many null commissions?\n",
        "8. does the style code replace original style text? we should create a new standardized style column\n",
        "\n",
        "9. review the parking code, still looks verbose. create standardized parking column?\n",
        "10. after the random forest runs for condo name it shows a bunch of prediction mistakes, but some of them aren't mistakes. The sequential ones? Should I make them all one? Maybe not. There was one condo that the only difference was a THE.\n",
        "11. Build Date still seems to have odd dates in power bi.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NF2zRQaddYh"
      },
      "source": [
        "\n",
        "# Data Loading and Combination Script"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# GOOGLE DRIVE MOUNTING\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell handles mounting your Google Drive to access files.\n",
        "\"\"\"\n",
        "# It's standard practice to place imports at the top of the script or cell.\n",
        "from google.colab import drive\n",
        "\n",
        "def mount_google_drive():\n",
        "  \"\"\"Mounts Google Drive, handling authorization and errors.\"\"\"\n",
        "  try:\n",
        "    # The print statement clearly indicates the process is starting.\n",
        "    print(\"‚è≥ Attempting to mount Google Drive...\")\n",
        "\n",
        "    # force_remount=True is useful. It prevents errors if the drive\n",
        "    # is already mounted, which often happens when re-running cells.\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    # If the above line runs without error, the mount was successful.\n",
        "    print(\"‚úÖ Google Drive mounted successfully!\")\n",
        "    print(\"   Your files are available at: /content/drive/My Drive/\")\n",
        "\n",
        "  except Exception as e:\n",
        "    # If any error occurs during the mount, it's caught here.\n",
        "    print(f\"‚ùå An error occurred while mounting Google Drive: {e}\")\n",
        "    print(\"   Please try running the cell again and complete the authorization.\")\n",
        "\n",
        "# --- Execute the mounting function ---\n",
        "mount_google_drive()"
      ],
      "metadata": {
        "id": "MpGsPYE1VwHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 1: ENVIRONMENT & PACKAGE INSTALLATION\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell prepares the Colab environment. It uninstalls potentially\n",
        "conflicting packages and then force-installs the specific versions\n",
        "of key libraries required for this notebook.\n",
        "\n",
        "IMPORTANT: You MUST restart the runtime after running this cell.\n",
        "\"\"\"\n",
        "import sys\n",
        "\n",
        "# We use simple print() statements here because the logger\n",
        "# hasn't been initialized yet. This cell runs *before* the\n",
        "# main imports.\n",
        "print(\"=\"*80)\n",
        "print(\"CELL 1: Starting environment preparation...\")\n",
        "print(f\"Using Python version: {sys.version.split()[0]}\")\n",
        "\n",
        "print(\"\\n‚è≥ Section 1: Uninstalling potentially conflicting packages...\")\n",
        "# Using &> /dev/null silences the (often noisy) uninstall output.\n",
        "!pip uninstall -y thinc spacy fastai langchain-core tensorflow pandas &> /dev/null\n",
        "print(\"‚úÖ Uninstall complete.\")\n",
        "\n",
        "print(\"\\n‚è≥ Section 2: Installing pinned versions of core libraries...\")\n",
        "# We use --quiet to silence the output, --no-cache-dir to ensure\n",
        "# we get fresh packages, and --force-reinstall on numpy/pandas\n",
        "# to override Colab's pre-installed versions.\n",
        "!pip install --quiet --no-cache-dir \\\n",
        "    \"numpy==1.26.4\" --force-reinstall \\\n",
        "    \"pandas==2.2.2\" --force-reinstall \\\n",
        "    \"scikit-learn==1.5.0\" \\\n",
        "    \"matplotlib==3.8.0\" \\\n",
        "    \"seaborn==0.13.2\" \\\n",
        "    \"lightgbm==4.3.0\" \\\n",
        "    \"packaging>=24.2.0\" \\\n",
        "    \"psutil==5.9.8\" \\\n",
        "    \"tqdm==4.66.4\" \\\n",
        "    \"tabulate==0.9.0\"\n",
        "print(\"‚úÖ Installation complete.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 1 COMPLETE.\")\n",
        "print(\"‚ÄºÔ∏è IMPORTANT: Please manually restart the runtime now.\")\n",
        "print(\"    Click 'Runtime' -> 'Restart runtime' in the menu above.\")\n",
        "print(\"    After restarting, run the next cell (Cell 2) to import packages\")\n",
        "print(\"    and initialize the logger.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "_VZNMLLYWToR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 2: IMPORTS, VERSION VERIFICATION, AND HELPER FUNCTIONS (After Restart)\n",
        "# =================================================================================\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import gc\n",
        "import psutil\n",
        "\n",
        "# --- [IMPROVED] NotebookLogger Class ---\n",
        "class NotebookLogger:\n",
        "    \"\"\"\n",
        "    A logger class that automatically saves logs to Google Drive if mounted,\n",
        "    otherwise falls back to the local Colab /content/ directory.\n",
        "    \"\"\"\n",
        "    def __init__(self, log_dir: Path):\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        self.log_file = log_dir / f'notebook_run_{self.timestamp}.log'\n",
        "        self.start_time = datetime.now()\n",
        "\n",
        "        # Ensure the log directory exists\n",
        "        self.log_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        logging.basicConfig(\n",
        "            filename=self.log_file,\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            force=True\n",
        "        )\n",
        "        self._logger = logging.getLogger(self.__class__.__name__)\n",
        "        self._logger.info(f\"=== Starting New Notebook Run ({self.timestamp}) ===\")\n",
        "        self._logger.info(f\"Logging to: {self.log_file.resolve()}\")\n",
        "        print(f\"=== Starting New Notebook Run ({self.timestamp}) ===\")\n",
        "        print(f\"‚úÖ Logging to: {self.log_file.resolve()}\")\n",
        "\n",
        "    def log_info(self, message, also_print=False):\n",
        "        self._logger.info(message)\n",
        "        if also_print:\n",
        "            print(message)\n",
        "\n",
        "    def log_error(self, message, also_print=True):\n",
        "        self._logger.error(message, exc_info=True)\n",
        "        if also_print:\n",
        "            print(f\"‚ùå ERROR: {message}\")\n",
        "\n",
        "    def log_warning(self, message, also_print=True):\n",
        "        self._logger.warning(message)\n",
        "        if also_print:\n",
        "            print(f\"‚ö†Ô∏è WARNING: {message}\")\n",
        "\n",
        "    def display_log_summary(self):\n",
        "        duration = datetime.now() - self.start_time\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"LOG SUMMARY\")\n",
        "        print(f\"   - Run Duration: {duration}\")\n",
        "        print(f\"   - Full log file at: {self.log_file.resolve()}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "# --- Logic to determine persistent log directory ---\n",
        "# Check if Google Drive is mounted (which persists across kernel restarts)\n",
        "drive_path = Path(\"/content/drive/My Drive\")\n",
        "if drive_path.exists():\n",
        "    # If Drive is present, log to a persistent folder\n",
        "    default_log_dir = drive_path / \"colab_logs\"\n",
        "    print(\"Google Drive detected. Logs will be persistent.\")\n",
        "else:\n",
        "    # Otherwise, fall back to a temporary local folder\n",
        "    default_log_dir = Path(\"/content/colab_logs\")\n",
        "    print(\"‚ö†Ô∏è Google Drive not found. Logs will be temporary (in /content/).\")\n",
        "\n",
        "logger = NotebookLogger(log_dir=default_log_dir)\n",
        "logger.log_info(\"Logger initialized successfully (after restart).\")\n",
        "\n",
        "\n",
        "# SECTION 3: IMPORTS AND VERSION VERIFICATION\n",
        "# =================================================================================\n",
        "print(\"\\nSECTION 3: Importing libraries and verifying versions...\")\n",
        "try:\n",
        "    # Core libraries\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from collections import defaultdict\n",
        "    import time\n",
        "    import tabulate # <-- [FIXED] Import the main package\n",
        "\n",
        "    # Visualization libraries\n",
        "    import matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    # Utilities\n",
        "    import tqdm\n",
        "    from tqdm.notebook import tqdm as tqdm_notebook\n",
        "    from IPython.display import display\n",
        "\n",
        "    # --- Centralized Version Check (Unchanged, this is excellent) ---\n",
        "    expected_versions = {\n",
        "        \"numpy\": \"1.26.4\",\n",
        "        \"pandas\": \"2.2.2\",\n",
        "        \"scikit-learn\": \"1.5.0\",\n",
        "        \"matplotlib\": \"3.8.0\",\n",
        "        \"seaborn\": \"0.13.2\",\n",
        "        \"lightgbm\": \"4.3.0\",\n",
        "        \"psutil\": \"5.9.8\",\n",
        "        \"tqdm\": \"4.66.4\",\n",
        "        \"tabulate\": \"0.9.0\",\n",
        "    }\n",
        "    actual_versions = {\n",
        "        \"python\": sys.version.split()[0],\n",
        "        \"numpy\": np.__version__,\n",
        "        \"pandas\": pd.__version__,\n",
        "        \"scikit-learn\": __import__('sklearn').__version__,\n",
        "        \"matplotlib\": matplotlib.__version__,\n",
        "        \"seaborn\": sns.__version__,\n",
        "        \"lightgbm\": __import__('lightgbm').__version__,\n",
        "        \"psutil\": psutil.__version__,\n",
        "        \"tqdm\": tqdm.__version__,\n",
        "        \"tabulate\": tabulate.__version__, # <-- This will now work correctly\n",
        "    }\n",
        "\n",
        "    # Verify all expected versions\n",
        "    all_versions_correct = True\n",
        "    for lib, expected_ver in expected_versions.items():\n",
        "        actual_ver = actual_versions.get(lib)\n",
        "        if actual_ver != expected_ver:\n",
        "            error_msg = f\"CRITICAL: Incorrect {lib} version! Expected {expected_ver}, got {actual_ver}\"\n",
        "            logger.log_error(error_msg, also_print=True)\n",
        "            all_versions_correct = False\n",
        "\n",
        "    # <-- [FIXED] Call the function using 'tabulate.tabulate'\n",
        "    version_table = tabulate.tabulate(actual_versions.items(), headers=[\"Library\", \"Version\"], tablefmt=\"grid\")\n",
        "    logger.log_info(\"Library Version Verification:\\n\" + version_table)\n",
        "\n",
        "    if not all_versions_correct:\n",
        "        raise AssertionError(\"Stopping: One or more critical libraries have incorrect versions.\")\n",
        "\n",
        "    print(\"‚úÖ All libraries imported and versions verified successfully.\")\n",
        "\n",
        "except (ImportError, AssertionError) as e:\n",
        "    logger.log_error(f\"Failed during library import or version check: {e}\", also_print=True)\n",
        "    # This raise will stop the notebook, which is correct\n",
        "    raise SystemExit(\"Stopping: Critical library setup failed.\")\n",
        "\n",
        "\n",
        "# SECTION 4: HELPER FUNCTIONS (Unchanged, these are excellent)\n",
        "# =================================================================================\n",
        "def configure_display_settings():\n",
        "    \"\"\"Configure pandas and visualization settings.\"\"\"\n",
        "    try:\n",
        "        pd.set_option('display.max_columns', None)\n",
        "        pd.set_option('display.max_rows', 100)\n",
        "        pd.set_option('display.width', 1000)\n",
        "        pd.set_option('display.float_format', '{:.3f}'.format)\n",
        "        sns.set_theme(style=\"whitegrid\")\n",
        "        if 'matplotlib' in sys.modules:\n",
        "            plt.rcParams.update({'figure.figsize': [12, 7], 'font.size': 12, 'figure.dpi': 90})\n",
        "        logger.log_info(\"Display settings configured.\")\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error configuring display settings: {e}\")\n",
        "        raise\n",
        "\n",
        "def check_memory_usage(warning_threshold_mb=2000):\n",
        "    \"\"\"Monitor and log notebook memory usage (RSS).\"\"\"\n",
        "    try:\n",
        "        gc.collect()\n",
        "        process = psutil.Process(os.getpid())\n",
        "        memory_usage_mb = process.memory_info().rss / (1024 * 1024)\n",
        "        logger.log_info(f\"Current memory usage (RSS): {memory_usage_mb:.2f} MB\")\n",
        "        if memory_usage_mb > warning_threshold_mb:\n",
        "            logger.log_error(f\"Memory usage ({memory_usage_mb:.2f} MB) exceeds threshold ({warning_threshold_mb} MB)\")\n",
        "        return memory_usage_mb\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error checking memory usage: {e}\")\n",
        "        return None\n",
        "\n",
        "def verify_final_setup():\n",
        "    \"\"\"Visually verify the setup with a test plot and memory check.\"\"\"\n",
        "    logger.log_info(\"--- Verifying Final Setup ---\", also_print=True)\n",
        "    try:\n",
        "        plt.figure()\n",
        "        sns.histplot(np.random.randn(100), kde=True)\n",
        "        plt.title(\"Test Plot for Visualization Verification\")\n",
        "        plt.show()\n",
        "        logger.log_info(\"Test plot generated successfully.\")\n",
        "\n",
        "        check_memory_usage()\n",
        "        logger.log_info(\"Setup verification complete.\")\n",
        "        print(\"‚úÖ Setup verified (Test plot generated, memory checked).\")\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error during final setup verification: {e}\")\n",
        "        raise\n",
        "\n",
        "# SECTION 5: EXECUTION (Unchanged)\n",
        "# =================================================================================\n",
        "print(\"\\nSECTION 5: Configuring notebook and verifying setup...\")\n",
        "configure_display_settings()\n",
        "verify_final_setup()\n",
        "logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 2 COMPLETE.\")\n",
        "print(\"‚ú® All setup steps completed successfully! Ready for data loading.\")\n",
        "print(\"=\"*80) # <-- [FIXED]"
      ],
      "metadata": {
        "id": "_B8iLA1WXP7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "Go2sdH13MFtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 3: CSV FILE MERGING AND CLEANING (Corrected)\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell defines the core data loading and cleaning pipeline.\n",
        "It finds all CSVs in a specified folder, loads them individually,\n",
        "cleans column names, applies robust type and value cleaning\n",
        "(handling nulls, special characters, and dtypes), and then\n",
        "merges them into a single, unified DataFrame.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import tabulate\n",
        "import gc\n",
        "from datetime import datetime\n",
        "import re\n",
        "import traceback\n",
        "import numpy as np\n",
        "\n",
        "def _clean_column_names_for_loading(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"A non-verbose helper to standardize column names on load.\"\"\"\n",
        "    cols = df.columns\n",
        "    new_cols = [re.sub(r'[^a-zA-Z0-9_]+', '_', col).strip('_') for col in cols]\n",
        "    df.columns = new_cols\n",
        "\n",
        "    rename_dict = {\n",
        "        'MLS': 'MLS_Num', 'LINC': 'LINC_Num', 'DOM': 'Days_On_Market',\n",
        "        'CDOM': 'Cumulative_Days_On_Market', 'Bedrms_Above_Grade': 'Bedrooms_Above_Grade',\n",
        "        'BG_Fin_Area': 'Below_Grade_Finished_Area', 'Num_Garage_Sp': 'Num_Garage_Spaces'\n",
        "    }\n",
        "\n",
        "    df = df.rename(columns=rename_dict, errors='ignore')\n",
        "    return df\n",
        "\n",
        "def merge_and_clean_files_final(folder_path: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads, renames, merges, and cleans all CSV files from a folder,\n",
        "    handling common data quality issues and enforcing a schema.\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Starting Hardened data merging process...\", also_print=True)\n",
        "\n",
        "    schema = {\n",
        "        'RMS_Total': 'float64', 'Close_Price': 'float64', 'Days_On_Market': 'float64',\n",
        "        'Cumulative_Days_On_Market': 'float64', 'Total_Baths': 'float64', 'Num_Garage_Spaces': 'float64',\n",
        "        'Close_Date': 'datetime64[ns]', 'List_Date': 'datetime64[ns]',\n",
        "        'MLS_Num': 'str', 'Subdivision_Name': 'str', 'Property_Sub_Type': 'str', 'LINC_Num': 'str'\n",
        "    }\n",
        "\n",
        "    null_placeholders = ['nan', 'None', 'none', 'N/A', 'NA', 'n/a', '-']\n",
        "\n",
        "    try:\n",
        "        csv_files = list(folder_path.glob('*.csv'))\n",
        "        if not csv_files:\n",
        "            logger.log_error(\"No CSV files found in directory.\", also_print=True)\n",
        "            raise ValueError(f\"No CSV files found in {folder_path}\")\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error listing CSV files: {e}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "    print(f\"--> Found {len(csv_files)} CSV files. Starting processing...\")\n",
        "\n",
        "    all_dataframes = []\n",
        "    for file_path in tqdm(csv_files, desc=\"Reading and Cleaning CSVs\"):\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, low_memory=False, skipinitialspace=True)\n",
        "            df = _clean_column_names_for_loading(df)\n",
        "\n",
        "            for col in df.select_dtypes(include=['object']).columns:\n",
        "                df[col] = df[col].str.strip()\n",
        "                if col in ['Subdivision_Name', 'Property_Sub_Type', 'Status']:\n",
        "                    df[col] = df[col].str.lower()\n",
        "\n",
        "            for col, dtype in schema.items():\n",
        "                if col in df.columns:\n",
        "                    # First, standardize all common null placeholders to np.nan\n",
        "                    df[col] = df[col].replace(null_placeholders, np.nan)\n",
        "\n",
        "                    # === [THE FIX] ===\n",
        "                    # Apply type-specific conversions\n",
        "\n",
        "                    if dtype in ['float64', 'int64']:\n",
        "                        # 1. Convert to string (to safely use .str)\n",
        "                        # 2. Clean the string\n",
        "                        # 3. Convert to numeric\n",
        "\n",
        "                        # .astype(str) handles all input types (int, float, object)\n",
        "                        cleaned_series = df[col].astype(str).str.replace(r'[,\\$]', '', regex=True)\n",
        "\n",
        "                        # pd.to_numeric will correctly interpret 'nan' strings as np.nan\n",
        "                        df[col] = pd.to_numeric(cleaned_series, errors='coerce')\n",
        "\n",
        "                    elif dtype == 'datetime64[ns]':\n",
        "                        # This conversion is already safe\n",
        "                        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "                    elif dtype == 'str':\n",
        "                        # 1. Convert to string (to safely use .str)\n",
        "                        # 2. Clean the string\n",
        "                        # 3. Re-replace 'nan' string (from astype) back to np.nan\n",
        "\n",
        "                        # .astype(str) handles all input types\n",
        "                        cleaned_str = df[col].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
        "\n",
        "                        # After .astype(str), real nulls are now the string 'nan'. Fix this.\n",
        "                        df[col] = cleaned_str.replace('nan', np.nan)\n",
        "\n",
        "            df['source_file'] = file_path.name\n",
        "            all_dataframes.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log the error and the file, but continue processing\n",
        "            logger.log_error(f\"Failed to process file {file_path.name}: {e}\", also_print=True)\n",
        "            # Log the full traceback for this specific file to the log\n",
        "            logger.log_info(f\"Traceback for {file_path.name}:\\n{traceback.format_exc()}\")\n",
        "\n",
        "    if not all_dataframes:\n",
        "        logger.log_error(\"No CSV files were successfully processed.\", also_print=True)\n",
        "        raise ValueError(\"No CSV files were successfully processed.\")\n",
        "\n",
        "    print(\"\\n--> Merging all processed files...\")\n",
        "    combined_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
        "    logger.log_info(f\"Concatenation complete. Total rows pre-deduplication: {len(combined_df)}\")\n",
        "\n",
        "    print(\"\\n--> Verifying final data types against schema...\")\n",
        "    verification_report = []\n",
        "    for col, expected_dtype in schema.items():\n",
        "        if col in combined_df.columns:\n",
        "            actual_dtype = combined_df[col].dtype\n",
        "            is_ok = str(actual_dtype) == expected_dtype or (expected_dtype == 'str' and str(actual_dtype) == 'object')\n",
        "            status = \"‚úÖ OK\" if is_ok else \"‚ö†Ô∏è MISMATCH\"\n",
        "            verification_report.append([col, expected_dtype, str(actual_dtype), status])\n",
        "\n",
        "    print(tabulate.tabulate(verification_report, headers=[\"Column\", \"Expected Type\", \"Actual Type\", \"Status\"], tablefmt=\"grid\"))\n",
        "\n",
        "    print(\"\\n‚úÖ Hardened merging process complete!\")\n",
        "    return combined_df\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 3: Data Loading ---\", also_print=True)\n",
        "\n",
        "    folder_path = Path('/content/drive/My Drive/Realtor/Data Project/Pillar9_RawCSV_Files')\n",
        "\n",
        "    combined_data = merge_and_clean_files_final(folder_path)\n",
        "\n",
        "    print(f\"\\n‚úÖ Successfully loaded and cleaned {len(combined_data):,} records.\")\n",
        "    check_memory_usage()\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"PROCESS HALTED during data loading: {e}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "finally:\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 3 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "OXJyE2o5Ze-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 4: DUPLICATE ANALYSIS AND REMOVAL\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell analyzes and removes duplicate records based on a unique ID\n",
        "(MLS_Num), intelligently keeping the most recent record based on a\n",
        "date column (Close_Date).\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import tabulate  # <-- [FIX] Standardized import to match Cell 2\n",
        "from datetime import datetime\n",
        "import re\n",
        "import traceback\n",
        "import numpy as np\n",
        "\n",
        "def analyze_duplicates(df: pd.DataFrame, id_column: str, date_column: str, verbose: bool = True) -> tuple[pd.DataFrame, dict]:\n",
        "    \"\"\"\n",
        "    Analyzes and removes duplicates, keeping the most recent record based on a date column.\n",
        "\n",
        "    Logs detailed analysis to a file and prints a high-level summary if verbose.\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Starting Duplicate Analysis and Removal...\", also_print=True)\n",
        "\n",
        "    # --- 1. Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "    for col in [id_column, date_column]:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Required column missing from DataFrame: '{col}'\")\n",
        "\n",
        "    # [BEST PRACTICE] This copy prevents SettingWithCopyWarning\n",
        "    # and side effects.\n",
        "    df = df.copy()\n",
        "\n",
        "    # Ensure date column is datetime, converting if necessary\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df[date_column]):\n",
        "        logger.log_info(f\"Converting '{date_column}' to datetime for sorting...\", also_print=verbose)\n",
        "        df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
        "\n",
        "        if df[date_column].isnull().any():\n",
        "            # This is a critical warning\n",
        "            logger.log_error(f\"WARNING: Found null dates in '{date_column}' after conversion. These rows may be dropped from de-duplication.\", also_print=True)\n",
        "\n",
        "    # --- 2. Initial Analysis ---\n",
        "    total_rows = len(df)\n",
        "    duplicates_mask = df.duplicated(subset=[id_column], keep=False)\n",
        "    total_duplicate_rows = duplicates_mask.sum()\n",
        "    unique_ids_with_dupes = df.loc[duplicates_mask, id_column].nunique()\n",
        "\n",
        "    initial_report = (\n",
        "        f\"\\n--- Initial Duplicate Analysis (based on '{id_column}') ---\\n\"\n",
        "        f\"  - Total Rows Before: {total_rows:,}\\n\"\n",
        "        f\"  - Total Rows Involved in Duplication: {total_duplicate_rows:,}\\n\"\n",
        "        f\"  - Unique IDs with duplicates: {unique_ids_with_dupes:,}\"\n",
        "    )\n",
        "    logger.log_info(initial_report, also_print=verbose)\n",
        "\n",
        "    # --- 3. Detailed Analysis (Logged to File, warnings printed) ---\n",
        "    if total_duplicate_rows > 0:\n",
        "        dupe_df = df[duplicates_mask].sort_values([id_column, date_column])\n",
        "\n",
        "        # Log top 5 most frequent duplicates to file only\n",
        "        id_counts = dupe_df[id_column].value_counts()\n",
        "        logger.log_info(f\"Top 5 most duplicated IDs:\\n{id_counts.head().to_string()}\")\n",
        "\n",
        "        # [EXCELLENT CHECK] Log check for price discrepancies\n",
        "        if 'Close_Price' in df.columns:\n",
        "            discrepancies = dupe_df.groupby(id_column).filter(lambda x: x['Close_Price'].nunique() > 1)\n",
        "            if not discrepancies.empty:\n",
        "                logger.log_error(f\"WARNING: Found {discrepancies[id_column].nunique():,} IDs with differing 'Close_Price' values. Review needed.\", also_print=True)\n",
        "        else:\n",
        "            logger.log_info(\" 'Close_Price' column not found, skipping price discrepancy check.\")\n",
        "\n",
        "    # --- 4. Removal Process ---\n",
        "    # Sort by date (ascending), so 'last' is the most recent.\n",
        "    # na_position='first' ensures NaT/null dates are dropped.\n",
        "    df_sorted = df.sort_values(by=date_column, ascending=True, na_position='first')\n",
        "    df_clean = df_sorted.drop_duplicates(subset=[id_column], keep='last')\n",
        "\n",
        "    rows_removed = total_rows - len(df_clean)\n",
        "\n",
        "    # --- 5. Final Report and Return ---\n",
        "    summary_stats = {\n",
        "        'total_rows_before': total_rows,\n",
        "        'duplicates_removed': rows_removed,\n",
        "        'total_rows_after': len(df_clean),\n",
        "        'unique_ids_with_dupes': unique_ids_with_dupes,\n",
        "        'total_duplicate_rows': total_duplicate_rows\n",
        "    }\n",
        "\n",
        "    # Final verification should show 0 duplicates. This is CRITICAL.\n",
        "    if df_clean.duplicated(subset=[id_column]).sum() != 0:\n",
        "        logger.log_error(\"CRITICAL: Duplicate IDs remain after removal process! Investigate logic.\", also_print=True)\n",
        "    else:\n",
        "        logger.log_info(\"Verification: No duplicate IDs found after removal.\", also_print=verbose)\n",
        "\n",
        "    return df_clean, summary_stats\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 4: Duplicate Analysis ---\", also_print=True)\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty. Please run previous cells.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Duplicate Analysis and Removal Process\")\n",
        "\n",
        "    # Call the function and get the cleaned data and stats\n",
        "    combined_data, dupe_stats = analyze_duplicates(\n",
        "        df=combined_data,\n",
        "        id_column='MLS_Num',\n",
        "        date_column='Close_Date',\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Present the final summary in a clean table\n",
        "    summary_table = [\n",
        "        [\"Initial Rows\", f\"{dupe_stats['total_rows_before']:,}\"],\n",
        "        [\"Rows Involved in Duplication\", f\"{dupe_stats['total_duplicate_rows']:,}\"],\n",
        "        [\"Rows Removed\", f\"{dupe_stats['duplicates_removed']:,}\"],\n",
        "        [\"Final Rows\", f\"{dupe_stats['total_rows_after']:,}\"]\n",
        "    ]\n",
        "\n",
        "    # <-- [FIX] Using tabulate.tabulate() for consistency\n",
        "    report = tabulate.tabulate(summary_table, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\")\n",
        "\n",
        "    print(f\"\\n--- Duplicate Removal Summary ---\\n{report}\")\n",
        "    print(\"\\n‚úÖ Duplicate removal process completed successfully!\")\n",
        "\n",
        "    # Log memory usage after this operation\n",
        "    check_memory_usage()\n",
        "\n",
        "except (NameError, ValueError, TypeError) as e:\n",
        "    logger.log_error(f\"PROCESS HALTED: A configuration or data error occurred: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"An unexpected error occurred during duplicate analysis: {e}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "finally:\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 4 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "KzpPr-Xua6MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXM7zXjbxopw"
      },
      "source": [
        "# Data Analysis: Temporal Coverage and Quality Assessment\n",
        "\n",
        "## Overview\n",
        "This code block examines the quality and completeness of dates in our real estate dataset. It identifies gaps in our temporal data, analyzes listing patterns over time, and provides detailed statistics about our date coverage. This analysis is crucial for understanding any potential data collection issues or seasonal patterns in our MLS listings.\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### Input\n",
        "* DataFrame: Cleaned dataset with transaction dates\n",
        "* Primary Date Column: 'Close_Date'\n",
        "* Date Format: Handles multiple input formats automatically\n",
        "\n",
        "### Key Analysis Components\n",
        "1. Date Validation and Cleaning\n",
        "   * Checks for invalid date formats\n",
        "   * Converts strings to datetime objects\n",
        "   * Reports problematic date entries\n",
        "   * Creates backup of original dates\n",
        "\n",
        "2. Coverage Analysis\n",
        "   * Calculates total date range\n",
        "   * Identifies missing dates\n",
        "   * Groups consecutive missing dates into ranges\n",
        "   * Analyzes patterns in missing data\n",
        "\n",
        "3. Temporal Distribution Analysis\n",
        "   * Monthly listing counts\n",
        "   * Year-over-year comparisons\n",
        "   * Growth rate calculations\n",
        "   * Seasonal pattern identification\n",
        "\n",
        "### Quality Metrics Reported\n",
        "* Total date range covered\n",
        "* Number of unique dates\n",
        "* Average listings per date\n",
        "* Invalid date entries\n",
        "* Gaps in coverage\n",
        "* Longest periods without data\n",
        "\n",
        "## Process Output\n",
        "The analysis provides:\n",
        "* Date range summary\n",
        "* Missing date patterns\n",
        "* Monthly distribution tables\n",
        "* Year-over-year growth rates\n",
        "* Optional CSV export of missing dates\n",
        "* Comprehensive error logging\n",
        "\n",
        "## Error Protection\n",
        "* Creates date column backup\n",
        "* Validates date formats\n",
        "* Restores original data if errors occur\n",
        "* Provides detailed error messages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 5: DATE COVERAGE ANALYSIS\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell performs a detailed analysis of the date coverage for a\n",
        "specified column (e.g., 'Close_Date'). It identifies the total range,\n",
        "finds missing dates and consecutive gaps, and generates visualizations\n",
        "of the data distribution over time.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity and safety ---\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tabulate  # <-- [FIX] Standardized import for consistency\n",
        "from pathlib import Path\n",
        "from datetime import timedelta\n",
        "import traceback\n",
        "import numpy as np\n",
        "\n",
        "def analyze_date_coverage(df: pd.DataFrame, date_column: str, create_plots: bool = True, save_path: str = None) -> dict:\n",
        "    \"\"\"\n",
        "    Analyzes date coverage, identifies gaps, and optionally creates visualizations.\n",
        "    \"\"\"\n",
        "    logger.log_info(f\"Starting date coverage analysis for column '{date_column}'...\", also_print=True)\n",
        "\n",
        "    # --- 1. Validate and Prepare Date Column ---\n",
        "    if date_column not in df.columns:\n",
        "        raise ValueError(f\"Column '{date_column}' not found in DataFrame.\")\n",
        "\n",
        "    dates_converted = pd.to_datetime(df[date_column], errors='coerce')\n",
        "    valid_dates = dates_converted.dropna()\n",
        "\n",
        "    if valid_dates.empty:\n",
        "        logger.log_error(\"No valid dates found after parsing! Cannot perform analysis.\", also_print=True)\n",
        "        return None\n",
        "\n",
        "    # --- 2. Date Range and Gap Analysis ---\n",
        "    earliest_date, latest_date = valid_dates.min(), valid_dates.max()\n",
        "    full_date_range = pd.to_datetime(pd.date_range(start=earliest_date, end=latest_date, freq='D'))\n",
        "\n",
        "    # [PERFORMANCE FIX] This is much more efficient than converting a large\n",
        "    # Series to a set.\n",
        "    # 1. Get all unique days (normalized to remove time)\n",
        "    unique_observed_days = pd.Series(valid_dates.dt.normalize().unique())\n",
        "    # 2. Find all dates in the full range that are NOT in our observed list\n",
        "    missing_dates = full_date_range[~full_date_range.isin(unique_observed_days)]\n",
        "    missing_dates = sorted(list(missing_dates)) # Match original output type\n",
        "\n",
        "    # --- 3. High-Level Summary ---\n",
        "    summary = {\n",
        "        'earliest_date': earliest_date.strftime('%Y-%m-%d'),\n",
        "        'latest_date': latest_date.strftime('%Y-%m-%d'),\n",
        "        'total_days_in_range': (latest_date - earliest_date).days + 1,\n",
        "        'unique_dates_found': len(unique_observed_days), # Use the optimized variable\n",
        "        'missing_dates_count': len(missing_dates),\n",
        "        'valid_entries': len(valid_dates),\n",
        "        'invalid_entries': dates_converted.isnull().sum()\n",
        "    }\n",
        "    logger.log_info(f\"Detailed date analysis summary:\\n{pd.Series(summary).to_string()}\")\n",
        "\n",
        "    # --- 4. Detailed Gap Analysis (Unchanged, this is excellent) ---\n",
        "    if summary['missing_dates_count'] > 0:\n",
        "        missing_df = pd.DataFrame(missing_dates, columns=['date']).set_index('date')\n",
        "        missing_df['gap_group'] = (missing_df.index.to_series().diff() != timedelta(days=1)).cumsum()\n",
        "        gaps = missing_df.groupby('gap_group').agg(['first', 'last', 'size'])\n",
        "        gaps.columns = gaps.columns.droplevel(0)\n",
        "        gaps = gaps.rename(columns={'size': 'days_missing'})\n",
        "\n",
        "        longest_gaps = gaps.sort_values('days_missing', ascending=False).head(5)\n",
        "        logger.log_info(f\"Top 5 longest missing date gaps:\\n{longest_gaps.to_string()}\")\n",
        "\n",
        "        if save_path:\n",
        "            save_path_obj = Path(save_path) # Convert string to Path\n",
        "            save_path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
        "            longest_gaps.to_csv(save_path_obj)\n",
        "            logger.log_info(f\"Saved longest gaps report to {save_path_obj}\", also_print=True)\n",
        "\n",
        "    # --- 5. Visualizations (Unchanged, this is excellent) ---\n",
        "    if create_plots:\n",
        "        print(\"\\n--> Generating visualizations...\")\n",
        "        # Monthly Listings Heatmap\n",
        "        try:\n",
        "            monthly_counts = valid_dates.groupby([valid_dates.dt.year, valid_dates.dt.month]).size().unstack(fill_value=0)\n",
        "            num_years = len(monthly_counts)\n",
        "            fig_height = max(6, num_years * 0.7)\n",
        "            plt.figure(figsize=(14, fig_height))\n",
        "\n",
        "            sns.heatmap(monthly_counts, cmap=\"viridis\", linewidths=.5, annot=True, fmt=\"d\")\n",
        "            plt.title(f'Listings per Month ({date_column})', fontsize=16)\n",
        "            plt.xlabel('Month'), plt.ylabel('Year'), plt.yticks(rotation=0)\n",
        "            plt.tight_layout(), plt.show()\n",
        "            logger.log_info(\"Monthly heatmap generated successfully.\")\n",
        "        except Exception as e:\n",
        "            logger.log_error(f\"Could not generate monthly heatmap: {e}\", also_print=True)\n",
        "\n",
        "        # Yearly Listings Bar Chart\n",
        "        try:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            valid_dates.dt.year.value_counts().sort_index().plot(kind='bar', color='darkcyan')\n",
        "            plt.title(f'Total Listings per Year ({date_column})', fontsize=16)\n",
        "            plt.ylabel('Number of Listings'), plt.xlabel('Year')\n",
        "            plt.xticks(rotation=45), plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "            plt.tight_layout(), plt.show()\n",
        "            logger.log_info(\"Yearly bar chart generated successfully.\")\n",
        "        except Exception as e:\n",
        "            logger.log_error(f\"Could not generate yearly bar chart: {e}\", also_print=True)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 5: Date Coverage Analysis ---\", also_print=True)\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty. Please run previous cells.\")\n",
        "\n",
        "    # [CLEANUP] Removed duplicate header\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Comprehensive Date Coverage Analysis\")\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    # --- Parameters ---\n",
        "    target_date_column = 'Close_Date'\n",
        "    generate_plots = True\n",
        "    missing_dates_output_path = '/content/drive/My Drive/Realtor/Data Project/longest_date_gaps.csv'\n",
        "\n",
        "    # --- Execution ---\n",
        "    analysis_results = analyze_date_coverage(\n",
        "        df=combined_data,\n",
        "        date_column=target_date_column,\n",
        "        create_plots=generate_plots,\n",
        "        save_path=missing_dates_output_path\n",
        "    )\n",
        "\n",
        "    # --- Final Report ---\n",
        "    if analysis_results:\n",
        "        summary_table = [\n",
        "            [\"Date Range\", f\"{analysis_results['earliest_date']} to {analysis_results['latest_date']}\"],\n",
        "            [\"Total Days in Range\", f\"{analysis_results['total_days_in_range']:,}\"],\n",
        "            [\"Days with Data\", f\"{analysis_results['unique_dates_found']:,}\"],\n",
        "            [\"Days Missing Data\", f\"{analysis_results['missing_dates_count']:,}\"],\n",
        "            [\"Valid Date Entries\", f\"{analysis_results['valid_entries']:,}\"],\n",
        "            [\"Invalid/Unparseable Entries\", f\"{analysis_results['invalid_entries']:,}\"]\n",
        "        ]\n",
        "        # <-- [FIX] Using tabulate.tabulate() for consistency\n",
        "        report = tabulate.tabulate(summary_table, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\")\n",
        "        print(f\"\\n--- Date Coverage Summary for '{target_date_column}' ---\\n{report}\")\n",
        "        print(\"\\n‚úÖ Date analysis completed successfully!\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Date analysis failed to return results. See logs for details.\")\n",
        "\n",
        "    # Log memory usage after this operation\n",
        "    check_memory_usage()\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"PROCESS HALTED: A configuration or data error occurred: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"An unexpected error occurred during date analysis: {e}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "finally:\n",
        "    # [FIX] Added for consistency with other cells\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 5 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "6BCY0C7kdW6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 6: MANUAL INSPECTION OF EXTREME RMS_TOTAL VALUES\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell provides a read-only, manual inspection of the properties\n",
        "with the highest and lowest recorded square footage. This helps\n",
        "to manually identify anomalies and validate the corrections\n",
        "in the next cell.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import traceback\n",
        "\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 6: Extreme Value Inspection ---\", also_print=True)\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame):\n",
        "        raise NameError(\"Variable 'combined_data' not found. Please run previous cells.\")\n",
        "\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "    print(\"Inspecting extreme values in 'combined_data'...\")\n",
        "\n",
        "    # [BEST PRACTICE] Create a temporary copy for safe inspection.\n",
        "    inspection_df = combined_data.copy()\n",
        "\n",
        "    inspection_df['RMS_Total'] = pd.to_numeric(inspection_df['RMS_Total'], errors='coerce')\n",
        "    inspection_df['Close_Price'] = pd.to_numeric(inspection_df['Close_Price'], errors='coerce')\n",
        "\n",
        "    display_cols = ['Subdivision_Name', 'Property_Sub_Type', 'RMS_Total', 'Close_Price']\n",
        "\n",
        "    # --- Display the 20 LARGEST RMS values ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TOP 20 LARGEST RMS_Total VALUES\")\n",
        "    print(\"=\"*80)\n",
        "    largest_rms = inspection_df.sort_values(by='RMS_Total', ascending=False).head(20)\n",
        "    display(largest_rms[display_cols])\n",
        "\n",
        "    # --- Display the 20 SMALLEST (but > 0) RMS values ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TOP 20 SMALLEST (Positive) RMS_Total VALUES\")\n",
        "    print(\"=\"*80)\n",
        "    # [GOOD LOGIC] Filtering for > 0 avoids zeros or errors.\n",
        "    smallest_rms = inspection_df[inspection_df['RMS_Total'] > 0].sort_values(by='RMS_Total', ascending=True).head(20)\n",
        "    display(smallest_rms[display_cols])\n",
        "\n",
        "except (NameError, Exception) as e:\n",
        "    logger.log_error(f\"Error during value inspection: {e}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "finally:\n",
        "    # This is a read-only step, so no data changes\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"\\nDataFrame shape at end: {combined_data.shape} (unchanged)\")\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 6 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "FNSoiWEjeEIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 7: ANOMALY CORRECTION (V16.2 - Exclude High-Price Anomalies)\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell runs the advanced, heuristic-first anomaly correction function.\n",
        "It identifies and corrects implausible RMS_Total and Close_Price values\n",
        "based on peer-group medians, while intelligently excluding legitimate\n",
        "high-price sales from correction.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import traceback\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "from IPython.display import display\n",
        "\n",
        "def correct_anomalies_with_heuristic(\n",
        "    df: pd.DataFrame,\n",
        "    ratio_threshold: float = 4.0,\n",
        "    min_peer_group_size: int = 5, # Not used in this version, but good parameter to have\n",
        "    clip_percentiles: tuple[float, float] = (0.05, 0.95)\n",
        ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Finds and corrects anomalies in RMS_Total and Close_Price using a heuristic-first approach\n",
        "    with a statistical fallback for maximum accuracy and plausibility. (V16.2)\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Starting Heuristic-First Anomaly Correction (V16.2)...\", also_print=True)\n",
        "\n",
        "    df_work = df.copy()\n",
        "    df_work.reset_index(inplace=True, names='original_index')\n",
        "\n",
        "    # --- 1. Preparation & Benchmark Calculation ---\n",
        "    df_work['Year'] = pd.to_datetime(df_work['Close_Date'], errors='coerce').dt.year\n",
        "    df_work['Price_Per_SqFt'] = pd.to_numeric(df_work['Close_Price'], errors='coerce') / pd.to_numeric(df_work['RMS_Total'], errors='coerce')\n",
        "\n",
        "    group_cols = ['Subdivision_Name', 'Property_Sub_Type', 'Year']\n",
        "    stats_to_agg = {\n",
        "        'RMS_Total': ['median',\n",
        "                      lambda x: x.quantile(clip_percentiles[0]),\n",
        "                      lambda x: x.quantile(clip_percentiles[1])],\n",
        "        'Close_Price': ['median',\n",
        "                        lambda x: x.quantile(clip_percentiles[0]),\n",
        "                        lambda x: x.quantile(clip_percentiles[1])],\n",
        "        'Price_Per_SqFt': ['median']\n",
        "    }\n",
        "\n",
        "    # [GOOD LOGIC] This correctly calculates stats and flattens the multi-index\n",
        "    peer_stats = df_work.groupby(group_cols).agg(stats_to_agg)\n",
        "    peer_stats.columns = ['_'.join(col).strip() for col in peer_stats.columns.values]\n",
        "    peer_stats = peer_stats.reset_index()\n",
        "\n",
        "    peer_stats.rename(columns={\n",
        "        'RMS_Total_<lambda_0>': 'RMS_Total_p05', 'RMS_Total_<lambda_1>': 'RMS_Total_p95',\n",
        "        'Close_Price_<lambda_0>': 'Close_Price_p05', 'Close_Price_<lambda_1>': 'Close_Price_p95',\n",
        "        'Price_Per_SqFt_median': 'Price_Per_SqFt_Peer_Median'\n",
        "    }, inplace=True)\n",
        "\n",
        "    # [GOOD LOGIC] Re-assignment (no inplace=True)\n",
        "    peer_stats = peer_stats.rename(columns={\n",
        "        'RMS_Total_<lambda_0>': 'RMS_Total_p05', 'RMS_Total_<lambda_1>': 'RMS_Total_p95',\n",
        "        'Close_Price_<lambda_0>': 'Close_Price_p05', 'Close_Price_<lambda_1>': 'Close_Price_p95',\n",
        "        'Price_Per_SqFt_median': 'Price_Per_SqFt_Peer_Median'\n",
        "    }, errors='ignore') # Added errors='ignore' for safety\n",
        "\n",
        "    df_work = pd.merge(df_work, peer_stats, on=group_cols, how='left', suffixes=('', '_Peer'))\n",
        "\n",
        "    # --- 2. Identify & Diagnose Anomalies ---\n",
        "    df_work.set_index('original_index', inplace=True, drop=True)\n",
        "    df_work['PPSF_Ratio'] = df_work['Price_Per_SqFt'] / df_work['Price_Per_SqFt_Peer_Median']\n",
        "\n",
        "    anomaly_mask_initial = (df_work['PPSF_Ratio'] > ratio_threshold) | (df_work['PPSF_Ratio'] < 1/ratio_threshold)\n",
        "    valid_peer_data_mask = df_work[['RMS_Total_median', 'Close_Price_median']].notna().all(axis=1)\n",
        "\n",
        "    anomalies = df_work[valid_peer_data_mask & anomaly_mask_initial].copy()\n",
        "\n",
        "    if anomalies.empty:\n",
        "        logger.log_info(\"No initial anomalies found based on PPSF ratio.\", also_print=True)\n",
        "        return df, pd.DataFrame()\n",
        "\n",
        "    size_dev = np.abs(anomalies['RMS_Total'] - anomalies['RMS_Total_median']) / anomalies['RMS_Total_median']\n",
        "    price_dev = np.abs(anomalies['Close_Price'] - anomalies['Close_Price_median']) / anomalies['Close_Price_median']\n",
        "    anomalies['Diagnosed_Error_In'] = np.where(size_dev > price_dev, 'RMS_Total', 'Close_Price')\n",
        "\n",
        "    # [EXCELLENT BUSINESS LOGIC] (V16.2)\n",
        "    is_high_price_anomaly_to_exclude = (anomalies['PPSF_Ratio'] > ratio_threshold) & \\\n",
        "                                     (anomalies['Diagnosed_Error_In'] == 'Close_Price')\n",
        "\n",
        "    initial_anomalies_count = len(anomalies)\n",
        "    anomalies = anomalies[~is_high_price_anomaly_to_exclude].copy()\n",
        "\n",
        "    if initial_anomalies_count > len(anomalies):\n",
        "        logger.log_info(f\"Excluded {initial_anomalies_count - len(anomalies)} high-price anomalies from correction.\", also_print=True)\n",
        "\n",
        "    if anomalies.empty:\n",
        "        logger.log_info(\"No anomalies remain after excluding high-price, diagnosed-as-price-error cases.\", also_print=True)\n",
        "        return df, pd.DataFrame()\n",
        "\n",
        "    logger.log_info(f\"Proceeding with correction for {len(anomalies)} eligible anomalies.\", also_print=True)\n",
        "\n",
        "    # --- 3. HYBRID CORRECTION LOGIC ---\n",
        "    is_too_small_error = anomalies['PPSF_Ratio'] > ratio_threshold\n",
        "    is_too_large_error = anomalies['PPSF_Ratio'] < 1/ratio_threshold\n",
        "\n",
        "    anomalies['Heuristic_RMS'] = np.select(\n",
        "        [is_too_small_error, is_too_large_error],\n",
        "        [anomalies['RMS_Total'] * 10, anomalies['RMS_Total'] / 10],\n",
        "        default=anomalies['RMS_Total']\n",
        "    )\n",
        "    anomalies['Heuristic_Price'] = np.select(\n",
        "        [is_too_large_error, is_too_small_error],\n",
        "        [anomalies['Close_Price'] * 10, anomalies['Close_Price'] / 10],\n",
        "        default=anomalies['Close_Price']\n",
        "    )\n",
        "\n",
        "    anomalies['Is_RMS_Heuristic_Sensible'] = anomalies['Heuristic_RMS'].between(anomalies['RMS_Total_p05'], anomalies['RMS_Total_p95'])\n",
        "    anomalies['Is_Price_Heuristic_Sensible'] = anomalies['Heuristic_Price'].between(anomalies['Close_Price_p05'], anomalies['Close_Price_p95'])\n",
        "\n",
        "    imputed_rms = (anomalies['Close_Price'] / anomalies['Price_Per_SqFt_Peer_Median']).clip(anomalies['RMS_Total_p05'], anomalies['RMS_Total_p95'])\n",
        "    imputed_price = (anomalies['RMS_Total'] * anomalies['Price_Per_SqFt_Peer_Median']).clip(anomalies['Close_Price_p05'], anomalies['Close_Price_p95'])\n",
        "\n",
        "    rms_is_error = anomalies['Diagnosed_Error_In'] == 'RMS_Total'\n",
        "    price_is_error = anomalies['Diagnosed_Error_In'] == 'Close_Price'\n",
        "\n",
        "    anomalies['Corrected_RMS'] = np.select(\n",
        "        [rms_is_error & anomalies['Is_RMS_Heuristic_Sensible'],\n",
        "         rms_is_error & ~anomalies['Is_RMS_Heuristic_Sensible']],\n",
        "        [anomalies['Heuristic_RMS'], imputed_rms],\n",
        "        default=anomalies['RMS_Total']\n",
        "    )\n",
        "    anomalies['Corrected_Price'] = np.select(\n",
        "        [price_is_error & anomalies['Is_Price_Heuristic_Sensible'],\n",
        "         price_is_error & ~anomalies['Is_Price_Heuristic_Sensible']],\n",
        "        [anomalies['Heuristic_Price'], imputed_price],\n",
        "        default=anomalies['Close_Price']\n",
        "    )\n",
        "\n",
        "    anomalies['Correction_Method'] = np.where(\n",
        "        (rms_is_error & anomalies['Is_RMS_Heuristic_Sensible']) | \\\n",
        "        (price_is_error & anomalies['Is_Price_Heuristic_Sensible']),\n",
        "        'Heuristic', 'Statistical'\n",
        "    )\n",
        "\n",
        "    # --- 4. Apply Fixes and Generate Final Report ---\n",
        "    # [EXCELLENT METHOD] Applies fixes back to a fresh copy\n",
        "    corrected_df = df.copy()\n",
        "    corrected_df.loc[anomalies.index, 'RMS_Total'] = anomalies['Corrected_RMS']\n",
        "    corrected_df.loc[anomalies.index, 'Close_Price'] = anomalies['Corrected_Price']\n",
        "\n",
        "    report_df = anomalies.rename(columns={'RMS_Total': 'Original_RMS', 'Close_Price': 'Original_Price', 'Year': 'Sale_Year'})\n",
        "    final_report_cols = [\n",
        "        'MLS_Num', 'Subdivision_Name', 'Sale_Year', 'Diagnosed_Error_In',\n",
        "        'Correction_Method', 'Original_RMS', 'Corrected_RMS', 'Original_Price', 'Corrected_Price'\n",
        "    ]\n",
        "\n",
        "    # Ensure all report columns exist before returning\n",
        "    final_report_cols = [col for col in final_report_cols if col in report_df.columns]\n",
        "    return corrected_df, report_df[final_report_cols]\n",
        "\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 7: Anomaly Correction ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üöÄ Running Heuristic-First Anomaly Correction (V16.2)\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty. Please run previous cells.\")\n",
        "\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    # [BEST PRACTICE] Pass a copy to the function\n",
        "    data_to_correct = combined_data.copy()\n",
        "\n",
        "    corrected_data, final_fixes_report = correct_anomalies_with_heuristic(data_to_correct)\n",
        "\n",
        "    if not final_fixes_report.empty:\n",
        "        print(f\"\\n--- Anomaly Correction Report ({len(final_fixes_report)} records corrected) ---\")\n",
        "\n",
        "        display_headers = ['MLS Num', 'Subdivision', 'Year', 'Diagnosed Error', 'Method',\n",
        "                           'Orig. RMS', 'Corr. RMS', 'Orig. Price', 'Corr. Price']\n",
        "\n",
        "        # [FIX] Using tabulate.tabulate() for consistency\n",
        "        print(tabulate.tabulate(final_fixes_report.head(20), headers=display_headers, tablefmt='grid', floatfmt=\",.0f\"))\n",
        "\n",
        "        if len(final_fixes_report) > 20:\n",
        "            print(f\"\\n... (showing top 20 of {len(final_fixes_report)} corrected records)\")\n",
        "            print(\"Full report available in 'final_fixes_report' DataFrame.\")\n",
        "\n",
        "        # --- DETAILED EDGE CASE ANOMALY REPORT ---\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"DETAILED ANOMALY REPORT: EXTREME RMS_Total VALUES (Corrected Cases)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        largest_orig_rms_anomalies = final_fixes_report.sort_values(by='Original_RMS', ascending=False).head(10)\n",
        "        if not largest_orig_rms_anomalies.empty:\n",
        "            print(\"\\n--- Top 10 Largest Original RMS (from Corrected Anomalies) ---\")\n",
        "            # [FIX] Using tabulate.tabulate()\n",
        "            print(tabulate.tabulate(largest_orig_rms_anomalies, headers=display_headers, tablefmt='grid', floatfmt=\",.0f\"))\n",
        "        else:\n",
        "            print(\"No anomalies with large original RMS values were corrected.\")\n",
        "\n",
        "        smallest_orig_rms_anomalies = final_fixes_report[final_fixes_report['Original_RMS'] > 0].sort_values(by='Original_RMS', ascending=True).head(10)\n",
        "        if not smallest_orig_rms_anomalies.empty:\n",
        "            print(\"\\n--- Top 10 Smallest Positive Original RMS (from Corrected Anomalies) ---\")\n",
        "            # [FIX] Using tabulate.tabulate()\n",
        "            print(tabulate.tabulate(smallest_orig_rms_anomalies, headers=display_headers, tablefmt='grid', floatfmt=\",.0f\"))\n",
        "        else:\n",
        "            print(\"No anomalies with small positive original RMS values were corrected.\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"DETAILED ANOMALY REPORT: EXTREME Close_Price VALUES (Corrected Cases)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        largest_orig_price_anomalies = final_fixes_report.sort_values(by='Original_Price', ascending=False).head(10)\n",
        "        if not largest_orig_price_anomalies.empty:\n",
        "            print(\"\\n--- Top 10 Largest Original Price (from Corrected Anomalies) ---\")\n",
        "            # [FIX] Using tabulate.tabulate()\n",
        "            print(tabulate.tabulate(largest_orig_price_anomalies, headers=display_headers, tablefmt='grid', floatfmt=\",.0f\"))\n",
        "        else:\n",
        "            print(\"No anomalies with large original price values were corrected.\")\n",
        "\n",
        "        smallest_orig_price_anomalies = final_fixes_report[final_fixes_report['Original_Price'] > 0].sort_values(by='Original_Price', ascending=True).head(10)\n",
        "        if not smallest_orig_price_anomalies.empty:\n",
        "            print(\"\\n--- Top 10 Smallest Positive Original Price (from Corrected Anomalies) ---\")\n",
        "            # [FIX] Using tabulate.tabulate()\n",
        "            print(tabulate.tabulate(smallest_orig_price_anomalies, headers=display_headers, tablefmt='grid', floatfmt=\",.0f\"))\n",
        "        else:\n",
        "            print(\"No anomalies with small positive original price values were corrected.\")\n",
        "\n",
        "        # [CRITICAL STEP] The main DataFrame is updated with the corrected data\n",
        "        combined_data = corrected_data\n",
        "        print(f\"\\n‚úÖ 'combined_data' has been updated with {len(final_fixes_report)} corrections.\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ No significant data entry errors were found (or no sufficient peer data for comparison).\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"PROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"An unexpected error occurred during anomaly correction: {e}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape}\")\n",
        "    # [FIX] Added for consistency\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 7 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "GK4os3kteOf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 8: SUITE STATUS CLEANING AND STANDARDIZATION\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell analyzes and standardizes the 'Suite' column. It maps\n",
        "various free-text inputs to three clean categories:\n",
        "'No Suite', 'Suite - Illegal', and 'Suite - Legal'.\n",
        "It includes a robust backup-and-restore-on-failure mechanism.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "import traceback # <-- [FIX] Added for exception logging\n",
        "\n",
        "def analyze_suite_distribution(df, column='Suite', log_to_console=False):\n",
        "    \"\"\"\n",
        "    Analyze the distribution of values in the suite column.\n",
        "    (This function is excellent, no changes needed)\n",
        "    \"\"\"\n",
        "    total_rows = len(df)\n",
        "    rows_with_data = df[column].notna().sum()\n",
        "    percentage_with_data = (rows_with_data / total_rows) * 100 if total_rows > 0 else 0\n",
        "\n",
        "    logger.log_info(f\"\\nSuite Data Analysis for '{column}':\", also_print=log_to_console)\n",
        "    logger.log_info(\"-\" * 50, also_print=log_to_console)\n",
        "    logger.log_info(f\"Total rows: {total_rows:,}\", also_print=log_to_console)\n",
        "    logger.log_info(f\"Rows with suite data: {rows_with_data:,}\", also_print=log_to_console)\n",
        "    logger.log_info(f\"Percentage with data: {percentage_with_data:.2f}%\", also_print=log_to_console)\n",
        "    logger.log_info(\"\\nCurrent Value Distribution (detailed list):\", also_print=log_to_console)\n",
        "    logger.log_info(\"-\" * 50, also_print=log_to_console)\n",
        "\n",
        "    value_counts = df[column].value_counts(dropna=False)\n",
        "\n",
        "    for value, count in value_counts.items():\n",
        "        value_str = 'NULL' if pd.isna(value) else value\n",
        "        percentage = (count / total_rows) * 100 if total_rows > 0 else 0\n",
        "        logger.log_info(f\"{value_str:<20} {count:>8,} ({percentage:>6.2f}%)\", also_print=log_to_console)\n",
        "\n",
        "    return {\n",
        "        'total_rows': total_rows,\n",
        "        'rows_with_data': rows_with_data,\n",
        "        'percentage_with_data': percentage_with_data,\n",
        "        'value_counts': value_counts.to_dict()\n",
        "    }\n",
        "\n",
        "def clean_suite(value):\n",
        "    \"\"\"\n",
        "    Clean and standardize suite values to three possible categories.\n",
        "    \"\"\"\n",
        "    if pd.isna(value):\n",
        "        return value\n",
        "\n",
        "    value_clean = str(value).strip().lower()\n",
        "\n",
        "    # [NEW] Log a warning if the ambiguous value is found, per your comment\n",
        "    if value_clean == 'suite - illegal, suite - legal':\n",
        "        logger.log_warning(f\"Found ambiguous value 'suite - illegal, suite - legal'. Classifying as 'No Suite'.\")\n",
        "\n",
        "    suite_mapping = {\n",
        "        'no': 'No Suite',\n",
        "        'suite - none': 'No Suite',\n",
        "        'suite - illegal, suite - legal': 'No Suite', # Per your logic\n",
        "        'suite - illegal': 'Suite - Illegal',\n",
        "        'yes': 'Suite - Illegal', # Common alias\n",
        "        'suite - legal': 'Suite - Legal'\n",
        "    }\n",
        "\n",
        "    # Return mapped value or default to \"No Suite\"\n",
        "    return suite_mapping.get(value_clean, 'No Suite')\n",
        "\n",
        "def standardize_suite_data(df, column='Suite'):\n",
        "    \"\"\"\n",
        "    Standardize the suite information in the DataFrame.\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Starting suite data standardization...\", also_print=True)\n",
        "\n",
        "    try:\n",
        "        df_clean = df.copy()\n",
        "        logger.log_info(\"Created working copy of DataFrame.\")\n",
        "\n",
        "        original_values = df_clean[column].copy()\n",
        "\n",
        "        logger.log_info(\"\\nOriginal Suite Distribution (detailed to log):\")\n",
        "        original_stats = analyze_suite_distribution(df_clean, column, log_to_console=False)\n",
        "\n",
        "        logger.log_info(\"Applying standardization function to column...\", also_print=True)\n",
        "        df_clean[column] = df_clean[column].apply(clean_suite)\n",
        "\n",
        "        logger.log_info(\"\\nStandardized Suite Distribution (detailed to log):\")\n",
        "        new_stats = analyze_suite_distribution(df_clean, column, log_to_console=False)\n",
        "\n",
        "        changes = (df_clean[column] != original_values).sum()\n",
        "        summary = {\n",
        "            'total_rows': len(df_clean),\n",
        "            'rows_changed': changes,\n",
        "            'percentage_changed': (changes / len(df_clean)) * 100\n",
        "        }\n",
        "\n",
        "        # --- Console Output for Summary ---\n",
        "        print(\"\\n--- Suite Standardization Summary ---\")\n",
        "        summary_table = [\n",
        "            [\"Total Rows Processed\", f\"{summary['total_rows']:,}\"],\n",
        "            [\"Values Changed\", f\"{summary['rows_changed']:,}\"],\n",
        "            [\"Percentage Changed\", f\"{summary['percentage_changed']:.2f}%\"],\n",
        "            [\"Original Rows with Data\", f\"{original_stats['rows_with_data']:,} ({original_stats['percentage_with_data']:.2f}%)\"],\n",
        "            [\"Standardized Rows with Data\", f\"{new_stats['rows_with_data']:,} ({new_stats['percentage_with_data']:.2f}%)\"]\n",
        "        ]\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(summary_table, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "\n",
        "        print(\"\\n--- Original Suite Distribution ---\")\n",
        "        original_dist_table = []\n",
        "        for value, count in original_stats['value_counts'].items():\n",
        "            value_str = 'NULL' if pd.isna(value) else value\n",
        "            percentage = (count / original_stats['total_rows']) * 100 if original_stats['total_rows'] > 0 else 0\n",
        "            original_dist_table.append([value_str, count, f\"{percentage:.2f}%\"])\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(original_dist_table, headers=[\"Value\", \"Count\", \"Percentage\"], tablefmt=\"grid\"))\n",
        "\n",
        "        print(\"\\n--- Standardized Suite Distribution ---\")\n",
        "        new_dist_table = []\n",
        "        for value, count in new_stats['value_counts'].items():\n",
        "            value_str = 'NULL' if pd.isna(value) else value\n",
        "            percentage = (count / new_stats['total_rows']) * 100 if new_stats['total_rows'] > 0 else 0\n",
        "            new_dist_table.append([value_str, count, f\"{percentage:.2f}%\"])\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(new_dist_table, headers=[\"Value\", \"Count\", \"Percentage\"], tablefmt=\"grid\"))\n",
        "\n",
        "        if changes > 0:\n",
        "            logger.log_info(\"\\nExample Changes (first 10, detailed to log):\", also_print=True)\n",
        "            changed_mask = (df_clean[column] != original_values)\n",
        "            changes_df = pd.DataFrame({\n",
        "                'Original Value': original_values[changed_mask],\n",
        "                'New Value': df_clean.loc[changed_mask, column]\n",
        "            })\n",
        "\n",
        "            change_examples_str = \"\"\n",
        "            for _, row in changes_df.head(10).iterrows():\n",
        "                orig = 'NULL' if pd.isna(row['Original Value']) else row['Original Value']\n",
        "                new = 'NULL' if pd.isna(row['New Value']) else row['New Value']\n",
        "                change_examples_str += f\"{orig:<20} -> {new}\\n\"\n",
        "            logger.log_info(change_examples_str) # Log details to file\n",
        "\n",
        "        logger.log_info(\"Suite data standardization completed successfully.\")\n",
        "        return df_clean, summary\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in standardize_suite_data: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 8: Suite Standardization ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty.\")\n",
        "\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    # [EXCELLENT PRACTICE] Backup and restore logic\n",
        "    suite_backup = combined_data['Suite'].copy()\n",
        "    logger.log_info(\"Created backup of 'Suite' column for error recovery.\")\n",
        "\n",
        "    combined_data, changes = standardize_suite_data(combined_data)\n",
        "\n",
        "    print(\"\\n‚úÖ Suite status cleaning and standardization completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, Exception) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during suite standardization:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "\n",
        "    if 'combined_data' in locals() and 'suite_backup' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        try:\n",
        "            combined_data['Suite'] = suite_backup\n",
        "            logger.log_info(\"Restored original 'Suite' values due to error.\", also_print=True)\n",
        "        except Exception as restore_e:\n",
        "            logger.log_error(f\"CRITICAL ERROR: Failed to restore original 'Suite' values: {restore_e}\", also_print=True)\n",
        "\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_seq(e)}\") # Using format_seq\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape}\")\n",
        "\n",
        "    # [FIX] Added for consistency\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 8 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "7HoGMgY-fJrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 9: BASEMENT AND SUITE FEATURE ENGINEERING\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell engineers new, standardized features from the 'Basement' column:\n",
        "- Basement_Finish: (fully finished, partially finished, unfinished)\n",
        "- Is_Walkout: (yes, no)\n",
        "- Suite_Separate_Entry: (yes, no)\n",
        "\n",
        "It also cross-references 'Basement' text with the 'Suite' column to\n",
        "find and update suite information (e.g., if 'Basement' mentions a suite\n",
        "but 'Suite' is 'No Suite').\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "import traceback # <-- [FIX] Added for exception logging\n",
        "\n",
        "def categorize_basement_features(df):\n",
        "    \"\"\"\n",
        "    Create standardized columns for basement features and cross-reference with suite data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting basement feature categorization...\", also_print=True)\n",
        "        df_work = df.copy()\n",
        "        logger.log_info(\"Created working copy of DataFrame.\", also_print=False)\n",
        "\n",
        "        # --- STEP 1: Create Basement_Finish categorization ---\n",
        "        logger.log_info(\"Categorizing basement finish status...\", also_print=True)\n",
        "\n",
        "        def determine_finish_status(value):\n",
        "            if pd.isna(value):\n",
        "                return 'unfinished' # Default\n",
        "            value = str(value).lower()\n",
        "            if any(term in value for term in ['full', 'fully finished', 'developed']):\n",
        "                return 'fully finished'\n",
        "            elif any(term in value for term in ['partial', 'part finished']):\n",
        "                return 'partially finished'\n",
        "            else:\n",
        "                return 'unfinished'\n",
        "\n",
        "        df_work['Basement_Finish'] = df_work['Basement'].apply(determine_finish_status)\n",
        "        logger.log_info(\"Finished Basement_Finish categorization.\")\n",
        "\n",
        "        # --- STEP 2: Create Is_Walkout categorization ---\n",
        "        logger.log_info(\"Categorizing walkout status...\", also_print=True)\n",
        "\n",
        "        def determine_walkout_status(value):\n",
        "            if pd.isna(value):\n",
        "                return 'no'\n",
        "            value = str(value).lower()\n",
        "            return 'yes' if any(term in value for term in ['walk-out', 'walkout']) else 'no'\n",
        "\n",
        "        df_work['Is_Walkout'] = df_work['Basement'].apply(determine_walkout_status)\n",
        "        logger.log_info(\"Finished Is_Walkout categorization.\")\n",
        "\n",
        "        # --- STEP 3: Create Suite_Separate_Entry categorization ---\n",
        "        logger.log_info(\"Categorizing separate entry status...\", also_print=True)\n",
        "\n",
        "        def determine_separate_entry(value):\n",
        "            if pd.isna(value):\n",
        "                return 'no'\n",
        "            # Look for exact match of dropdown term\n",
        "            return 'yes' if 'Separate/Exterior Entry' in str(value) else 'no'\n",
        "\n",
        "        df_work['Suite_Separate_Entry'] = df_work['Basement'].apply(determine_separate_entry)\n",
        "        logger.log_info(\"Finished Suite_Separate_Entry categorization.\")\n",
        "\n",
        "        # --- STEP 4: Cross-reference suite information (VECTORIZED) ---\n",
        "        logger.log_info(\"Cross-referencing suite information (vectorized)...\", also_print=True)\n",
        "\n",
        "        # [PERFORMANCE FIX] Replaced a slow for-loop with a fast,\n",
        "        # vectorized operation.\n",
        "\n",
        "        # 1. Find all rows where 'Basement' mentions \"suite\"\n",
        "        basement_suite_mask = df_work['Basement'].str.contains('suite',\n",
        "                                                              case=False,\n",
        "                                                              na=False)\n",
        "\n",
        "        # 2. Find rows that meet *both* conditions for an update:\n",
        "        #    - 'Basement' mentions \"suite\" (from above)\n",
        "        #    - 'Suite' column is 'No Suite' OR is null (pd.isna)\n",
        "        mask_to_update = (\n",
        "            basement_suite_mask &\n",
        "            (df_work['Suite'].isna() | (df_work['Suite'] == 'No Suite'))\n",
        "        )\n",
        "\n",
        "        # 3. Get the count of rows we are about to change\n",
        "        updates = mask_to_update.sum()\n",
        "\n",
        "        # 4. Apply the update in one single operation\n",
        "        if updates > 0:\n",
        "            df_work.loc[mask_to_update, 'Suite'] = 'Suite - Illegal'\n",
        "\n",
        "        # 5. Check for any remaining mismatches (should be 0)\n",
        "        mismatches_after_update = df_work.loc[basement_suite_mask & (df_work['Suite'] == 'No Suite')].shape[0]\n",
        "\n",
        "        logger.log_info(\"Finished cross-referencing suite information.\")\n",
        "\n",
        "        # --- STEP 5: Consolidate Report for Console & Log ---\n",
        "        print(\"\\n--- Basement Feature & Suite Analysis Summary ---\")\n",
        "\n",
        "        def print_distribution_table(title, series):\n",
        "            counts = series.value_counts(dropna=False)\n",
        "            total = len(series)\n",
        "            table_data = []\n",
        "            for status, count in counts.items():\n",
        "                status_str = 'NULL' if pd.isna(status) else status\n",
        "                percentage = (count / total) * 100\n",
        "                table_data.append([status_str, f\"{count:,}\", f\"{percentage:.2f}%\"])\n",
        "            print(f\"\\n{title}:\")\n",
        "            # [FIX] Using tabulate.tabulate()\n",
        "            print(tabulate.tabulate(table_data, headers=[\"Category\", \"Count\", \"Percentage\"], tablefmt=\"grid\"))\n",
        "            # [FIX] Using tabulate.tabulate()\n",
        "            logger.log_info(f\"\\n{title} (detailed):\\n{tabulate.tabulate(table_data, headers=['Category', 'Count', 'Percentage'], tablefmt='plain')}\")\n",
        "\n",
        "        print_distribution_table(\"Basement Finish Distribution\", df_work['Basement_Finish'])\n",
        "        print_distribution_table(\"Walkout Status Distribution\", df_work['Is_Walkout'])\n",
        "        print_distribution_table(\"Separate Entry Distribution\", df_work['Suite_Separate_Entry'])\n",
        "\n",
        "        print(\"\\nSuite Cross-Reference Results:\")\n",
        "        suite_x_ref_data = [\n",
        "            [\"Basement descriptions indicating 'suite'\", f\"{basement_suite_mask.sum():,}\"],\n",
        "            [\"'Suite' column updated based on 'Basement' text\", f\"{updates:,}\"],\n",
        "            [\"Remaining cases where 'Suite' is 'No Suite' but 'Basement' indicates 'suite'\", f\"{mismatches_after_update:,}\"]\n",
        "        ]\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(suite_x_ref_data, headers=[\"Metric\", \"Count\"], tablefmt=\"grid\"))\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        logger.log_info(f\"\\nSuite Cross-Reference Results (detailed):\\n{tabulate.tabulate(suite_x_ref_data, headers=['Metric', 'Count'], tablefmt='plain')}\")\n",
        "\n",
        "        logger.log_info(\"Basement feature categorization and suite cross-referencing completed.\", also_print=True)\n",
        "        return df_work\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"\\nPROCESS HALTED: Error in categorize_basement_features:\", also_print=True)\n",
        "        logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 9: Basement/Suite Analysis ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Basement and Suite Feature Analysis\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty. Please run previous cells.\")\n",
        "\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    # [EXCELLENT PRACTICE] Backup and restore logic\n",
        "    basement_backup = combined_data['Basement'].copy()\n",
        "    suite_backup = combined_data['Suite'].copy() if 'Suite' in combined_data.columns else None\n",
        "    logger.log_info(\"Created backup of 'Basement' and 'Suite' columns.\")\n",
        "\n",
        "    combined_data = categorize_basement_features(combined_data)\n",
        "\n",
        "    print(\"\\n‚úÖ Basement and Suite Analysis completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, Exception) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during basement and suite analysis:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame) and 'basement_backup' in locals():\n",
        "        try:\n",
        "            combined_data['Basement'] = basement_backup\n",
        "            if suite_backup is not None:\n",
        "                combined_data['Suite'] = suite_backup\n",
        "            logger.log_info(\"Restored original 'Basement' and 'Suite' values due to error.\", also_print=True)\n",
        "        except Exception as restore_e:\n",
        "            logger.log_error(f\"CRITICAL ERROR: Failed to restore 'Basement'/'Suite' values: {restore_e}\", also_print=True)\n",
        "\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape}\")\n",
        "\n",
        "    # [FIX] Added for consistency\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 9 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "6CNbzBodfzX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 10: PARKING ANALYSIS AND STANDARDIZATION\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell analyzes and standardizes the 'Parking' column.\n",
        "It uses a data-driven two-pass approach:\n",
        "1. (Vectorized) First pass: Calculates the frequency of all parking terms.\n",
        "2. (Row-wise) Second pass: Applies cleaning rules based on term\n",
        "   frequency and business logic to create standardized new columns.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "import traceback # <-- [FIX] Added for exception logging\n",
        "\n",
        "# [EXCELLENT PRACTICE] Rules are defined as a central constant.\n",
        "PARKING_RULES = {\n",
        "    'remove_terms': {\n",
        "        'oversized', 'insulated', 'indoor', 'garage door opener', 'driveway',\n",
        "        'concrete driveway', 'alley access', 'garage faces front',\n",
        "        'garage faces rear', 'see remarks', 'additional parking', 'secured',\n",
        "        'none', 'gravel driveway', 'enclosed', 'workshop in garage', 'paved'\n",
        "    },\n",
        "    'preserve_terms': {\n",
        "        'plug-in', 'single garage', 'no garage', 'quad or more attached',\n",
        "        'triple garage', 'quad or more detached', 'heated driveway',\n",
        "        'electric vehicle charging station', 'carport'\n",
        "    },\n",
        "    'term_standardization': {\n",
        "        'in garage electric vehicle charging station(s)': 'electric vehicle charging station',\n",
        "        'private electric vehicle charging station(s)': 'electric vehicle charging station',\n",
        "        'attached carport': 'carport',\n",
        "        'attached garage': 'single garage attached',\n",
        "        'detached garage': 'single garage detached',\n",
        "        'double garage': 'double garage attached'\n",
        "    }\n",
        "}\n",
        "\n",
        "def analyze_parking_none_values(df, parking_column='Parking'):\n",
        "    \"\"\"\n",
        "    Analyze rows where 'none' appears in the parking description.\n",
        "    (This function is well-written, no changes needed)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Analyzing parking descriptions containing 'none'...\")\n",
        "\n",
        "        if parking_column not in df.columns:\n",
        "            raise ValueError(f\"Column '{parking_column}' not found in DataFrame\")\n",
        "\n",
        "        # [GOOD LOGIC] .fillna('') is safe\n",
        "        none_mask = df[parking_column].fillna('').str.contains('none', case=False)\n",
        "        none_descriptions = df.loc[none_mask, parking_column]\n",
        "        unique_descriptions = none_descriptions.unique()\n",
        "        value_counts = none_descriptions.value_counts()\n",
        "\n",
        "        logger.log_info(\"=\" * 50)\n",
        "        logger.log_info(\"ANALYSIS OF 'NONE' PARKING DESCRIPTIONS (Detailed to Log)\")\n",
        "        logger.log_info(\"=\" * 50)\n",
        "        logger.log_info(f\"Total descriptions containing 'none': {len(none_descriptions):,}\")\n",
        "\n",
        "        if len(unique_descriptions) > 0:\n",
        "            logger.log_info(\"\\nUnique descriptions containing 'none':\")\n",
        "            logger.log_info(\"\\n\".join([f\"{idx}. {desc}\" for idx, desc in enumerate(unique_descriptions, 1)]))\n",
        "            logger.log_info(\"\\nFrequency of each 'none' description:\")\n",
        "            logger.log_info(value_counts.to_string())\n",
        "        else:\n",
        "            logger.log_info(\"\\nNo descriptions containing 'none' found.\")\n",
        "\n",
        "        return len(none_descriptions), unique_descriptions, value_counts\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_parking_none_values: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "def clean_parking_description(description, term_frequencies, rules, frequency_threshold):\n",
        "    \"\"\"\n",
        "    Clean and standardize a single parking description.\n",
        "    [FIX] Now accepts frequency_threshold as a parameter.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if pd.isna(description):\n",
        "            return description\n",
        "\n",
        "        description = str(description).lower().strip()\n",
        "        terms = [term.strip() for term in description.split(',')]\n",
        "\n",
        "        if len(terms) == 1 and terms[0] == 'none':\n",
        "            return 'no parking'\n",
        "\n",
        "        terms = [term for term in terms if term != 'none']\n",
        "\n",
        "        standardized_terms = [rules['term_standardization'].get(term, term) for term in terms]\n",
        "\n",
        "        cleaned_terms = []\n",
        "        for term in standardized_terms:\n",
        "            if (term in rules['preserve_terms'] or\n",
        "                (term_frequencies.get(term, 0) >= frequency_threshold and # <-- [FIX] Uses parameter\n",
        "                 term not in rules['remove_terms'])):\n",
        "                cleaned_terms.append(term)\n",
        "\n",
        "        cleaned_terms = sorted(list(set(cleaned_terms)))\n",
        "        return ', '.join(cleaned_terms) if cleaned_terms else 'no parking'\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error cleaning parking description '{description}': {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def analyze_and_standardize_parking(df, parking_column='Parking', frequency_threshold=2000):\n",
        "    \"\"\"\n",
        "    Analyze parking descriptions and create standardized versions.\n",
        "    [FIX] Uses a vectorized \"first pass\" for term frequency calculation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting parking analysis and standardization...\", also_print=True)\n",
        "\n",
        "        if parking_column not in df.columns:\n",
        "            raise ValueError(f\"Column '{parking_column}' not found in DataFrame\")\n",
        "\n",
        "        df_copy = df.copy()\n",
        "        total_rows = len(df)\n",
        "        original_nulls = df[parking_column].isnull().sum()\n",
        "        original_valid = total_rows - original_nulls\n",
        "\n",
        "        print(\"\\n--- Parking Data Overview ---\")\n",
        "        overview_data = [\n",
        "            [\"Total Properties in Dataset\", f\"{total_rows:,}\"],\n",
        "            [\"Properties with Parking Data\", f\"{original_valid:,} ({(original_valid/total_rows*100):.1f}%)\"],\n",
        "            [\"Properties with No Parking Data (Nulls)\", f\"{original_nulls:,} ({(original_nulls/total_rows*100):.1f}%)\"]\n",
        "        ]\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(overview_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nParking Data Overview (detailed):\\n{tabulate.tabulate(overview_data, headers=['Metric', 'Value'], tablefmt='plain')}\")\n",
        "\n",
        "        # [PERFORMANCE FIX] Vectorized \"first pass\" to get term frequencies.\n",
        "        logger.log_info(\"Collecting and standardizing terms (vectorized)...\", also_print=True)\n",
        "        valid_descriptions = df[parking_column].dropna()\n",
        "\n",
        "        # 1. Split all descriptions into terms and explode into a single Series\n",
        "        exploded_terms = valid_descriptions.str.lower().str.split(',').explode().str.strip()\n",
        "\n",
        "        # 2. Standardize terms using the rules\n",
        "        standardized_terms = exploded_terms.replace(PARKING_RULES['term_standardization'])\n",
        "\n",
        "        # 3. Filter out terms marked for removal\n",
        "        filtered_terms = standardized_terms[~standardized_terms.isin(PARKING_RULES['remove_terms'])]\n",
        "\n",
        "        # 4. Get the value counts and convert to a dictionary\n",
        "        term_frequencies = filtered_terms.value_counts().to_dict()\n",
        "        # --- End of vectorized section ---\n",
        "\n",
        "        logger.log_info(f\"\\nTop 10 Most Frequent Parking Terms (detailed to log):\")\n",
        "        top_terms_data = []\n",
        "        for term, freq in sorted(term_frequencies.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "            top_terms_data.append([term, f\"{freq:,}\"])\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        logger.log_info(tabulate.tabulate(top_terms_data, headers=[\"Term\", \"Occurrences\"], tablefmt=\"plain\"))\n",
        "\n",
        "        # Second pass: clean and standardize descriptions\n",
        "        logger.log_info(\"Applying full standardization rules to create new columns...\", also_print=True)\n",
        "        df_copy['standardized_parking'] = df[parking_column].apply(\n",
        "            lambda x: clean_parking_description(\n",
        "                x,\n",
        "                term_frequencies,\n",
        "                PARKING_RULES,\n",
        "                frequency_threshold  # <-- [FIX] Pass threshold to helper\n",
        "            )\n",
        "        )\n",
        "\n",
        "        df_copy['standardized_parking_complete'] = df_copy['standardized_parking'].fillna('no parking')\n",
        "\n",
        "        # Calculate and log final statistics\n",
        "        final_nulls = df_copy['standardized_parking'].isnull().sum()\n",
        "        no_parking_count = (df_copy['standardized_parking_complete'] == 'no parking').sum()\n",
        "\n",
        "        print(\"\\n--- Parking Standardization Results ---\")\n",
        "        results_data = [\n",
        "            [\"Original Null Values\", f\"{original_nulls:,}\"],\n",
        "            [\"New Nulls (uncleanable descriptions)\", f\"{final_nulls - original_nulls:,}\"],\n",
        "            [\"Total 'No Parking' Designations (incl. original nulls)\", f\"{no_parking_count:,}\"]\n",
        "        ]\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(results_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nParking Standardization Results (detailed):\\n{tabulate.tabulate(results_data, headers=['Metric', 'Value'], tablefmt='plain')}\")\n",
        "\n",
        "        print(\"\\n--- Top 10 Standardized Parking Types ---\")\n",
        "        value_counts = df_copy['standardized_parking_complete'].value_counts()\n",
        "        top_10_parking_types = []\n",
        "        for type_, count in value_counts.head(10).items():\n",
        "            percentage = (count/total_rows*100)\n",
        "            top_10_parking_types.append([type_, f\"{count:,}\", f\"{percentage:.1f}%\"])\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(top_10_parking_types, headers=[\"Parking Type\", \"Count\", \"Percentage\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nFull Standardized Parking Types Distribution:\\n{value_counts.to_string()}\")\n",
        "\n",
        "        logger.log_info(\"Parking analysis and standardization completed.\", also_print=True)\n",
        "        return df_copy\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_and_standardize_parking: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 10: Parking Standardization ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Comprehensive Parking Analysis and Standardization\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty. Please run previous cells.\")\n",
        "\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    if 'Parking' not in combined_data.columns:\n",
        "        raise ValueError(\"Required column 'Parking' not found in DataFrame. Cannot proceed.\")\n",
        "\n",
        "    # [EXCELLENT PRACTICE] Backup and restore logic\n",
        "    parking_backup = combined_data['Parking'].copy()\n",
        "    logger.log_info(\"Created backup of 'Parking' column for error recovery.\")\n",
        "\n",
        "    none_count, _, _ = analyze_parking_none_values(combined_data)\n",
        "    if none_count > 0:\n",
        "        logger.log_info(f\"Analyzed {none_count} parking descriptions containing 'none'. Details in log.\", also_print=True)\n",
        "    else:\n",
        "        logger.log_info(\"No parking descriptions containing 'none' found.\", also_print=True)\n",
        "\n",
        "    combined_data = analyze_and_standardize_parking(combined_data)\n",
        "\n",
        "    print(\"\\n‚úÖ Parking analysis and standardization completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An unexpected error occurred during parking analysis:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame) and 'parking_backup' in locals():\n",
        "        try:\n",
        "            combined_data['Parking'] = parking_backup\n",
        "            logger.log_info(\"Restored original 'Parking' values due to error.\", also_print=True)\n",
        "        except Exception as restore_e:\n",
        "            logger.log_error(f\"CRITICAL ERROR: Failed to restore 'Parking' values: {restore_e}\", also_print=True)\n",
        "\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape}\")\n",
        "\n",
        "    # [FIX] Added for consistency\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 10 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "-l1GB8aIgNbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 11: COMMISSION ANALYSIS AND STANDARDIZATION (Original, Robust Logic)\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell analyzes and standardizes the 'Commission' column using\n",
        "the original, robust, row-by-row processing logic.\n",
        "This approach is resilient to index issues.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "import traceback # <-- [FIX] Added for exception logging\n",
        "\n",
        "# =================================================================================\n",
        "# HELPER FUNCTIONS (Your original, correct functions)\n",
        "# =================================================================================\n",
        "\n",
        "def identify_common_patterns(series, min_frequency=10):\n",
        "    \"\"\"\n",
        "    Identify common commission patterns in a pandas Series.\n",
        "    Uses defaultdict to efficiently count pattern occurrences.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pattern_counts = defaultdict(int)\n",
        "        for text in series.dropna():\n",
        "            numbers = extract_commission_numbers(text)\n",
        "            if numbers:\n",
        "                pattern_counts[numbers] += 1\n",
        "\n",
        "        common_patterns = {\n",
        "            pattern: count for pattern, count in pattern_counts.items()\n",
        "            if count >= min_frequency\n",
        "        }\n",
        "\n",
        "        logger.log_info(f\"Identified {len(common_patterns)} common patterns with min_frequency={min_frequency}.\")\n",
        "        if common_patterns:\n",
        "            logger.log_info(\"Top common patterns found (detailed to log):\")\n",
        "            sorted_patterns = sorted(common_patterns.items(), key=lambda item: item[1], reverse=True)[:10]\n",
        "            for pattern, count in sorted_patterns:\n",
        "                logger.log_info(f\"  Pattern: {pattern}, Count: {count}\")\n",
        "\n",
        "        # [This was your correct logic]\n",
        "        return list(common_patterns.keys()), common_patterns\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in identify_common_patterns: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "def identify_common_commission_pattern(text):\n",
        "    \"\"\"\n",
        "    Checks for common commission patterns (3.5/1.5 or 3.0/1.5) in text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = str(text).lower()\n",
        "        commission_terms = ['%', 'percent', 'commission', 'balance', 'remainder', 'first']\n",
        "        if '3.5' in text and '1.5' in text:\n",
        "            if any(word in text for word in commission_terms):\n",
        "                return (3.5, 1.5)\n",
        "        if ('3.0' in text or ' 3 ' in text or '3%' in text) and '1.5' in text:\n",
        "            if any(word in text for word in commission_terms):\n",
        "                return (3.0, 1.5)\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in identify_common_commission_pattern: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def convert_fraction_to_decimal(text):\n",
        "    \"\"\"\n",
        "    Converts written fractions to decimal numbers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = str(text)\n",
        "        text = re.sub(r'\\s*&\\s*', ' ', text)\n",
        "        mixed_pattern = r'(\\d+)[\\s-]*(\\d+)/(\\d+)'\n",
        "\n",
        "        def convert_mixed_number(match):\n",
        "            try:\n",
        "                whole = int(match.group(1))\n",
        "                numerator = int(match.group(2))\n",
        "                denominator = int(match.group(3))\n",
        "                if denominator == 0:\n",
        "                    return match.group(0)\n",
        "                result = whole + (numerator / denominator)\n",
        "                return f\"{result}\"\n",
        "            except (ValueError, ZeroDivisionError):\n",
        "                return match.group(0)\n",
        "\n",
        "        prev_text = None\n",
        "        while prev_text != text:\n",
        "            prev_text = text\n",
        "            text = re.sub(mixed_pattern, convert_mixed_number, text)\n",
        "\n",
        "        simple_fraction_pattern = r'^(\\d+)/(\\d+)$'\n",
        "        match_simple = re.match(simple_fraction_pattern, text.strip())\n",
        "        if match_simple:\n",
        "            try:\n",
        "                numerator = int(match_simple.group(1))\n",
        "                denominator = int(match_simple.group(2))\n",
        "                if denominator == 0:\n",
        "                    return text\n",
        "                return f\"{numerator / denominator}\"\n",
        "            except (ValueError, ZeroDivisionError):\n",
        "                return text\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in convert_fraction_to_decimal: {str(e)}\")\n",
        "        return text\n",
        "\n",
        "def normalize_decimal(text):\n",
        "    \"\"\"Normalizes decimal numbers by handling different formats.\"\"\"\n",
        "    try:\n",
        "        text = str(text)\n",
        "        text = re.sub(r'(\\d+),(\\d+)(?=\\s*%?)', r'\\1.\\2', text)\n",
        "        text = re.sub(r'(\\d+)\\.\\s+(\\d+)', r'\\1.\\2', text)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in normalize_decimal: {str(e)}\")\n",
        "        return text\n",
        "\n",
        "def correct_common_errors(text):\n",
        "    \"\"\"Corrects common input errors in commission values.\"\"\"\n",
        "    try:\n",
        "        text = str(text)\n",
        "        corrections = [\n",
        "            (r'\\b1/5\\b', '1.5'), (r'\\b2/5\\b', '2.5'), (r'\\b3/5\\b', '3.5'), (r'\\b4/5\\b', '4.5'),\n",
        "            (r'\\b1/5%', '1.5%'), (r'\\b2/5%', '2.5%'), (r'\\b3/5%', '3.5%'), (r'\\b4/5%', '4.5%'),\n",
        "        ]\n",
        "        for pattern, replacement in corrections:\n",
        "            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in correct_common_errors: {str(e)}\")\n",
        "        return text\n",
        "\n",
        "def extract_commission_numbers(text):\n",
        "    \"\"\"\n",
        "    Extracts commission percentages from text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = str(text).lower()\n",
        "        common_pattern = identify_common_commission_pattern(text)\n",
        "        if common_pattern:\n",
        "            return common_pattern\n",
        "\n",
        "        text = correct_common_errors(text)\n",
        "        text = convert_fraction_to_decimal(text)\n",
        "        text = normalize_decimal(text)\n",
        "        text = re.sub(r'\\b1st\\b|\\bfirst\\b|\\b1\\b(?=\\s*\\$?\\s*100)', 'first', text)\n",
        "\n",
        "        numbers = []\n",
        "        pattern = r'(?:^|[^\\d.])(\\d+\\.?\\d*)\\s*(?:%|percent|commission|(?=\\s*(?:balance|remainder|of|on)))'\n",
        "        first_100k_pattern = r'(\\d+\\.?\\d*)\\s*(?:%|percent)\\s+on\\s+(?:first|1st)\\s*(?:\\$?\\s*100k|\\$?\\s*100,\\s*000)'\n",
        "\n",
        "        for match in re.finditer(pattern, text):\n",
        "            try:\n",
        "                num = float(match.group(1))\n",
        "                if 0 < num < 10:\n",
        "                    numbers.append(num)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        first_100k_match = re.search(first_100k_pattern, text)\n",
        "        if first_100k_match:\n",
        "            try:\n",
        "                num = float(first_100k_match.group(1))\n",
        "                if 0 < num < 10:\n",
        "                    if len(numbers) == 1 and numbers[0] == num and not re.search(r'balance|remainder', text):\n",
        "                        pass\n",
        "                    else:\n",
        "                        numbers.append(num)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        unique_numbers = sorted(list(set(numbers)))\n",
        "        return tuple(unique_numbers) if unique_numbers else None\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in extract_commission_numbers for text '{text}': {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def standardize_commission_column(df, commission_column, min_frequency=10):\n",
        "    \"\"\"\n",
        "    Main function for pattern matching standardization.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(f\"Starting commission standardization for {len(df):,} rows...\", also_print=True)\n",
        "        logger.log_info(f\"Initial unique '{commission_column}' values: {df[commission_column].nunique():,}\")\n",
        "\n",
        "        patterns, counts = identify_common_patterns(df[commission_column], min_frequency)\n",
        "        logger.log_info(f\"Identified {len(patterns)} common patterns to standardize.\")\n",
        "\n",
        "        def standardize_value(text):\n",
        "            if pd.isna(text):\n",
        "                return 'Missing'\n",
        "            text_lower = str(text).lower()\n",
        "            numbers = extract_commission_numbers(text_lower)\n",
        "\n",
        "            if numbers and numbers in counts and counts[numbers] >= min_frequency:\n",
        "                if len(numbers) == 2:\n",
        "                    if numbers == (3.0, 1.5): return \"3.0% on first $100K, 1.5% on balance\"\n",
        "                    if numbers == (3.5, 1.5): return \"3.5% on first $100K, 1.5% on balance\"\n",
        "                    return f\"{numbers[0]}% on first portion, {numbers[1]}% on balance\"\n",
        "                elif len(numbers) == 1:\n",
        "                    return f\"{numbers[0]}%\"\n",
        "                else:\n",
        "                    return f\"{', '.join(f'{n}%' for n in numbers)}\"\n",
        "            return 'Other'\n",
        "\n",
        "        df_result = df.copy()\n",
        "        logger.log_info(\"Applying standardization rules to 'Commission' column (this may take time)...\", also_print=True)\n",
        "        df_result['standardized_commission'] = df[commission_column].apply(standardize_value)\n",
        "        logger.log_info(\"Commission standardization application complete.\")\n",
        "        logger.log_info(f\"Final unique 'standardized_commission' values: {df_result['standardized_commission'].nunique():,}\")\n",
        "        logger.log_info(\"Standardized commission value counts:\\n\" + df_result['standardized_commission'].value_counts(dropna=False).to_string())\n",
        "        return df_result\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in standardize_commission_column: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "def analyze_other_category(df, original_column, standardized_column):\n",
        "    \"\"\"Analyzes entries that couldn't be standardized.\"\"\"\n",
        "    try:\n",
        "        other_entries = df[df[standardized_column] == 'Other'][original_column]\n",
        "        missing_entries = df[df[standardized_column] == 'Missing'][original_column]\n",
        "\n",
        "        logger.log_info(f\"\\nAnalyzing 'Other' category: {len(other_entries):,} entries.\", also_print=True)\n",
        "        logger.log_info(f\"Analyzing 'Missing' category (original nulls): {len(missing_entries):,} entries.\", also_print=True)\n",
        "\n",
        "        if len(other_entries) > 0:\n",
        "            sample = other_entries.sample(min(10, len(other_entries)), random_state=42)\n",
        "            logger.log_info(\"\\nRandom sample of 'Other' entries (detailed to log):\")\n",
        "            sample_str = \"\\n\".join([f\"{idx}. {entry}\" for idx, entry in enumerate(sample, 1)])\n",
        "            logger.log_info(sample_str)\n",
        "        else:\n",
        "            logger.log_info(\"No 'Other' entries found after standardization.\")\n",
        "        return {'other_count': len(other_entries), 'missing_count': len(missing_entries)}\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_other_category: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def pattern_matching_approach(df, commission_column, min_frequency=50):\n",
        "    \"\"\"\n",
        "    Pattern matching approach for commission standardization.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        logger.log_info(\"Starting commission pattern matching approach...\", also_print=True)\n",
        "        processed_data = standardize_commission_column(df, commission_column, min_frequency)\n",
        "        other_analysis_results = analyze_other_category(\n",
        "            processed_data,\n",
        "            commission_column,\n",
        "            'standardized_commission'\n",
        "        )\n",
        "        end_time = time.time()\n",
        "        logger.log_info(f\"\\nCommission processing completed in {end_time - start_time:.2f} seconds.\", also_print=True)\n",
        "        logger.log_info(\"Pattern matching approach finished.\")\n",
        "        return processed_data\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in pattern_matching_approach: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# Main Execution Block (Wrapped in standard pipeline structure)\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 11: Commission Standardization (Original Logic) ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Commission Analysis and Standardization (Original, Robust Logic)\")\n",
        "    print(\"   (Note: This cell is slow but robust. My apologies for previous failures.)\")\n",
        "\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "    else:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty.\")\n",
        "\n",
        "    if 'Commission' not in combined_data.columns:\n",
        "        raise ValueError(\"Required column 'Commission' not found in DataFrame.\")\n",
        "\n",
        "    commission_backup = combined_data['Commission'].copy()\n",
        "    logger.log_info(\"Created backup of 'Commission' column for error recovery.\")\n",
        "\n",
        "    # Run your original, working standardization\n",
        "    combined_data = pattern_matching_approach(combined_data, 'Commission')\n",
        "\n",
        "    # Your original, correct summary logic\n",
        "    total_standardized = (combined_data['standardized_commission'] != 'Other').sum() - (combined_data['standardized_commission'] == 'Missing').sum()\n",
        "    total_other = (combined_data['standardized_commission'] == 'Other').sum()\n",
        "    total_missing = (combined_data['standardized_commission'] == 'Missing').sum()\n",
        "    total_rows = len(combined_data)\n",
        "\n",
        "    print(\"\\n--- Final Commission Standardization Summary ---\")\n",
        "    final_summary_table = [\n",
        "        [\"Total Listings Processed\", f\"{total_rows:,}\"],\n",
        "        [\"Standardized Commissions\", f\"{total_standardized:,} ({(total_standardized/total_rows*100):.2f}%)\"],\n",
        "        [\"'Other' Category (unmatched patterns)\", f\"{total_other:,} ({(total_other/total_rows*100):.2f}%)\"],\n",
        "        [\"'Missing' Category (original nulls)\", f\"{total_missing:,} ({(total_missing/total_rows*100):.2f}%)\"]\n",
        "    ]\n",
        "    # [FIX] Using tabulate.tabulate()\n",
        "    print(tabulate.tabulate(final_summary_table, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "\n",
        "    print(\"\\n‚úÖ Commission analysis and standardization completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during commission analysis:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame) and 'commission_backup' in locals():\n",
        "        try:\n",
        "            combined_data['Commission'] = commission_backup\n",
        "            if 'standardized_commission' in combined_data.columns:\n",
        "                combined_data = combined_data.drop(columns=['standardized_commission'])\n",
        "            logger.log_info(\"Restored original 'Commission' values due to error.\", also_print=True)\n",
        "        except Exception as restore_e:\n",
        "            logger.log_error(f\"CRITICAL ERROR: Failed to restore 'Commission' values: {restore_e}\", also_print=True)\n",
        "\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape}\")\n",
        "\n",
        "    # [FIX] Added for consistency\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 11 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "mvToW5hNiIm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 12: NUMERIC COLUMN ANALYSIS\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell performs a read-only analysis of all numeric columns\n",
        "in the DataFrame. It calculates statistics (mean, median), identifies\n",
        "outliers using the IQR rule, and ranks columns by outlier percentage\n",
        "to help identify which columns need the most cleaning.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import traceback\n",
        "\n",
        "def analyze_numeric_columns(df, top_n_to_display=10):\n",
        "    \"\"\"\n",
        "    Performs a detailed analysis of all numeric columns in the dataset,\n",
        "    identifying potential outliers and data quality issues.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Analyzing numeric columns for potential data quality issues...\", also_print=True)\n",
        "\n",
        "        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "        if numeric_cols.empty:\n",
        "            logger.log_info(\"No numeric columns found in the DataFrame to analyze.\", also_print=True)\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        analysis_results = []\n",
        "\n",
        "        for column in numeric_cols:\n",
        "            data = df[column].dropna()\n",
        "            total_missing = df[column].isna().sum()\n",
        "\n",
        "            # [BUG FIX] If data is empty, append a full row of NaNs/zeros\n",
        "            # This prevents an error when creating the DataFrame later.\n",
        "            if data.empty:\n",
        "                logger.log_info(f\"Skipping analysis for column '{column}': No valid data.\", also_print=False)\n",
        "                analysis_results.append({\n",
        "                    'Column': column,\n",
        "                    'Total Valid Values': 0,\n",
        "                    'Missing Values': total_missing,\n",
        "                    'Mean': np.nan, 'Median': np.nan, 'Std Dev': np.nan,\n",
        "                    'Minimum': np.nan, 'Maximum': np.nan,\n",
        "                    'Outliers (IQR)': 0, 'Outlier % (IQR)': 0.0,\n",
        "                    'Skewness': np.nan,\n",
        "                    'Sample Outliers (Top 5)': []\n",
        "                })\n",
        "                continue # Skip to the next column\n",
        "\n",
        "            # Calculate basic statistics\n",
        "            mean = data.mean()\n",
        "            median = data.median()\n",
        "            std = data.std()\n",
        "\n",
        "            # Calculate quartiles and IQR\n",
        "            q1 = data.quantile(0.25)\n",
        "            q3 = data.quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "\n",
        "            # Define outlier boundaries\n",
        "            lower_bound = q1 - (1.5 * iqr)\n",
        "            upper_bound = q3 + (1.5 * iqr)\n",
        "\n",
        "            # Count outliers\n",
        "            outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "            outlier_count = len(outliers)\n",
        "            outlier_percentage = (outlier_count / len(data)) * 100 if len(data) > 0 else 0.0\n",
        "\n",
        "            min_val = data.min()\n",
        "            max_val = data.max()\n",
        "            skewness = data.skew()\n",
        "\n",
        "            # Store results\n",
        "            analysis_results.append({\n",
        "                'Column': column,\n",
        "                'Total Valid Values': len(data),\n",
        "                'Missing Values': total_missing,\n",
        "                'Mean': mean,\n",
        "                'Median': median,\n",
        "                'Std Dev': std,\n",
        "                'Minimum': min_val,\n",
        "                'Maximum': max_val,\n",
        "                'Outliers (IQR)': outlier_count,\n",
        "                'Outlier % (IQR)': outlier_percentage,\n",
        "                'Skewness': skewness,\n",
        "                # [GOOD LOGIC] Get outliers, sort them, take top 5\n",
        "                'Sample Outliers (Top 5)': sorted(outliers.nlargest(5).tolist()) if outlier_count > 0 else []\n",
        "            })\n",
        "\n",
        "        results_df = pd.DataFrame(analysis_results)\n",
        "        results_df = results_df.sort_values('Outlier % (IQR)', ascending=False)\n",
        "\n",
        "        # --- Console Summary ---\n",
        "        print(\"\\n--- Numeric Column Analysis Summary ---\")\n",
        "        print(f\"Total numeric columns analyzed: {len(numeric_cols)}\")\n",
        "\n",
        "        if not results_df.empty:\n",
        "            console_summary_data = []\n",
        "            # Format rows for the console summary table\n",
        "            for _, row in results_df.head(top_n_to_display).iterrows():\n",
        "                console_summary_data.append([\n",
        "                    row['Column'],\n",
        "                    f\"{row['Outlier % (IQR)']:.2f}%\",\n",
        "                    f\"{row['Outliers (IQR)']:,}\",\n",
        "                    f\"{row['Minimum']:.2f}\",\n",
        "                    f\"{row['Maximum']:.2f}\",\n",
        "                    f\"{row['Mean']:.2f}\",\n",
        "                    f\"{row['Median']:.2f}\",\n",
        "                    f\"{row['Missing Values']:,}\"\n",
        "                ])\n",
        "\n",
        "            print(f\"\\nColumns ranked by percentage of outliers (Top {top_n_to_display}):\")\n",
        "            # [FIX] Using tabulate.tabulate()\n",
        "            print(tabulate.tabulate(console_summary_data,\n",
        "                                  headers=[\"Column\", \"Outlier %\", \"Outliers\", \"Min\", \"Max\", \"Mean\", \"Median\", \"Missing\"],\n",
        "                                  tablefmt=\"grid\"))\n",
        "\n",
        "            logger.log_info(\"\\nSample Outliers for Top Problematic Columns (detailed to log):\")\n",
        "            for _, row in results_df.head(top_n_to_display).iterrows():\n",
        "                if row['Outliers (IQR)'] > 0:\n",
        "                    logger.log_info(f\"  {row['Column']} Sample Outliers: {row['Sample Outliers (Top 5)']}\")\n",
        "                else:\n",
        "                    logger.log_info(f\"  {row['Column']}: No outliers found.\")\n",
        "        else:\n",
        "            print(\"\\nNo detailed numeric column analysis results to display.\")\n",
        "\n",
        "        logger.log_info(\"\\nFull Numeric Column Analysis Results (detailed to log):\")\n",
        "        logger.log_info(results_df.to_string())\n",
        "        return results_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_numeric_columns: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 12: Numeric Column Analysis ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Numeric Column Analysis\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty. Please run previous cells.\")\n",
        "\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    # Run the analysis\n",
        "    numeric_analysis_results_df = analyze_numeric_columns(combined_data)\n",
        "\n",
        "    print(\"\\n‚úÖ Numeric Column Analysis completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during numeric column analysis:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    # This is a read-only analysis, so shape will not change\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape} (unchanged)\")\n",
        "\n",
        "    # [FIX] Added for consistency\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 12 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "bMxMAIwOl5XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 13: COORDINATE ANALYSIS AND VALIDATION\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell performs a read-only analysis of coordinate data ('Latitude',\n",
        "'Longitude') on a per-city basis. It identifies properties with\n",
        "missing coordinates or coordinates that fall outside the typical\n",
        "IQR range for their specific city.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import traceback\n",
        "\n",
        "def analyze_regional_coordinates(df):\n",
        "    \"\"\"\n",
        "    Analyzes coordinate data by city to identify potential errors and outliers.\n",
        "    This function is read-only and does not modify the input DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # [FIX] Create a copy immediately to prevent any side effects\n",
        "        df_copy = df.copy()\n",
        "\n",
        "        # --- 0. Input validation (operates on the copy) ---\n",
        "        required_columns = ['City', 'Latitude', 'Longitude']\n",
        "        for col in required_columns:\n",
        "            if col not in df_copy.columns:\n",
        "                raise ValueError(f\"Required column '{col}' not found in DataFrame\")\n",
        "            if col in ['Latitude', 'Longitude']:\n",
        "                if not pd.api.types.is_numeric_dtype(df_copy[col]):\n",
        "                    # Coerce on the copy, not the original\n",
        "                    df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')\n",
        "                    logger.log_info(f\"WARNING: Coerced '{col}' to numeric for analysis.\", also_print=True)\n",
        "                if df_copy[col].dropna().empty:\n",
        "                    logger.log_error(f\"CRITICAL: Column '{col}' has no valid numeric data.\", also_print=True)\n",
        "                    raise ValueError(f\"No valid numeric data in '{col}' for analysis.\")\n",
        "\n",
        "        logger.log_info(\"Starting regional coordinate analysis...\", also_print=True)\n",
        "\n",
        "        # --- 1. Analyze city distribution ---\n",
        "        city_counts = df_copy['City'].value_counts()\n",
        "        print(\"\\n--- Properties by City ---\")\n",
        "        if not city_counts.empty:\n",
        "            city_data_for_table = [[city, f\"{count:,}\"] for city, count in city_counts.items()]\n",
        "            # [FIX] Using tabulate.tabulate()\n",
        "            print(tabulate.tabulate(city_data_for_table, headers=[\"City\", \"Properties\"], tablefmt=\"grid\"))\n",
        "            logger.log_info(f\"\\nProperties by City (detailed):\\n{city_counts.to_string()}\")\n",
        "        else:\n",
        "            print(\"No cities found in the dataset.\")\n",
        "            logger.log_info(\"No cities found in the dataset.\")\n",
        "\n",
        "        # --- 2. Calculate coordinate statistics for each city ---\n",
        "        logger.log_info(\"Calculating coordinate statistics by city...\", also_print=True)\n",
        "        city_stats = df_copy.groupby('City').agg(\n",
        "            Latitude_count=('Latitude', 'count'), Latitude_mean=('Latitude', 'mean'),\n",
        "            Latitude_median=('Latitude', 'median'), Latitude_min=('Latitude', 'min'), Latitude_max=('Latitude', 'max'),\n",
        "            Longitude_count=('Longitude', 'count'), Longitude_mean=('Longitude', 'mean'),\n",
        "            Longitude_median=('Longitude', 'median'), Longitude_min=('Longitude', 'min'), Longitude_max=('Longitude', 'max')\n",
        "        ).round(4)\n",
        "        logger.log_info(\"\\nCoordinate Statistics by City (detailed to log):\")\n",
        "        logger.log_info(city_stats.to_string())\n",
        "\n",
        "        # --- 3. Calculate city-specific coordinate boundaries (IQR method) ---\n",
        "        logger.log_info(\"Calculating city-specific coordinate boundaries (IQR method)...\", also_print=True)\n",
        "        city_boundaries = {}\n",
        "\n",
        "        for city in df_copy['City'].unique():\n",
        "            city_data = df_copy[df_copy['City'] == city]\n",
        "\n",
        "            if city_data['Latitude'].dropna().empty or city_data['Longitude'].dropna().empty:\n",
        "                logger.log_info(f\"Skipping boundary calculation for {city}: Insufficient valid data.\", also_print=False)\n",
        "                continue\n",
        "\n",
        "            lat_q1, lat_q3 = city_data['Latitude'].quantile(0.25), city_data['Latitude'].quantile(0.75)\n",
        "            lat_iqr = lat_q3 - lat_q1\n",
        "            lon_q1, lon_q3 = city_data['Longitude'].quantile(0.25), city_data['Longitude'].quantile(0.75)\n",
        "            lon_iqr = lon_q3 - lon_q1\n",
        "\n",
        "            city_boundaries[city] = {\n",
        "                'lat_min': lat_q1 - (1.5 * lat_iqr),\n",
        "                'lat_max': lat_q3 + (1.5 * lat_iqr),\n",
        "                'lon_min': lon_q1 - (1.5 * lon_iqr),\n",
        "                'lon_max': lon_q3 + (1.5 * lon_iqr)\n",
        "            }\n",
        "        logger.log_info(\"City-specific coordinate boundaries calculated.\")\n",
        "\n",
        "        # --- 4. Identify outliers based on city-specific boundaries ---\n",
        "        logger.log_info(\"Identifying coordinate outliers...\", also_print=True)\n",
        "        outliers = pd.DataFrame()\n",
        "\n",
        "        for city in df_copy['City'].unique():\n",
        "            if city not in city_boundaries:\n",
        "                continue\n",
        "\n",
        "            city_data = df_copy[df_copy['City'] == city] # Use the copy\n",
        "            bounds = city_boundaries[city]\n",
        "\n",
        "            city_outliers = city_data[\n",
        "                (city_data['Latitude'].isna()) | (city_data['Longitude'].isna()) |\n",
        "                (city_data['Latitude'] < bounds['lat_min']) |\n",
        "                (city_data['Latitude'] > bounds['lat_max']) |\n",
        "                (city_data['Longitude'] < bounds['lon_min']) |\n",
        "                (city_data['Longitude'] > bounds['lon_max'])\n",
        "            ]\n",
        "            outliers = pd.concat([outliers, city_outliers])\n",
        "\n",
        "        # --- 5. Log & Console Report of Outliers ---\n",
        "        print(\"\\n--- Coordinate Outlier Report ---\")\n",
        "        logger.log_info(f\"Found {len(outliers):,} properties with coordinates outside their city's normal range or with missing coordinates.\", also_print=True)\n",
        "\n",
        "        if not outliers.empty:\n",
        "            outlier_by_city_counts = outliers['City'].value_counts()\n",
        "            print(\"\\nOutliers by City (Top 10):\")\n",
        "            outlier_data_for_table = [[city, f\"{count:,}\"] for city, count in outlier_by_city_counts.head(10).items()]\n",
        "            # [FIX] Using tabulate.tabulate()\n",
        "            print(tabulate.tabulate(outlier_data_for_table, headers=[\"City\", \"Outliers\"], tablefmt=\"grid\"))\n",
        "            logger.log_info(f\"\\nOutliers by City (detailed):\\n{outlier_by_city_counts.to_string()}\")\n",
        "\n",
        "            logger.log_info(\"\\nExtreme Coordinate Analysis of Outliers by City (detailed to log):\")\n",
        "            for city in outlier_by_city_counts.index:\n",
        "                city_outliers_data = outliers[outliers['City'] == city]\n",
        "                logger.log_info(f\"\\n{city}:\")\n",
        "                logger.log_info(f\"  Latitude range (outliers):  {city_outliers_data['Latitude'].min():.4f} to {city_outliers_data['Latitude'].max():.4f}\")\n",
        "                logger.log_info(f\"  Longitude range (outliers): {city_outliers_data['Longitude'].min():.4f} to {city_outliers_data['Longitude'].max():.4f}\")\n",
        "        else:\n",
        "            print(\"No coordinate outliers found based on city-specific IQR.\")\n",
        "            logger.log_info(\"No coordinate outliers found based on city-specific IQR.\")\n",
        "\n",
        "        return outliers, city_boundaries\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"PROCESS HALTED: Error in analyze_regional_coordinates: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 13: Coordinate Analysis ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Coordinate Analysis and Validation\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty. Please run previous cells.\")\n",
        "\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    required_initial_cols = ['City', 'Latitude', 'Longitude']\n",
        "    if not all(col in combined_data.columns for col in required_initial_cols):\n",
        "        raise ValueError(f\"Missing one or more required columns: {required_initial_cols}\")\n",
        "\n",
        "    # [GOOD PRACTICE] Backup logic is still fine to keep as a safety net\n",
        "    coordinate_backup = combined_data[['Latitude', 'Longitude', 'City']].copy()\n",
        "    logger.log_info(\"Created backup of coordinate data.\", also_print=False)\n",
        "\n",
        "    print(\"\\n--- Initial Coordinate Data Check ---\")\n",
        "    missing_lat = combined_data['Latitude'].isna().sum()\n",
        "    missing_lon = combined_data['Longitude'].isna().sum()\n",
        "    print(f\"Latitude dtype: {combined_data['Latitude'].dtype}\")\n",
        "    print(f\"Longitude dtype: {combined_data['Longitude'].dtype}\")\n",
        "    print(f\"Missing Latitude values: {missing_lat:,}\")\n",
        "    print(f\"Missing Longitude values: {missing_lon:,}\")\n",
        "\n",
        "    logger.log_info(f\"Initial Coordinate Data Check Summary:\\n\"\n",
        "                    f\"  Latitude dtype: {combined_data['Latitude'].dtype}\\n\"\n",
        "                    f\"  Longitude dtype: {combined_data['Longitude'].dtype}\\n\"\n",
        "                    f\"  Missing Latitude values: {missing_lat:,}\\n\"\n",
        "                    f\"  Missing Longitude values: {missing_lon:,}\")\n",
        "\n",
        "    # Run the coordinate analysis\n",
        "    problem_properties, city_boundaries = analyze_regional_coordinates(combined_data)\n",
        "\n",
        "    print(\"\\n‚úÖ Coordinate analysis completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during coordinate analysis:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame) and 'coordinate_backup' in locals():\n",
        "        try:\n",
        "            for col_to_restore in ['Latitude', 'Longitude', 'City']:\n",
        "                if col_to_restore in coordinate_backup.columns:\n",
        "                     combined_data[col_to_restore] = coordinate_backup[col_to_restore]\n",
        "            logger.log_info(\"Restored original coordinate and city values due to error.\", also_print=True)\n",
        "        except Exception as restore_e:\n",
        "            logger.log_error(f\"CRITICAL ERROR: Failed to restore original coordinate values: {restore_e}\", also_print=True)\n",
        "\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape} (unchanged)\")\n",
        "\n",
        "    # [FIX] Added for consistency\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 13 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "7CgZYGkAmKaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 14: COORDINATE CLEANING AND CORRECTION\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell cleans and corrects coordinate data. It uses the\n",
        "'city_boundaries' from the previous cell to identify outliers (including\n",
        "missing values) and then attempts to impute correct coordinates\n",
        "using a tiered-fallback logic (Postal Code median, then\n",
        "Subdivision median) based on known-good data.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "from tqdm.notebook import tqdm\n",
        "import traceback\n",
        "\n",
        "def clean_regional_coordinates(df, city_boundaries):\n",
        "    \"\"\"\n",
        "    Cleans latitude and longitude data based on city-specific boundaries and patterns.\n",
        "    (This function is now 100% free of side-effects)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # [THE FIX] Copy first to prevent any side effects on the original DataFrame\n",
        "        df_clean = df.copy()\n",
        "        logger.log_info(\"Created working copy of DataFrame.\", also_print=False)\n",
        "\n",
        "        # --- 0. Input validation (operates on the copy) ---\n",
        "        required_columns = ['City', 'Latitude', 'Longitude', 'Postal_Code', 'Subdivision_Name', 'MLS_Num']\n",
        "        for col in required_columns:\n",
        "            if col not in df_clean.columns:\n",
        "                raise ValueError(f\"Required column '{col}' not found in DataFrame\")\n",
        "\n",
        "        for coord_col in ['Latitude', 'Longitude']:\n",
        "            if not pd.api.types.is_numeric_dtype(df_clean[coord_col]):\n",
        "                df_clean[coord_col] = pd.to_numeric(df_clean[coord_col], errors='coerce')\n",
        "                logger.log_info(f\"WARNING: Coerced '{coord_col}' to numeric type.\", also_print=True)\n",
        "\n",
        "        logger.log_info(\"Starting coordinate cleaning process...\", also_print=True)\n",
        "\n",
        "        fixes = {\n",
        "            'postal_code': 0,\n",
        "            'subdivision': 0,\n",
        "            'unfixed': 0,\n",
        "            'total_outliers_identified': 0\n",
        "        }\n",
        "\n",
        "        unique_cities = df_clean['City'].unique()\n",
        "        for city in unique_cities:\n",
        "            logger.log_info(f\"\\nProcessing anomalies in {city}...\", also_print=True)\n",
        "\n",
        "            bounds = city_boundaries.get(city)\n",
        "            if not bounds:\n",
        "                logger.log_error(f\"No boundary data for {city}, skipping cleaning.\", also_print=True)\n",
        "                continue\n",
        "\n",
        "            city_mask = df_clean['City'] == city\n",
        "            outliers_in_city_mask = (\n",
        "                (df_clean['Latitude'] < bounds['lat_min']) |\n",
        "                (df_clean['Latitude'] > bounds['lat_max']) |\n",
        "                (df_clean['Longitude'] < bounds['lon_min']) |\n",
        "                (df_clean['Longitude'] > bounds['lon_max']) |\n",
        "                df_clean['Latitude'].isna() |\n",
        "                df_clean['Longitude'].isna()\n",
        "            ) & city_mask\n",
        "\n",
        "            city_outlier_count = outliers_in_city_mask.sum()\n",
        "            fixes['total_outliers_identified'] += city_outlier_count\n",
        "            logger.log_info(f\"Identified {city_outlier_count:,} invalid coordinates in {city}.\", also_print=True)\n",
        "\n",
        "            if city_outlier_count > 0:\n",
        "                # [EXCELLENT LOGIC] Calculate medians from *only* valid, non-outlier data\n",
        "                valid_coords_in_city = df_clean[city_mask & ~outliers_in_city_mask]\n",
        "\n",
        "                postal_centers = valid_coords_in_city.groupby('Postal_Code').agg(\n",
        "                    Latitude=('Latitude', 'median'),\n",
        "                    Longitude=('Longitude', 'median'),\n",
        "                    property_count=('MLS_Num', 'count')\n",
        "                ).dropna(subset=['Latitude', 'Longitude'])\n",
        "\n",
        "                subdivision_centers = valid_coords_in_city.groupby('Subdivision_Name').agg(\n",
        "                    Latitude=('Latitude', 'median'),\n",
        "                    Longitude=('Longitude', 'median'),\n",
        "                    property_count=('MLS_Num', 'count')\n",
        "                ).dropna(subset=['Latitude', 'Longitude'])\n",
        "\n",
        "                # [GOOD IMPLEMENTATION] Loop *only* over outliers\n",
        "                outlier_indices = df_clean[outliers_in_city_mask].index\n",
        "                for idx in tqdm(outlier_indices, desc=f\"Correcting {city} outliers\", total=city_outlier_count, leave=False):\n",
        "                    postal_code = df_clean.loc[idx, 'Postal_Code']\n",
        "                    subdivision = df_clean.loc[idx, 'Subdivision_Name']\n",
        "                    fixed_this_record = False\n",
        "\n",
        "                    # Try fixing by postal code\n",
        "                    if (pd.notna(postal_code) and postal_code in postal_centers.index and\n",
        "                        postal_centers.loc[postal_code, 'property_count'] >= 5):\n",
        "\n",
        "                        df_clean.loc[idx, 'Latitude'] = postal_centers.loc[postal_code, 'Latitude']\n",
        "                        df_clean.loc[idx, 'Longitude'] = postal_centers.loc[postal_code, 'Longitude']\n",
        "                        fixes['postal_code'] += 1\n",
        "                        fixed_this_record = True\n",
        "\n",
        "                    # Else, try fixing by subdivision\n",
        "                    elif (pd.notna(subdivision) and subdivision in subdivision_centers.index and\n",
        "                          subdivision_centers.loc[subdivision, 'property_count'] >= 5):\n",
        "\n",
        "                        df_clean.loc[idx, 'Latitude'] = subdivision_centers.loc[subdivision, 'Latitude']\n",
        "                        df_clean.loc[idx, 'Longitude'] = subdivision_centers.loc[subdivision, 'Longitude']\n",
        "                        fixes['subdivision'] += 1\n",
        "                        fixed_this_record = True\n",
        "\n",
        "                    if not fixed_this_record:\n",
        "                        fixes['unfixed'] += 1\n",
        "\n",
        "        # --- Console & Log Comprehensive Summary ---\n",
        "        print(\"\\n--- Coordinate Cleaning Summary ---\")\n",
        "        summary_data = [\n",
        "            [\"Total Outliers Identified (Initial)\", f\"{fixes['total_outliers_identified']:,}\"],\n",
        "            [\"Fixed using Postal Code Centers\", f\"{fixes['postal_code']:,}\"],\n",
        "            [\"Fixed using Subdivision Centers\", f\"{fixes['subdivision']:,}\"],\n",
        "            [\"Unable to Fix\", f\"{fixes['unfixed']:,}\"]\n",
        "        ]\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(summary_data, headers=[\"Metric\", \"Count\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nCoordinate Cleaning Summary (detailed):\\n{tabulate.tabulate(summary_data, headers=['Metric', 'Count'], tablefmt='plain')}\")\n",
        "\n",
        "        # --- Verification by City ---\n",
        "        print(\"\\n--- Verification: Remaining Invalid Coordinates by City ---\")\n",
        "        verification_data = []\n",
        "        for city in unique_cities:\n",
        "            bounds = city_boundaries.get(city)\n",
        "            if not bounds: continue\n",
        "\n",
        "            city_mask = df_clean['City'] == city\n",
        "            remaining_outliers = (\n",
        "                (df_clean['Latitude'] < bounds['lat_min']) |\n",
        "                (df_clean['Latitude'] > bounds['lat_max']) |\n",
        "                (df_clean['Longitude'] < bounds['lon_min']) |\n",
        "                (df_clean['Longitude'] > bounds['lon_max']) |\n",
        "                df_clean['Latitude'].isna() |\n",
        "                df_clean['Longitude'].isna()\n",
        "            ) & city_mask\n",
        "            remaining_count = remaining_outliers.sum()\n",
        "            if remaining_count > 0:\n",
        "                 verification_data.append([city, f\"{remaining_count:,}\"])\n",
        "\n",
        "        if verification_data:\n",
        "            verification_data.sort(key=lambda x: int(x[1].replace(',', '')), reverse=True)\n",
        "            # [FIX] Using tabulate.tabulate()\n",
        "            print(tabulate.tabulate(verification_data, headers=[\"City\", \"Remaining Invalid\"], tablefmt=\"grid\"))\n",
        "            logger.log_info(f\"\\nVerification: Remaining Invalid Coordinates (detailed):\\n{tabulate.tabulate(verification_data, headers=['City', 'Remaining Invalid'], tablefmt='plain')}\")\n",
        "        else:\n",
        "            print(\"No remaining invalid coordinates found after cleaning.\")\n",
        "            logger.log_info(\"No remaining invalid coordinates found after cleaning.\")\n",
        "\n",
        "        logger.log_info(\"Coordinate cleaning process completed.\", also_print=True)\n",
        "        return df_clean, fixes\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"PROCESS HALTED: Error in clean_regional_coordinates: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 14: Coordinate Cleaning ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Coordinate Cleaning and Correction\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty. Please run previous cells.\")\n",
        "\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    required_cols_for_cleaning = ['City', 'Latitude', 'Longitude', 'Postal_Code', 'Subdivision_Name', 'MLS_Num']\n",
        "    if not all(col in combined_data.columns for col in required_cols_for_cleaning):\n",
        "        raise ValueError(f\"Missing one or more required columns: {required_cols_for_cleaning}\")\n",
        "\n",
        "    # [GOOD PRACTICE] Backup only the columns this cell will modify\n",
        "    coordinate_backup_cols = ['Latitude', 'Longitude']\n",
        "    coordinate_backup = combined_data[coordinate_backup_cols].copy()\n",
        "    logger.log_info(\"Created backup of 'Latitude' and 'Longitude' columns.\")\n",
        "\n",
        "    if 'city_boundaries' not in locals() or not isinstance(city_boundaries, dict) or not city_boundaries:\n",
        "        raise NameError(\"Variable 'city_boundaries' not found or is empty. Run Cell 13 first.\")\n",
        "\n",
        "    # Run the cleaning process\n",
        "    cleaned_data, fix_summary = clean_regional_coordinates(combined_data, city_boundaries)\n",
        "\n",
        "    # Update main DataFrame\n",
        "    combined_data = cleaned_data\n",
        "\n",
        "    # Final verification summary\n",
        "    print(\"\\n--- Final Coordinate Cleaning Check ---\")\n",
        "    null_lat_after = combined_data['Latitude'].isna().sum()\n",
        "    null_lon_after = combined_data['Longitude'].isna().sum()\n",
        "    print(f\"Remaining null Latitude: {null_lat_after:,}\")\n",
        "    print(f\"Remaining null Longitude: {null_lon_after:,}\")\n",
        "    logger.log_info(f\"Final Check: Remaining null Latitude: {null_lat_after:,}, Longitude: {null_lon_after:,}\")\n",
        "\n",
        "    print(\"\\n‚úÖ Coordinate cleaning completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during coordinate cleaning:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame) and 'coordinate_backup' in locals():\n",
        "        try:\n",
        "            for col_to_restore in coordinate_backup_cols:\n",
        "                if col_to_restore in coordinate_backup.columns:\n",
        "                     combined_data[col_to_restore] = coordinate_backup[col_to_restore]\n",
        "            logger.log_info(\"Restored original 'Latitude' and 'Longitude' values due to error.\", also_print=True)\n",
        "        except Exception as restore_e:\n",
        "            logger.log_error(f\"CRITICAL ERROR: Failed to restore coordinate values: {restore_e}\", also_print=True)\n",
        "\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape}\")\n",
        "\n",
        "    # [FIX] Added for consistency\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 14 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "YFuaTrEkm3RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 15: YEAR BUILT ANALYSIS AND CLEANING\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell analyzes, cleans, and imputes the 'Year_Built' column.\n",
        "It uses a tiered-fallback logic (neighborhood median, then overall\n",
        "median) to fill in missing or invalid values. Finally, it converts\n",
        "the column to a nullable integer ('Int64') for clean export.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import traceback\n",
        "\n",
        "def analyze_year_built_data(df):\n",
        "    \"\"\"\n",
        "    Analyzes Year_Built data to identify potential issues and patterns.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting Year Built analysis...\", also_print=True)\n",
        "\n",
        "        if 'Year_Built' not in df.columns:\n",
        "            raise ValueError(\"Required column 'Year_Built' not found.\")\n",
        "\n",
        "        year_data = pd.to_numeric(df['Year_Built'], errors='coerce')\n",
        "\n",
        "        print(\"\\n--- Year Built Basic Statistics ---\")\n",
        "        basic_stats_data = [\n",
        "            [\"Total properties\", f\"{len(df):,}\"],\n",
        "            [\"Missing values\", f\"{year_data.isna().sum():,}\"]\n",
        "        ]\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(basic_stats_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nYear Built Basic Statistics (detailed):\\n{tabulate.tabulate(basic_stats_data, headers=['Metric', 'Value'], tablefmt='plain')}\")\n",
        "\n",
        "        if not year_data.dropna().empty:\n",
        "            min_year = year_data.min()\n",
        "            max_year = year_data.max()\n",
        "            print(\"\\n--- Year Built Value Ranges ---\")\n",
        "            range_data = [\n",
        "                [\"Minimum year\", f\"{min_year:.0f}\"],\n",
        "                [\"Maximum year\", f\"{max_year:.0f}\"]\n",
        "            ]\n",
        "            # [FIX] Using tabulate.tabulate()\n",
        "            print(tabulate.tabulate(range_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "            logger.log_info(f\"\\nYear Built Value Ranges (detailed):\\n{tabulate.tabulate(range_data, headers=['Metric', 'Value'], tablefmt='plain')}\")\n",
        "        else:\n",
        "            logger.log_info(\"No valid year data to report ranges.\", also_print=True)\n",
        "\n",
        "        century_bins = [-np.inf, 1800, 1900, 2000, np.inf]\n",
        "        century_labels = ['Before 1800', '1800s', '1900s', '2000s']\n",
        "        century_dist = pd.cut(year_data, bins=century_bins, labels=century_labels, right=False)\n",
        "        century_counts = century_dist.value_counts(dropna=False).sort_index()\n",
        "\n",
        "        print(\"\\n--- Year Built Distribution by Century ---\")\n",
        "        century_table_data = []\n",
        "        for century, count in century_counts.items():\n",
        "            century_str = 'Uncategorized (NaN)' if pd.isna(century) else str(century)\n",
        "            percentage = (count / len(df)) * 100\n",
        "            century_table_data.append([century_str, f\"{count:,}\", f\"{percentage:.2f}\"])\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(century_table_data, headers=[\"Century\", \"Properties\", \"Percentage\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nYear Built Distribution (detailed):\\n{tabulate.tabulate(century_table_data, headers=['Century', 'Properties', 'Percentage'], tablefmt='plain')}\")\n",
        "\n",
        "        current_year = datetime.now().year\n",
        "        suspicious = year_data[(year_data < 1880) | (year_data > current_year + 1)].dropna()\n",
        "        if len(suspicious) > 0:\n",
        "            print(f\"\\n--- Suspicious Year Built Entries ---\")\n",
        "            print(f\"Found {len(suspicious):,} properties with suspicious 'Year_Built' values. See log for details.\")\n",
        "            logger.log_info(\"\\nSuspicious Years Found (detailed to log):\")\n",
        "            suspicious_counts = suspicious.value_counts().sort_index()\n",
        "            for year, count in suspicious_counts.items():\n",
        "                logger.log_info(f\"Year {year:.0f}: {count:,} properties\")\n",
        "        else:\n",
        "            logger.log_info(\"No suspicious 'Year_Built' entries found.\", also_print=True)\n",
        "\n",
        "        return year_data\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_year_built_data: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "def clean_year_built(df):\n",
        "    \"\"\"\n",
        "    Cleans Year_Built data using vectorized neighborhood/overall medians.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting Year Built cleaning process...\", also_print=True)\n",
        "        df_clean = df.copy()\n",
        "\n",
        "        if 'Year_Built' not in df_clean.columns:\n",
        "            raise ValueError(\"Required column 'Year_Built' not found.\")\n",
        "\n",
        "        df_clean['Year_Built'] = pd.to_numeric(df_clean['Year_Built'], errors='coerce')\n",
        "\n",
        "        earliest_valid_year = 1880\n",
        "        current_year = datetime.now().year\n",
        "        latest_valid_year = current_year + 1\n",
        "        logger.log_info(f\"Setting valid year range: {earliest_valid_year} to {latest_valid_year}.\", also_print=True)\n",
        "\n",
        "        def fix_two_digit_year(year):\n",
        "            \"\"\"Helper to fix 2-digit years (0-99).\"\"\"\n",
        "            # [LOGIC FIX] Only apply to numbers < 100\n",
        "            if pd.isna(year) or year >= 100:\n",
        "                return year\n",
        "            if year <= (latest_valid_year - 2000): # e.g., 25 -> 2025\n",
        "                return year + 2000\n",
        "            if year < 100: # e.g., 99 -> 1999\n",
        "                return year + 1900\n",
        "            return year\n",
        "\n",
        "        logger.log_info(\"Attempting to fix two-digit year formats...\", also_print=True)\n",
        "        original_year_built = df_clean['Year_Built'].copy()\n",
        "        df_clean['Year_Built'] = df_clean['Year_Built'].apply(fix_two_digit_year)\n",
        "        two_digit_fixes_count = (original_year_built != df_clean['Year_Built']).sum()\n",
        "        logger.log_info(f\"Fixed {two_digit_fixes_count:,} two-digit year entries.\", also_print=True)\n",
        "\n",
        "        # Identify all invalid or missing values *after* the 2-digit fix\n",
        "        invalid_or_nan_mask = (\n",
        "            (df_clean['Year_Built'] < earliest_valid_year) |\n",
        "            (df_clean['Year_Built'] > latest_valid_year) |\n",
        "            df_clean['Year_Built'].isna()\n",
        "        )\n",
        "        invalid_count = invalid_or_nan_mask.sum()\n",
        "        fixes = {'neighborhood': 0, 'overall': 0}\n",
        "\n",
        "        if invalid_count > 0:\n",
        "            logger.log_info(f\"Found {invalid_count:,} invalid or missing values to impute.\", also_print=True)\n",
        "\n",
        "            # --- [PERFORMANCE FIX] Vectorized Imputation ---\n",
        "            valid_years_mask = ~invalid_or_nan_mask\n",
        "\n",
        "            # 1. Calculate overall median (fallback)\n",
        "            overall_median_year = df_clean.loc[valid_years_mask, 'Year_Built'].median()\n",
        "\n",
        "            if pd.isna(overall_median_year):\n",
        "                logger.log_error(\"CRITICAL: Overall median for 'Year_Built' is NaN. Cannot impute.\", also_print=True)\n",
        "                fixes['unfixed'] = invalid_count\n",
        "            else:\n",
        "                logger.log_info(\"Attempting to fix using neighborhood and overall medians...\", also_print=True)\n",
        "\n",
        "                # 2. Calculate neighborhood medians\n",
        "                neighborhood_medians = df_clean.loc[valid_years_mask].groupby('Subdivision_Name')['Year_Built'].median()\n",
        "\n",
        "                # 3. Map neighborhood medians to the *entire* 'Year_Built' column\n",
        "                imputed_by_neighborhood = df_clean['Subdivision_Name'].map(neighborhood_medians)\n",
        "\n",
        "                # 4. Use np.where for tiered fallback:\n",
        "                #    - Keep original value if valid\n",
        "                #    - Else, use neighborhood value if available\n",
        "                #    - Else, use overall median\n",
        "                df_clean['Year_Built'] = np.where(\n",
        "                    ~invalid_or_nan_mask,  # Condition 1: Keep if valid\n",
        "                    df_clean['Year_Built'],\n",
        "                    np.where(\n",
        "                        imputed_by_neighborhood.notna(), # Condition 2: Use neighborhood if available\n",
        "                        imputed_by_neighborhood,\n",
        "                        overall_median_year # Condition 3: Fallback to overall\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                # --- [PARQUET/STREAMLIT FIX] ---\n",
        "                # Convert to nullable integer type for clean export\n",
        "                logger.log_info(\"Converting 'Year_Built' to nullable integer (Int64)...\", also_print=True)\n",
        "                df_clean['Year_Built'] = df_clean['Year_Built'].astype('Int64')\n",
        "\n",
        "                # --- Calculate summary (for logging) ---\n",
        "                original_invalid = original_year_built[invalid_or_nan_mask]\n",
        "                final_fixed = df_clean['Year_Built'][invalid_or_nan_mask]\n",
        "\n",
        "                fixes['neighborhood'] = (final_fixed == imputed_by_neighborhood[invalid_or_nan_mask]).sum()\n",
        "                fixes['overall'] = (final_fixed == overall_median_year).sum()\n",
        "\n",
        "        else:\n",
        "            logger.log_info(\"No invalid or missing 'Year_Built' values found to impute.\", also_print=True)\n",
        "            # Still convert to Int64 for consistency\n",
        "            df_clean['Year_Built'] = df_clean['Year_Built'].astype('Int64')\n",
        "\n",
        "        # --- Console & Log Summary ---\n",
        "        print(\"\\n--- Year Built Cleaning Summary ---\")\n",
        "        summary_data = [\n",
        "            [\"Total properties processed\", f\"{len(df_clean):,}\"],\n",
        "            [\"Invalid/Missing values identified\", f\"{invalid_count:,}\"],\n",
        "            [\"Fixed using two-digit heuristic\", f\"{two_digit_fixes_count:,}\"],\n",
        "            [\"Imputed using neighborhood median\", f\"{fixes['neighborhood']:,}\"],\n",
        "            [\"Imputed using overall median\", f\"{fixes['overall']:,}\"]\n",
        "        ]\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(summary_data, headers=[\"Metric\", \"Count\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nYear Built Cleaning Summary (detailed):\\n{tabulate.tabulate(summary_data, headers=['Metric', 'Count'], tablefmt='plain')}\")\n",
        "\n",
        "        logger.log_info(\"Year Built cleaning process completed.\", also_print=True)\n",
        "        return df_clean\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in clean_year_built: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 15: Year Built Analysis/Cleaning ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Year Built Analysis and Cleaning\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty.\")\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    if 'Year_Built' not in combined_data.columns:\n",
        "        raise ValueError(\"Required column 'Year_Built' not found.\")\n",
        "\n",
        "    year_built_backup = combined_data['Year_Built'].copy()\n",
        "    logger.log_info(\"Created backup of 'Year_Built' column.\")\n",
        "\n",
        "    print(\"\\n--- Analyzing initial state of Year Built data ---\")\n",
        "    analyze_year_built_data(combined_data)\n",
        "\n",
        "    print(\"\\n--- Cleaning Year Built data ---\")\n",
        "    combined_data = clean_year_built(combined_data)\n",
        "\n",
        "    print(\"\\n--- Analyzing final state of Year Built data ---\")\n",
        "    analyze_year_built_data(combined_data)\n",
        "\n",
        "    print(\"\\n‚úÖ Year Built analysis and cleaning completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during Year Built analysis:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "\n",
        "    # [CRITICAL BUG FIX] Fixed the try/except logic for restoration\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame) and 'year_built_backup' in locals():\n",
        "        try:\n",
        "            combined_data['Year_Built'] = year_built_backup\n",
        "            logger.log_info(\"Restored original 'Year_Built' values due to error.\", also_print=True)\n",
        "        except Exception as restore_e:\n",
        "            logger.log_error(f\"CRITICAL ERROR: Failed to restore 'Year_Built' values: {restore_e}\", also_print=True)\n",
        "\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape}\")\n",
        "\n",
        "    # [FIX] Added for consistency\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 15 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "fi414nUpnBhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 16: STYLE DATA ANALYSIS\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell performs a read-only analysis of the 'Style' column.\n",
        "It calculates the distribution of property styles, shows the top N\n",
        "most common, and optionally logs potential variations\n",
        "(e.g., \"Bungalow\" vs. \"Bungalow; Other\") to the log file.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "import traceback\n",
        "\n",
        "def analyze_style_distribution(df, top_n=10, log_variations=False):\n",
        "    \"\"\"\n",
        "    Analyzes the distribution of property styles with controlled output.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting style distribution analysis...\", also_print=True)\n",
        "\n",
        "        if 'Style' not in df.columns:\n",
        "            raise ValueError(\"Required column 'Style' not found in DataFrame\")\n",
        "\n",
        "        style_counts = df['Style'].value_counts(dropna=False)\n",
        "        total_properties = len(df)\n",
        "        missing_count = df['Style'].isna().sum()\n",
        "\n",
        "        # --- Console Output: Style Data Summary ---\n",
        "        print(\"\\n--- Style Data Summary ---\")\n",
        "        summary_data = [\n",
        "            [\"Total Unique Styles (including NaN)\", f\"{style_counts.nunique():,}\"],\n",
        "            [\"Total Properties\", f\"{total_properties:,}\"],\n",
        "            [\"Properties Missing Style\", f\"{missing_count:,} ({ (missing_count / total_properties * 100):.2f}%)\"]\n",
        "        ]\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(summary_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nStyle Data Summary (detailed):\\n{tabulate.tabulate(summary_data, headers=['Metric', 'Value'], tablefmt='plain')}\")\n",
        "\n",
        "\n",
        "        # --- Console Output: Top N Styles ---\n",
        "        print(f\"\\n--- Top {top_n} Styles ---\")\n",
        "        top_styles_data = []\n",
        "        for style, count in style_counts.head(top_n).items():\n",
        "            percentage = (count / total_properties) * 100\n",
        "            style_str = 'NULL' if pd.isna(style) else str(style)\n",
        "            top_styles_data.append([style_str, f\"{count:,}\", f\"{percentage:.1f}%\"])\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(top_styles_data, headers=[\"Style\", \"Count\", \"Percentage\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nFull Style Distribution (detailed):\\n{style_counts.to_string()}\")\n",
        "\n",
        "\n",
        "        # --- Optional: Analyze Style Variations (Logged to File) ---\n",
        "        style_groups = {}\n",
        "        if log_variations:\n",
        "            logger.log_info(\"\\nAnalyzing potential style variations (detailed to log)...\", also_print=True)\n",
        "\n",
        "            # [GOOD LOGIC] This O(n^2) loop is fine for a few hundred unique styles\n",
        "            style_index_dropna = style_counts.index.dropna()\n",
        "            for style in style_index_dropna:\n",
        "                style_lower = str(style).lower()\n",
        "                similar_styles = [\n",
        "                    other for other in style_index_dropna\n",
        "                    if (style != other and\n",
        "                        (style_lower in str(other).lower() or\n",
        "                         str(other).lower() in style_lower))\n",
        "                ]\n",
        "                if similar_styles:\n",
        "                    style_groups[style] = similar_styles\n",
        "\n",
        "            if style_groups:\n",
        "                print(\"\\nPotential style name variations found. See log for details.\", end='\\n\\n')\n",
        "                logger.log_info(\"\\nPotential Style Variations (detailed to log):\")\n",
        "                for main_style, similar_styles in style_groups.items():\n",
        "                    logger.log_info(f\"  '{main_style}': {len(similar_styles)} variations -> {', '.join(similar_styles)}\")\n",
        "            else:\n",
        "                logger.log_info(\"No significant style name variations found.\", also_print=True)\n",
        "\n",
        "        logger.log_info(\"Style distribution analysis completed.\", also_print=True)\n",
        "        return style_counts, style_groups\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_style_distribution: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 16: Style Data Analysis ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Property Style Analysis\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty. Please run previous cells.\")\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    if 'Style' not in combined_data.columns:\n",
        "        raise ValueError(\"Required column 'Style' not found in DataFrame. Cannot proceed.\")\n",
        "\n",
        "    style_counts_result, style_variations_result = analyze_style_distribution(\n",
        "        combined_data,\n",
        "        top_n=10,\n",
        "        log_variations=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚úÖ Property style analysis completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during style analysis:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")"
      ],
      "metadata": {
        "id": "O2dMKUgyni-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 17: PROPERTY STYLE STANDARDIZATION (Corrected)\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell standardizes the 'Style' column by mapping messy,\n",
        "free-text variations to a clean, predefined list of categories.\n",
        "It uses a two-pass logic (direct match, then substring match) and\n",
        "converts the final column to 'category' dtype for high efficiency.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # <-- [THE FIX] Import the module, not the function\n",
        "import traceback\n",
        "from collections import defaultdict\n",
        "\n",
        "def create_style_mapping():\n",
        "    \"\"\"\n",
        "    Creates a comprehensive mapping of style variations to standardized categories.\n",
        "    (This is your excellent, unchanged business logic)\n",
        "    \"\"\"\n",
        "    return {\n",
        "        # Bungalow variations\n",
        "        'BUNGALOW': 'Bungalow', 'MODIFIED BUNGALOW': 'Bungalow', 'RAISED BUNGALOW': 'Bungalow',\n",
        "        'RANCH': 'Bungalow',\n",
        "\n",
        "        # 2 Storey variations\n",
        "        '2 STOREY': '2 Storey', 'TWO STOREY': '2 Storey', '2-STOREY': '2 Storey', '2 1/2 STOREY': '2 Storey',\n",
        "\n",
        "        # Single Level Unit variations\n",
        "        'SINGLE LEVEL': 'Single Level Unit', 'ONE STOREY': 'Single Level Unit', 'MAIN FLOOR': 'Single Level Unit',\n",
        "\n",
        "        # Bi-level variations\n",
        "        'BI-LEVEL': 'Bi-level', 'BILEVEL': 'Bi-level',\n",
        "\n",
        "        # 4 Level Split variations\n",
        "        '4 LEVEL SPLIT': '4 Level Split', 'FOUR LEVEL SPLIT': '4 Level Split',\n",
        "\n",
        "        # Apartment Low-Rise variations\n",
        "        'LOW RISE APARTMENT': 'Apartment Low-Rise', 'APARTMENT STYLE': 'Apartment Low-Rise', 'APARTMENT': 'Apartment Low-Rise',\n",
        "\n",
        "        # 3 Storey variations\n",
        "        '3 STOREY': '3 Storey', 'THREE STOREY': '3 Storey',\n",
        "\n",
        "        # Apartment High-Rise variations\n",
        "        'HIGH RISE APARTMENT': 'Apartment High-Rise', 'HIGH-RISE': 'Apartment High-Rise',\n",
        "\n",
        "        # Mobile/Modular variations\n",
        "        'MOBILE HOME': 'Mobile Modular Home', 'MODULAR': 'Mobile Modular Home',\n",
        "\n",
        "        # Townhouse variations\n",
        "        'TOWNHOUSE': 'Townhouse', 'ROW HOUSE': 'Townhouse', 'SEMI-DETACHED BUNGALOW': 'Townhouse',\n",
        "\n",
        "        # Other Plexes\n",
        "        'DUPLEX': 'Duplex', 'TRIPLEX': 'Triplex', 'FOURPLEX': 'Fourplex', 'DUPLEX/FOURPLEX': 'Fourplex',\n",
        "\n",
        "        # Condo variations\n",
        "        'CONDO': 'Condo', 'LOFT': 'Condo',\n",
        "\n",
        "        # House variations\n",
        "        'MANSION': 'House', 'FARM HOUSE': 'House', 'COTTAGE': 'House', 'TRADITIONAL': 'House',\n",
        "        'CONTEMPORARY': 'House', 'ACREAGE': 'House', 'DETACHED': 'House', 'HOUSE': 'House',\n",
        "\n",
        "        # Other\n",
        "        'CONVERTED HOME': 'Other', 'COMMERCIAL': 'Commercial',\n",
        "    }\n",
        "\n",
        "def standardize_property_styles(df, style_mapping=None):\n",
        "    \"\"\"\n",
        "    Standardize property style descriptions to consistent categories.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting property style standardization...\", also_print=True)\n",
        "        if 'Style' not in df.columns:\n",
        "            raise ValueError(\"Required column 'Style' not found in DataFrame\")\n",
        "\n",
        "        df_clean = df.copy()\n",
        "        logger.log_info(\"Created working copy of DataFrame.\", also_print=False)\n",
        "\n",
        "        if style_mapping is None:\n",
        "            style_mapping = create_style_mapping()\n",
        "            logger.log_info(\"Using default style mapping.\", also_print=False)\n",
        "\n",
        "        # [PERFORMANCE FIX] Create the sorted list *once*\n",
        "        sorted_map_keys = sorted(style_mapping.keys(), key=len, reverse=True)\n",
        "\n",
        "        df_clean['Original_Style'] = df_clean['Style'] # Preserve original\n",
        "        unmapped_styles_tracker = defaultdict(int)\n",
        "        standardized_rows_count = 0\n",
        "\n",
        "        def _standardize_single_style(style):\n",
        "            \"\"\"Helper function to standardize a single style value.\"\"\"\n",
        "            nonlocal standardized_rows_count, unmapped_styles_tracker\n",
        "\n",
        "            if pd.isna(style):\n",
        "                return style\n",
        "\n",
        "            style_upper = str(style).upper().strip()\n",
        "\n",
        "            # Pass 1: Try direct mapping first (fast)\n",
        "            if style_upper in style_mapping:\n",
        "                standardized_rows_count += 1\n",
        "                return style_mapping[style_upper]\n",
        "\n",
        "            # Pass 2: Try pattern matching (slower fallback)\n",
        "            for key_pattern in sorted_map_keys: # Use pre-sorted list\n",
        "                if key_pattern in style_upper:\n",
        "                    standardized_rows_count += 1\n",
        "                    return style_mapping[key_pattern]\n",
        "\n",
        "            # If no mapping found, track and assign 'Other'\n",
        "            unmapped_styles_tracker[style] += 1\n",
        "            return 'Other'\n",
        "\n",
        "        logger.log_info(\"Applying standardization rules to 'Style' column...\", also_print=True)\n",
        "        df_clean['Style'] = df_clean['Style'].apply(_standardize_single_style)\n",
        "        logger.log_info(\"Style standardization application complete.\")\n",
        "\n",
        "        # [PARQUET/STREAMLIT FIX] Convert to category dtype\n",
        "        logger.log_info(\"Converting 'Style' column to 'category' dtype for memory efficiency...\", also_print=True)\n",
        "        df_clean['Style'] = df_clean['Style'].astype('category')\n",
        "\n",
        "        # --- Console & Log Summary ---\n",
        "        total_rows = len(df_clean)\n",
        "        original_unique_styles = df['Style'].nunique(dropna=False)\n",
        "        new_unique_styles = df_clean['Style'].nunique(dropna=False)\n",
        "\n",
        "        print(\"\\n--- Property Style Standardization Summary ---\")\n",
        "        summary_data = [\n",
        "            [\"Total Rows Processed\", f\"{total_rows:,}\"],\n",
        "            [\"Original Unique Styles (incl. NaN)\", f\"{original_unique_styles:,}\"],\n",
        "            [\"New Unique Styles (incl. NaN)\", f\"{new_unique_styles:,}\"],\n",
        "            [\"Rows Standardized\", f\"{standardized_rows_count:,}\"],\n",
        "            [\"Standardization Percentage\", f\"{(standardized_rows_count / total_rows * 100):.2f}%\"]\n",
        "        ]\n",
        "        # [THE FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(summary_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nProperty Style Standardization Summary (detailed):\\n{tabulate.tabulate(summary_data, headers=['Metric', 'Value'], tablefmt='plain')}\")\n",
        "\n",
        "        # --- Console & Log: New Style Distribution ---\n",
        "        print(\"\\n--- New Style Distribution ---\")\n",
        "        new_style_counts = df_clean['Style'].value_counts(dropna=False)\n",
        "        new_style_data_for_table = []\n",
        "        for style, count in new_style_counts.items():\n",
        "            style_str = 'NULL' if pd.isna(style) else str(style)\n",
        "            percentage = (count / total_rows) * 100\n",
        "            new_style_data_for_table.append([style_str, f\"{count:,}\", f\"{percentage:.2f}%\"])\n",
        "        # [THE FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(new_style_data_for_table, headers=[\"Standardized Style\", \"Count\", \"Percentage\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nNew Style Distribution (detailed):\\n{tabulate.tabulate(new_style_data_for_table, headers=['Standardized Style', 'Count', 'Percentage'], tablefmt='plain')}\")\n",
        "\n",
        "        # --- Console & Log: Unmapped Styles ---\n",
        "        filtered_unmapped = {style: count for style, count in unmapped_styles_tracker.items() if count > 0}\n",
        "        if filtered_unmapped:\n",
        "            sorted_unmapped = sorted(filtered_unmapped.items(), key=lambda x: x[1], reverse=True)\n",
        "            print(f\"\\n--- {len(filtered_unmapped):,} Unmapped Styles Found ---\")\n",
        "            print(\"These values could not be categorized. See log for a detailed list.\")\n",
        "\n",
        "            logger.log_info(\"\\nUnmapped Style Details (detailed to log):\")\n",
        "            unmapped_table_data = []\n",
        "            for style, count in sorted_unmapped:\n",
        "                percentage = (count / total_rows) * 100\n",
        "                unmapped_table_data.append([str(style), f\"{count:,}\", f\"{percentage:.2f}%\"])\n",
        "            # [THE FIX] Using tabulate.tabulate()\n",
        "            logger.log_info(tabulate.tabulate(unmapped_table_data, headers=[\"Original Unmapped Style\", \"Count\", \"Percentage\"], tablefmt=\"plain\"))\n",
        "        else:\n",
        "            logger.log_info(\"No unmapped styles found after standardization.\", also_print=True)\n",
        "\n",
        "        summary = {\n",
        "            'original_unique_styles': original_unique_styles,\n",
        "            'new_unique_styles': new_unique_styles,\n",
        "            'unmapped_styles_counts': dict(unmapped_styles_tracker)\n",
        "        }\n",
        "        logger.log_info(\"Property style standardization completed.\", also_print=True)\n",
        "        return df_clean, summary\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in standardize_property_styles: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 17: Property Style Standardization ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Property Style Standardization\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty.\")\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    if 'Style' not in combined_data.columns:\n",
        "        raise ValueError(\"Required column 'Style' not found in DataFrame.\")\n",
        "\n",
        "    style_backup = combined_data['Style'].copy()\n",
        "    logger.log_info(\"Created backup of original 'Style' column.\")\n",
        "\n",
        "    combined_data, style_stats = standardize_property_styles(combined_data)\n",
        "\n",
        "    print(\"\\n‚úÖ Property style standardization completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during style standardization:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame) and 'style_backup' in locals():\n",
        "        try:\n",
        "            combined_data['Style'] = style_backup\n",
        "            if 'Original_Style' in combined_data.columns:\n",
        "                combined_data = combined_data.drop(columns=['Original_Style'])\n",
        "            logger.log_info(\"Restored original 'Style' column due to error.\", also_print=True)\n",
        "        except Exception as restore_e:\n",
        "            logger.log_error(f\"CRITICAL ERROR: Failed to restore 'Style' column: {restore_e}\", also_print=True)\n",
        "\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape}\")\n",
        "\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 17 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "RvtqE-wyoFUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 18: ADVANCED STYLE CLASSIFICATION (PHASE 2)\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell performs the second phase of 'Style' standardization using a\n",
        "Machine Learning model. Its primary goal is to intelligently clean up\n",
        "the 'Other' category that was created by the rules-based standardization\n",
        "in Cell 17.\n",
        "\n",
        "THE WORKFLOW:\n",
        "1.  IT TRUSTS CELL 17: It takes all the \"clean\" styles (e.g., \"Bungalow\",\n",
        "    \"2 Storey\") from Cell 17 as its \"ground truth\" or training data.\n",
        "2.  IT LEARNS: It trains a RandomForestClassifier to find patterns. It\n",
        "    learns to associate the 'Original_Style', 'Structure_Type', and\n",
        "    'Property_Sub_Type' text with the clean 'Style' labels.\n",
        "3.  IT PREDICTS: It then uses this trained model to predict the *true*\n",
        "    style for all the rows that Cell 17 flagged as 'Other'.\n",
        "4.  IT UPDATES (SAFELY): It only updates the 'Style' column (changing\n",
        "    'Other' to its prediction, e.g., \"Bungalow\") if the model is\n",
        "    highly confident (e.g., > 70% probability).\n",
        "5.  IT RE-CONVERTS: Finally, it re-converts the 'Style' column back\n",
        "    to a 'category' dtype to maintain memory and performance gains.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # Import the module\n",
        "import traceback\n",
        "\n",
        "# Import scikit-learn components\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def prepare_style_classification_data(df):\n",
        "    \"\"\"\n",
        "    Prepare data for style classification, handling missing values and creating features.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Preparing data for style classification...\", also_print=True)\n",
        "        working_df = df.copy()\n",
        "        logger.log_info(\"Created working copy of DataFrame.\", also_print=False)\n",
        "\n",
        "        columns_to_check = ['Original_Style', 'Style', 'Structure_Type', 'Property_Sub_Type']\n",
        "\n",
        "        print(\"\\n--- Initial Style Classification Data Check ---\")\n",
        "        missing_data = []\n",
        "        for col in columns_to_check:\n",
        "            if col not in working_df.columns:\n",
        "                working_df[col] = np.nan\n",
        "\n",
        "            missing_count = working_df[col].isna().sum()\n",
        "            missing_data.append([col, f\"{missing_count:,}\"])\n",
        "\n",
        "        print(tabulate.tabulate(missing_data, headers=[\"Column\", \"Missing Values\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nInitial Missing Value Check (detailed):\\n{tabulate.tabulate(missing_data, headers=['Column', 'Missing Values'], tablefmt='plain')}\")\n",
        "\n",
        "        # Handle missing values\n",
        "        working_df['Original_Style'] = working_df['Original_Style'].fillna('Unknown Style')\n",
        "        if pd.api.types.is_categorical_dtype(working_df['Style']):\n",
        "            if 'Other' not in working_df['Style'].cat.categories:\n",
        "                working_df['Style'] = working_df['Style'].cat.add_categories('Other')\n",
        "        working_df['Style'] = working_df['Style'].fillna('Other')\n",
        "        working_df['Structure_Type'] = working_df['Structure_Type'].fillna('Unknown Structure')\n",
        "        working_df['Property_Sub_Type'] = working_df['Property_Sub_Type'].fillna('Unknown Type')\n",
        "\n",
        "        # Create combined features\n",
        "        working_df['Combined_Features'] = (\n",
        "            working_df['Original_Style'].astype(str) + ' ' +\n",
        "            working_df['Structure_Type'].astype(str) + ' ' +\n",
        "            working_df['Property_Sub_Type'].astype(str)\n",
        "        )\n",
        "        working_df['Combined_Features'] = (working_df['Combined_Features']\n",
        "                                           .str.upper()\n",
        "                                           .str.strip())\n",
        "\n",
        "        confident_mask = working_df['Style'] != 'Other'\n",
        "\n",
        "        print(\"\\n--- Training Data Preparation Summary ---\")\n",
        "        training_summary_data = [\n",
        "            [\"Total properties\", f\"{len(working_df):,}\"],\n",
        "            [\"Confident examples (for training)\", f\"{confident_mask.sum():,}\"],\n",
        "            [\"Uncertain examples (for prediction)\", f\"{(~confident_mask).sum():,}\"]\n",
        "        ]\n",
        "        print(tabulate.tabulate(training_summary_data, headers=[\"Metric\", \"Count\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nTraining Data Preparation Summary (detailed):\\n{tabulate.tabulate(training_summary_data, headers=['Metric', 'Count'], tablefmt='plain')}\")\n",
        "\n",
        "        logger.log_info(\"Data preparation for style classification complete.\", also_print=True)\n",
        "        return working_df, confident_mask\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in prepare_style_classification_data: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "def train_style_classifier(df):\n",
        "    \"\"\"\n",
        "    Train a machine learning model to categorize property styles.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        working_df, confident_mask = prepare_style_classification_data(df)\n",
        "\n",
        "        confident_styles = working_df[confident_mask].copy()\n",
        "        uncertain_styles = working_df[~confident_mask].copy()\n",
        "\n",
        "        if confident_styles.empty:\n",
        "            logger.log_error(\"No confident style examples found for training. Aborting.\", also_print=True)\n",
        "            raise ValueError(\"Insufficient confident data for training.\")\n",
        "\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            analyzer='word',\n",
        "            ngram_range=(1, 3),\n",
        "            max_features=1500,\n",
        "            stop_words='english'\n",
        "        )\n",
        "\n",
        "        logger.log_info(\"\\nPreparing training data with TF-IDF vectorizer...\", also_print=True)\n",
        "        X = vectorizer.fit_transform(confident_styles['Combined_Features'])\n",
        "        y = confident_styles['Style'].values\n",
        "\n",
        "        logger.log_info(f\"Created {X.shape[1]} features from combined descriptions.\", also_print=True)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "        logger.log_info(f\"Training data split: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test.\", also_print=False)\n",
        "\n",
        "        logger.log_info(\"\\nTraining the RandomForestClassifier...\", also_print=True)\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            random_state=42,\n",
        "            min_samples_leaf=5,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        model.fit(X_train, y_train)\n",
        "        logger.log_info(\"Classifier training complete.\", also_print=False)\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        accuracy = report['accuracy']\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "\n",
        "        print(\"\\n--- Model Performance Summary ---\")\n",
        "        performance_summary = [\n",
        "            [\"Accuracy\", f\"{accuracy:.2%}\"],\n",
        "            [\"Macro Avg F1-Score\", f\"{macro_f1:.2f}\"]\n",
        "        ]\n",
        "        print(tabulate.tabulate(performance_summary, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nFull Classification Report (detailed to log):\\n{classification_report(y_test, y_pred)}\")\n",
        "\n",
        "        logger.log_info(\"\\nMaking predictions for uncertain styles...\", also_print=True)\n",
        "        if uncertain_styles.empty:\n",
        "            logger.log_info(\"No uncertain styles to predict.\", also_print=True)\n",
        "            return model, vectorizer, pd.DataFrame(), np.array([])\n",
        "\n",
        "        uncertain_vectors = vectorizer.transform(uncertain_styles['Combined_Features'])\n",
        "        predictions_array = model.predict(uncertain_vectors)\n",
        "        prediction_probs_array = model.predict_proba(uncertain_vectors)\n",
        "\n",
        "        confidence_threshold = 0.7\n",
        "        confident_predictions_mask = prediction_probs_array.max(axis=1) >= confidence_threshold\n",
        "\n",
        "        results = uncertain_styles.copy()\n",
        "        results['Predicted_Style'] = predictions_array\n",
        "        results['Confidence'] = prediction_probs_array.max(axis=1)\n",
        "\n",
        "        print(\"\\n--- Prediction Results Summary ---\")\n",
        "        prediction_summary_data = [\n",
        "            [\"Total uncertain styles\", f\"{len(uncertain_styles):,}\"],\n",
        "            [f\"High confidence (>= {confidence_threshold:.0%})\", f\"{confident_predictions_mask.sum():,}\"],\n",
        "            [f\"Low confidence (< {confidence_threshold:.0%})\", f\"{(~confident_predictions_mask).sum():,}\"]\n",
        "        ]\n",
        "        print(tabulate.tabulate(prediction_summary_data, headers=[\"Metric\", \"Count\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nPrediction Results Summary (detailed):\\n{tabulate.tabulate(prediction_summary_data, headers=['Metric', 'Count'], tablefmt='plain')}\")\n",
        "\n",
        "        logger.log_info(\"Style classification training and prediction complete.\", also_print=True)\n",
        "        return model, vectorizer, results, confident_predictions_mask\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in train_style_classifier: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "def analyze_low_confidence_cases(predictions, confidence_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Analyze predictions that fell below the confidence threshold.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        low_confidence_df = predictions[predictions['Confidence'] < confidence_threshold].copy()\n",
        "        print(f\"\\n--- Analysis of Low Confidence Predictions ({len(low_confidence_df):,} cases) ---\")\n",
        "\n",
        "        if low_confidence_df.empty:\n",
        "            print(\"No low confidence predictions to analyze.\")\n",
        "            logger.log_info(\"No low confidence predictions to analyze.\", also_print=False)\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        pattern_analysis = low_confidence_df.groupby('Original_Style').agg(\n",
        "            Predicted_Style=('Predicted_Style', lambda x: x.mode()[0] if not x.empty else np.nan),\n",
        "            Count=('Predicted_Style', 'count'),\n",
        "            Average_Confidence=('Confidence', 'mean')\n",
        "        ).sort_values('Count', ascending=False)\n",
        "\n",
        "        print(\"Top 10 common patterns in low confidence cases:\")\n",
        "        if not pattern_analysis.empty:\n",
        "            top_10_patterns_data = []\n",
        "            for original_style, row in pattern_analysis.head(10).iterrows():\n",
        "                top_10_patterns_data.append([\n",
        "                    original_style,\n",
        "                    row['Predicted_Style'],\n",
        "                    f\"{row['Average_Confidence']:.2f}\",\n",
        "                    f\"{row['Count']:,}\"\n",
        "                ])\n",
        "            print(tabulate.tabulate(top_10_patterns_data, headers=[\"Original Style Text\", \"Most Common Prediction\", \"Avg Confidence\", \"Count\"], tablefmt=\"grid\"))\n",
        "            logger.log_info(f\"\\nFull Low Confidence Prediction Pattern Analysis (detailed to log):\\n{pattern_analysis.to_string()}\")\n",
        "        else:\n",
        "            print(\"No significant patterns found in low confidence cases.\")\n",
        "            logger.log_info(\"No significant patterns found in low confidence cases.\", also_print=False)\n",
        "\n",
        "        return pattern_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in analyze_low_confidence_cases: {str(e)}\", also_print=True)\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 18: Advanced Style Classification ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Advanced Style Classification Using Machine Learning\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty.\")\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    required_ml_cols = ['Original_Style', 'Style', 'Structure_Type', 'Property_Sub_Type']\n",
        "    if not all(col in combined_data.columns for col in required_ml_cols):\n",
        "        raise ValueError(f\"Missing one or more required columns: {required_ml_cols}\")\n",
        "\n",
        "    style_backup_ml = combined_data['Style'].copy()\n",
        "    logger.log_info(\"Created backup of 'Style' column for ML error recovery.\")\n",
        "\n",
        "    # Run classification\n",
        "    model, vectorizer, predictions, confident_ml_mask = train_style_classifier(combined_data)\n",
        "\n",
        "    # Update dataset with high-confidence predictions\n",
        "    logger.log_info(\"\\nUpdating dataset with high-confidence ML predictions...\", also_print=True)\n",
        "    high_confidence_predictions = predictions[confident_ml_mask]\n",
        "\n",
        "    if not high_confidence_predictions.empty:\n",
        "        combined_data.loc[high_confidence_predictions.index, 'Style'] = high_confidence_predictions['Predicted_Style']\n",
        "        logger.log_info(f\"Applied {len(high_confidence_predictions):,} high-confidence predictions.\", also_print=True)\n",
        "\n",
        "        # Re-convert to category to maintain memory savings\n",
        "        logger.log_info(\"Re-converting 'Style' column to 'category' dtype...\", also_print=True)\n",
        "        combined_data['Style'] = combined_data['Style'].astype('category')\n",
        "\n",
        "    else:\n",
        "        logger.log_info(\"No high-confidence predictions to apply.\", also_print=True)\n",
        "\n",
        "    # Analyze low confidence cases\n",
        "    low_confidence_analysis_results = analyze_low_confidence_cases(predictions)\n",
        "\n",
        "    # Show final distribution\n",
        "    print(\"\\n--- Final Style Distribution After ML Classification ---\")\n",
        "    final_style_counts = combined_data['Style'].value_counts(dropna=False)\n",
        "    final_style_data_for_table = []\n",
        "    total_after_ml = len(combined_data)\n",
        "\n",
        "    for style, count in final_style_counts.items():\n",
        "        style_str = 'NULL' if pd.isna(style) else str(style)\n",
        "        percentage = (count / total_after_ml) * 100\n",
        "        final_style_data_for_table.append([style_str, f\"{count:,}\", f\"{percentage:.2f}%\"])\n",
        "\n",
        "    print(tabulate.tabulate(final_style_data_for_table, headers=[\"Final Style\", \"Count\", \"Percentage\"], tablefmt=\"grid\"))\n",
        "    logger.log_info(f\"\\nFinal Style Distribution After ML (detailed):\\n{tabulate.tabulate(final_style_data_for_table, headers=['Final Style', 'Count', 'Percentage'], tablefmt='plain')}\")\n",
        "\n",
        "    remaining_other = (combined_data['Style'] == 'Other').sum()\n",
        "    logger.log_info(f\"\\nRemaining 'Other' (unclassified) properties: {remaining_other:,}.\", also_print=True)\n",
        "\n",
        "    print(\"\\n‚úÖ Advanced style classification completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during advanced style classification:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame) and 'style_backup_ml' in locals():\n",
        "        try:\n",
        "            combined_data['Style'] = style_backup_ml\n",
        "            logger.log_info(\"Restored original 'Style' column due to error.\", also_print=True)\n",
        "        except Exception as restore_e:\n",
        "            logger.log_error(f\"CRITICAL ERROR: Failed to restore 'Style' column: {restore_e}\", also_print=True)\n",
        "\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape}\")\n",
        "\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 18 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "F0A-wYNLo2dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 19: CONDO DATA ANALYSIS - DETAILED BREAKDOWN AND STATISTICS\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell performs a detailed read-only analysis of all condo-related\n",
        "properties. It breaks down data by 'Condo_Type', analyzes the\n",
        "'Condo_Name' for placeholders (e.g., \"Z-name Not Listed\"), and\n",
        "cross-references these categories with the presence of 'Condo_Fee'\n",
        "to identify data quality issues.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "import traceback\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def analyze_condo_properties(df):\n",
        "    \"\"\"\n",
        "    Performs detailed analysis of condo properties.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting detailed condo property analysis...\", also_print=True)\n",
        "\n",
        "        required_columns = ['Condo_Type', 'Condo_Name', 'Condo_Fee']\n",
        "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {', '.join(missing_cols)}\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # --- PART 1: Analyze Property Types ---\n",
        "        logger.log_info(\"\\nAnalyzing property types...\", also_print=True)\n",
        "        total_properties = len(df)\n",
        "        non_condo_count = len(df[df['Condo_Type'] == 'Not a Condo'])\n",
        "        total_condos = len(df[df['Condo_Type'] != 'Not a Condo'])\n",
        "        condo_type_counts = df['Condo_Type'].value_counts(dropna=False)\n",
        "\n",
        "        print(\"\\n--- Condo Type Distribution ---\")\n",
        "        condo_type_data = []\n",
        "        for ctype, count in condo_type_counts.items():\n",
        "            display_type = 'NULL' if pd.isna(ctype) else str(ctype)\n",
        "            percentage = (count / total_properties) * 100\n",
        "            condo_type_data.append([display_type, f\"{count:,}\", f\"{percentage:.2f}%\"])\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(condo_type_data, headers=[\"Condo Type\", \"Count\", \"Percentage\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nCondo Type Distribution (detailed):\\n{tabulate.tabulate(condo_type_data, headers=['Condo Type', 'Count', 'Percentage'], tablefmt='plain')}\")\n",
        "\n",
        "        print(\"\\n--- Overall Condo Property Counts ---\")\n",
        "        overall_counts_data = [\n",
        "            [\"Total Properties in Dataset\", f\"{total_properties:,}\"],\n",
        "            [\"Non-Condo Properties\", f\"{non_condo_count:,}\"],\n",
        "            [\"Total Condo Properties (incl. various types)\", f\"{total_condos:,}\"]\n",
        "        ]\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(overall_counts_data, headers=[\"Category\", \"Count\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nOverall Condo Property Counts (detailed):\\n{tabulate.tabulate(overall_counts_data, headers=['Category', 'Count'], tablefmt='plain')}\")\n",
        "\n",
        "        # --- PART 2: Analyze Condo Names ---\n",
        "        logger.log_info(\"\\nAnalyzing condo names (placeholders)...\", also_print=True)\n",
        "        condo_data = df[df['Condo_Type'] != 'Not a Condo'].copy()\n",
        "        condo_data_for_name_check = condo_data['Condo_Name'].fillna('')\n",
        "\n",
        "        zname_mask = condo_data_for_name_check == 'Z-name Not Listed'\n",
        "        no_name_mask = condo_data_for_name_check == 'No Name'\n",
        "        name_not_mask = condo_data_for_name_check.str.contains('name not', case=False, na=False)\n",
        "\n",
        "        zname_count = zname_mask.sum()\n",
        "        no_name_count = no_name_mask.sum()\n",
        "        name_not_count = name_not_mask.sum()\n",
        "        regular_named_condos = total_condos - (zname_count + no_name_count + name_not_count)\n",
        "\n",
        "        print(\"\\n--- Condo Name Placeholder Breakdown ---\")\n",
        "        placeholder_data = [\n",
        "            [\"Condos with regular names\", f\"{regular_named_condos:,}\"],\n",
        "            [\"'Z-name Not Listed'\", f\"{zname_count:,}\"],\n",
        "            [\"'No Name'\", f\"{no_name_count:,}\"],\n",
        "            [\"Contains 'name not'\", f\"{name_not_count:,}\"]\n",
        "        ]\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(placeholder_data, headers=[\"Placeholder Type\", \"Count\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nCondo Name Placeholder Breakdown (detailed):\\n{tabulate.tabulate(placeholder_data, headers=['Placeholder Type', 'Count'], tablefmt='plain')}\")\n",
        "\n",
        "        # --- PART 3: Analyze Condo Fees ---\n",
        "        logger.log_info(\"\\nAnalyzing condo fees presence...\", also_print=True)\n",
        "\n",
        "        def analyze_fees_subset(data_subset):\n",
        "            data_subset_fee_col = pd.to_numeric(data_subset['Condo_Fee'], errors='coerce')\n",
        "            has_fee = data_subset_fee_col.notna().sum()\n",
        "            no_fee = data_subset_fee_col.isna().sum()\n",
        "            return {'has_fee': has_fee, 'no_fee': no_fee}\n",
        "\n",
        "        fee_analysis = {\n",
        "            'Regular Named Condos': analyze_fees_subset(condo_data[~(zname_mask | no_name_mask | name_not_mask)]),\n",
        "            'Z-name Not Listed': analyze_fees_subset(condo_data[zname_mask]),\n",
        "            'No Name': analyze_fees_subset(condo_data[no_name_mask]),\n",
        "            'Name Not': analyze_fees_subset(condo_data[name_not_mask])\n",
        "        }\n",
        "\n",
        "        print(\"\\n--- Condo Fee Presence by Name Category ---\")\n",
        "        fee_table_data = []\n",
        "        for category, stats in fee_analysis.items():\n",
        "            fee_table_data.append([category, f\"{stats['has_fee']:,}\", f\"{stats['no_fee']:,}\"])\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(fee_table_data, headers=[\"Category\", \"With Fee\", \"Without Fee\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nCondo Fee Presence by Name Category (detailed):\\n{tabulate.tabulate(fee_table_data, headers=['Category', 'With Fee', 'Without Fee'], tablefmt='plain')}\")\n",
        "\n",
        "        # --- PART 4: Find Least Common Condo Developments ---\n",
        "        logger.log_info(\"\\nIdentifying least common condo developments...\", also_print=True)\n",
        "        valid_condo_names_mask = ~(zname_mask | no_name_mask | name_not_mask) & (condo_data['Condo_Name'] != '') & (condo_data['Condo_Name'].notna())\n",
        "        valid_condos_with_names = condo_data[valid_condo_names_mask]\n",
        "\n",
        "        if not valid_condos_with_names.empty:\n",
        "            condo_counts = valid_condos_with_names['Condo_Name'].value_counts()\n",
        "            bottom_20 = condo_counts.nsmallest(20)\n",
        "\n",
        "            print(\"\\n--- Top 20 Least Common Condo Developments ---\")\n",
        "            least_common_data = []\n",
        "            for condo_name, count in bottom_20.items():\n",
        "                least_common_data.append([condo_name, f\"{count:,}\"])\n",
        "            # [FIX] Using tabulate.tabulate()\n",
        "            print(tabulate.tabulate(least_common_data, headers=[\"Condo Name\", \"Count\"], tablefmt=\"grid\"))\n",
        "            logger.log_info(f\"\\nLeast Common Condo Developments (detailed):\\n{tabulate.tabulate(least_common_data, headers=['Condo Name', 'Count'], tablefmt='plain')}\")\n",
        "        else:\n",
        "            print(\"No valid condo names to identify least common developments.\")\n",
        "            logger.log_info(\"No valid condo names to identify least common developments.\")\n",
        "\n",
        "        # --- PART 5: Store Results ---\n",
        "        results.update({\n",
        "            'property_counts': {\n",
        "                'total_properties': total_properties, 'non_condo_count': non_condo_count,\n",
        "                'total_condos': total_condos, 'regular_named_condos': regular_named_condos\n",
        "            },\n",
        "            'placeholder_counts': {\n",
        "                'zname_count': zname_count, 'no_name_count': no_name_count, 'name_not_count': name_not_count\n",
        "            },\n",
        "            'fee_analysis': fee_analysis,\n",
        "            'least_common_condos': bottom_20.to_dict() if 'bottom_20' in locals() else {}\n",
        "        })\n",
        "\n",
        "        # --- PART 6: Verification checks ---\n",
        "        total_accounted = (regular_named_condos + zname_count + no_name_count + name_not_count)\n",
        "        if total_accounted != total_condos:\n",
        "            print(\"\\n--- WARNING: Condo Counts Mismatch! ---\")\n",
        "            print(f\"Total condo properties identified: {total_condos:,}\")\n",
        "            print(f\"Sum of named/placeholder categories: {total_accounted:,}\")\n",
        "            print(f\"Difference: {total_condos - total_accounted:,}. Review 'Condo_Name' cleaning.\")\n",
        "            logger.log_error(f\"WARNING: Condo counts mismatch. Total: {total_condos:,}, Accounted: {total_accounted:,}\", also_print=True)\n",
        "        else:\n",
        "            logger.log_info(\"Verification: All condo properties accounted for.\", also_print=True)\n",
        "\n",
        "        logger.log_info(\"Detailed condo property analysis completed.\", also_print=True)\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"PROCESS HALTED: Error during condo analysis: {str(e)}\", also_print=True)\n",
        "        logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 19: Condo Data Analysis ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Starting Comprehensive Condo Analysis\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty.\")\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    # Run the analysis\n",
        "    condo_analysis_results = analyze_condo_properties(combined_data)\n",
        "\n",
        "    print(\"\\n‚úÖ Analysis completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during comprehensive condo analysis:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    # [FIX] Added standard finally block\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape} (unchanged)\")\n",
        "\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 19 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "fMD2QfwyrPo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 20: AUTOMATED CONDO NAME STANDARDIZATION (HYBRID)\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This cell automates the entire condo name standardization process by\n",
        "combining manual rules with automated discovery.\n",
        "\n",
        "WORKFLOW:\n",
        "1. Defines a `create_manual_overrides_map` for explicit business logic\n",
        "   (e.g., grouping \"KeyNote 2\" -> \"KeyNote\").\n",
        "2. Gets the value_counts() for all condo names to find the\n",
        "   \"Golden Records\" (most common spellings).\n",
        "3. Uses SequenceMatcher to find highly similar names (e.g., typos).\n",
        "4. Automatically creates a `discovered_map` that maps the typo\n",
        "   (less common) to the \"Golden Record\" (more common).\n",
        "5. Merges the `discovered_map` with the `manual_overrides_map`,\n",
        "   (giving manual rules priority).\n",
        "6. Applies this final, hybrid map to the DataFrame.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "from tabulate import tabulate\n",
        "from tqdm.notebook import tqdm\n",
        "import traceback\n",
        "\n",
        "def create_manual_overrides_map() -> dict:\n",
        "    \"\"\"\n",
        "    Creates the BASE mapping dictionary for standardizing condo names.\n",
        "    This map contains explicit business logic and overrides all\n",
        "    automated discovery.\n",
        "    \"\"\"\n",
        "    # These are your hard-coded rules from the original standardization cell\n",
        "    # These will ALWAYS take precedence.\n",
        "    spelling_standardization = {\n",
        "        'Chateaux Strathcona': 'Chateau Strathcona', 'Cheasapeake': 'Chesapeake',\n",
        "        \"Davenport's Country Lane \": \"Davenport's Country Lane\", 'Hamstead Estates': 'Hampstead Estates',\n",
        "        'Hanson Creek Manors': 'Hanson Creek Manor', 'Laural House At The Park': 'Laurel House At The Park',\n",
        "        'Oakhampton Court': 'Oak Hampton Court', 'Refrew House': 'Renfrew House',\n",
        "        'The Terraces': 'The Terrace', 'Woodmeadows': 'Wood Meadows',\n",
        "        'Calvannah Village': 'Calvanna Village', 'Chrystal Heights': 'Crystal Heights',\n",
        "        'Huntview Village': 'Huntsview Village', 'Mackenzie Village': 'McKenzie Village',\n",
        "        'The Pavillions': 'The Pavilions', 'Holly Spring': 'Holly Springs',\n",
        "        'Pattina Park': 'Patina Park', 'Sierra Grand': 'Sierra Grande',\n",
        "        'Copperfield Park II': 'Copperfield Park', 'KeyNote 2': 'KeyNote',\n",
        "        'The Mosaic In Mckenzie Town': 'The Mosaic In McKenzie Town',\n",
        "        'Courtyards At Garrison Woods': 'Courtyards of Garrison Woods',\n",
        "        'Aspen Stone': 'Aspenstone', 'Rocky Ridge Ranch': 'Rocky Ridge Royal Oak',\n",
        "        'The Riverstone': 'Riverstone', 'The Gateway': 'Gateway',\n",
        "        'Riverbend': 'Riverbend Station', 'Canyon Meadows Manor': 'Canyon Meadows',\n",
        "    }\n",
        "\n",
        "    # Case standardization\n",
        "    case_standardization = {\n",
        "        'renaissance': 'Renaissance', 'THE MOSAIC IN MCKENZIE TOWNE': 'The Mosaic In McKenzie Towne',\n",
        "        'CALEDONIA ON THE WATERFRONT': 'Caledonia On The Waterfront', 'THE PAVILIONS OF EAU CLAIRE': 'The Pavilions Of Eau Claire',\n",
        "        # ... (and all your other case standardizations) ...\n",
        "        'THE YORK': 'The York', 'VILLAGE GREEN': 'Village Green',\n",
        "    }\n",
        "\n",
        "    combined_map = {}\n",
        "    for k, v in spelling_standardization.items():\n",
        "        combined_map[k.upper().strip()] = v\n",
        "    for k, v in case_standardization.items():\n",
        "        if k.upper().strip() not in combined_map:\n",
        "            combined_map[k.upper().strip()] = v\n",
        "\n",
        "    # Add complex groupings that are also standardizations\n",
        "    complex_map_for_overrides = {\n",
        "        'KeyNote 2': 'KeyNote', 'Copperfield Park II': 'Copperfield Park', 'Copperfield Park III': 'Copperfield Park',\n",
        "        'Guardian North': 'The Guardian', 'Guardian South': 'The Guardian',\n",
        "        'Evolution Fuse': 'Evolution', 'Evolution Pulse': 'Evolution',\n",
        "        'University City I': 'University City', 'University City II': 'University City', 'University City III': 'University City', 'University City IV': 'University City',\n",
        "    }\n",
        "    for k, v in complex_map_for_overrides.items():\n",
        "        combined_map[k.upper().strip()] = v\n",
        "\n",
        "    return combined_map\n",
        "\n",
        "def discover_spelling_variations(df: pd.DataFrame, condo_counts: pd.Series, manual_map_keys: set) -> dict:\n",
        "    \"\"\"\n",
        "    Analyzes condo names to find typos and maps them to the\n",
        "    \"Golden Record\" (most common spelling).\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Starting automated discovery of spelling variations...\", also_print=True)\n",
        "\n",
        "    unique_names = sorted(condo_counts.index.tolist())\n",
        "    discovered_map = {}\n",
        "    potential_duplicates_logged = []\n",
        "\n",
        "    # This is the O(n^2) loop from your analysis cell\n",
        "    for i, name1 in tqdm(enumerate(unique_names), total=len(unique_names), desc=\"Discovering Typos\"):\n",
        "        # Stop check_memory_usage from spamming the log during the loop\n",
        "        if i % 1000 == 0:\n",
        "            check_memory_usage(warning_threshold_mb=4000) # Higher threshold during loop\n",
        "\n",
        "        name1_upper = name1.upper().strip()\n",
        "\n",
        "        # Skip if this name is already a target for a manual rule\n",
        "        if name1_upper in manual_map_keys:\n",
        "            continue\n",
        "\n",
        "        for name2 in unique_names[i+1:]:\n",
        "            name2_upper = name2.upper().strip()\n",
        "\n",
        "            # Skip if this name is also a target for a manual rule\n",
        "            if name2_upper in manual_map_keys:\n",
        "                continue\n",
        "\n",
        "            ratio = SequenceMatcher(None, name1_upper, name2_upper).ratio()\n",
        "\n",
        "            # Use a high similarity ratio for full automation.\n",
        "            # 0.9 is a good balance for catching typos without\n",
        "            # incorrectly grouping distinct names.\n",
        "            if ratio > 0.90 and ratio < 1.0:\n",
        "                count1 = condo_counts.get(name1, 0)\n",
        "                count2 = condo_counts.get(name2, 0)\n",
        "\n",
        "                # Determine the \"Golden Record\" and the \"Typo\"\n",
        "                if count1 > count2:\n",
        "                    golden_record = name1\n",
        "                    typo_variation = name2\n",
        "                else:\n",
        "                    golden_record = name2\n",
        "                    typo_variation = name1\n",
        "\n",
        "                # Create the rule\n",
        "                discovered_map[typo_variation.upper().strip()] = golden_record\n",
        "                potential_duplicates_logged.append((typo_variation, golden_record, ratio))\n",
        "\n",
        "    if potential_duplicates_logged:\n",
        "        logger.log_info(f\"Automatically discovered {len(potential_duplicates_logged)} new spelling variations.\")\n",
        "        for typo, golden, ratio in potential_duplicates_logged[:20]: # Log top 20\n",
        "             logger.log_info(f\"  Mapped TYPO: '{typo}' -> TO GOLDEN: '{golden}' (Ratio: {ratio:.2f})\")\n",
        "    else:\n",
        "        logger.log_info(\"No new spelling variations discovered on this run.\")\n",
        "\n",
        "    return discovered_map\n",
        "\n",
        "def standardize_condo_names_automated(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Main function to run the complete automated standardization.\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Starting automated condo name standardization...\", also_print=True)\n",
        "\n",
        "    if 'Condo_Name' not in df.columns:\n",
        "        raise ValueError(\"DataFrame must contain a 'Condo_Name' column.\")\n",
        "\n",
        "    standardized_df = df.copy()\n",
        "\n",
        "    # --- Step 1: Get Manual Overrides ---\n",
        "    manual_map = create_manual_overrides_map()\n",
        "    manual_map_keys_upper = set(manual_map.keys())\n",
        "    logger.log_info(f\"Loaded {len(manual_map)} manual override rules.\")\n",
        "\n",
        "    # --- Step 2: Get \"Golden Record\" Counts ---\n",
        "    # We get counts *before* any standardization\n",
        "    original_condo_counts = standardized_df['Condo_Name'].dropna().value_counts()\n",
        "    logger.log_info(f\"Analyzed {len(original_condo_counts)} unique condo names for Golden Records.\")\n",
        "\n",
        "    # --- Step 3: Discover New Variations ---\n",
        "    discovered_map = discover_spelling_variations(\n",
        "        standardized_df,\n",
        "        original_condo_counts,\n",
        "        manual_map_keys_upper\n",
        "    )\n",
        "    logger.log_info(f\"Discovered {len(discovered_map)} new typo variations.\")\n",
        "\n",
        "    # --- Step 4: Merge Maps (Manual rules win) ---\n",
        "    final_map = discovered_map.copy()\n",
        "    final_map.update(manual_map) # Apply manual rules, overwriting any discovered conflicts\n",
        "    logger.log_info(f\"Created final map with {len(final_map)} total rules.\")\n",
        "\n",
        "    # --- Step 5: Apply the Final Map ---\n",
        "    original_names = standardized_df['Condo_Name'].copy()\n",
        "\n",
        "    # Helper for fast, case-insensitive mapping\n",
        "    upper_name_map = {k.upper().strip(): v for k, v in final_map.items()}\n",
        "\n",
        "    def apply_final_map(name):\n",
        "        if pd.isna(name):\n",
        "            return np.nan\n",
        "\n",
        "        name_str = str(name)\n",
        "        name_upper = name_str.upper().strip()\n",
        "\n",
        "        # Check map first\n",
        "        if name_upper in upper_name_map:\n",
        "            return upper_name_map[name_upper]\n",
        "\n",
        "        # If no map, just clean and title case it\n",
        "        return name_str.strip().title()\n",
        "\n",
        "    logger.log_info(\"Applying final merged map to all records...\", also_print=True)\n",
        "    standardized_df['Condo_Name'] = standardized_df['Condo_Name'].apply(apply_final_map)\n",
        "\n",
        "    # --- Step 6: Report ---\n",
        "    changes_mask = (original_names != standardized_df['Condo_Name']) & (original_names.notna())\n",
        "    records_changed_count = changes_mask.sum()\n",
        "\n",
        "    original_unique_count = original_names.nunique()\n",
        "    final_unique_count = standardized_df['Condo_Name'].nunique()\n",
        "\n",
        "    print(\"\\n--- Automated Standardization Summary ---\")\n",
        "    summary_data = [\n",
        "        [\"Original Unique Names\", f\"{original_unique_count:,}\"],\n",
        "        [\"Final Unique Names\", f\"{final_unique_count:,}\"],\n",
        "        [\"Total Records Changed\", f\"{records_changed_count:,}\"]\n",
        "    ]\n",
        "    print(tabulate(summary_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "    logger.log_info(f\"Standardization Summary (detailed):\\n{tabulate(summary_data, headers=['Metric', 'Value'], tablefmt='plain')}\")\n",
        "\n",
        "    return standardized_df\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 20: Automated Condo Standardization ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running AUTOMATED Condo Name Standardization\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame):\n",
        "        raise NameError(\"Variable 'combined_data' not found.\")\n",
        "\n",
        "    if 'Condo_Name' not in combined_data.columns:\n",
        "        raise ValueError(\"Required column 'Condo_Name' not found.\")\n",
        "\n",
        "    condo_name_backup = combined_data['Condo_Name'].copy()\n",
        "    logger.log_info(\"Created backup of 'Condo_Name' column.\")\n",
        "\n",
        "    # Run the new automated function\n",
        "    combined_data = standardize_condo_names_automated(combined_data)\n",
        "\n",
        "    print(\"\\n‚úÖ Automated condo name standardization completed successfully! üéâ\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during automated condo standardization:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "\n",
        "    if 'combined_data' in locals() and 'condo_name_backup' in locals():\n",
        "        try:\n",
        "            combined_data['Condo_Name'] = condo_name_backup\n",
        "            logger.log_info(\"Restored original 'Condo_Name' values due to error.\", also_print=True)\n",
        "        except Exception as restore_e:\n",
        "            logger.log_error(f\"CRITICAL ERROR: Failed to restore 'Condo_Name' values: {restore_e}\", also_print=True)\n",
        "    raise\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape}\")\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 20 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "HJ-HQKrL8z9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 21: CONDO COMPLEX GROUPING AND RELATIONSHIP MAPPING\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This code block creates and manages condo complex relationships by:\n",
        "1. Creating a mapping between individual buildings and their parent complexes\n",
        "2. Adding a new 'Condo_Complex' column to track these relationships\n",
        "3. Analyzing the distribution of buildings within complexes\n",
        "\n",
        "This cell now runs *after* the automated standardization in Cell 20,\n",
        "so it operates on perfectly clean data.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "import traceback\n",
        "\n",
        "def create_complex_column(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a new column 'Condo_Complex' that groups related buildings together.\n",
        "    (This function is from your original pipeline, unchanged)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting Condo Complex grouping process...\", also_print=True)\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"--- Condo Complex Grouping and Mapping ---\")\n",
        "\n",
        "        if 'Condo_Name' not in df.columns:\n",
        "            raise ValueError(\"DataFrame must contain 'Condo_Name' column.\")\n",
        "\n",
        "        complex_df = df.copy()\n",
        "        logger.log_info(\"Created working copy of DataFrame.\", also_print=False)\n",
        "\n",
        "        # This map now references the *standardized* names\n",
        "        # Note: We can simplify this map, as \"KeyNote 2\" should already\n",
        "        # be \"KeyNote\". But having redundancy is safe.\n",
        "        complex_mapping = {\n",
        "            'Alpine Park Phase 1': 'Alpine Park', 'Alpine Park Phase 2': 'Alpine Park', 'Alpine Park Phase 3': 'Alpine Park',\n",
        "            'Bella Rosa': 'Bella Rosa', 'Bella Rosa 1': 'Bella Rosa', 'Bella Rosa I': 'Bella Rosa', 'Bella Rosa II': 'Bella Rosa', 'Bella Rosa III': 'Bella Rosa',\n",
        "            'Chinook': 'Chinook', 'Chinook Ii': 'Chinook', 'Chinook Iii': 'Chinook', 'Chinook IV': 'Chinook',\n",
        "            'Copperfield Park': 'Copperfield Park', 'Copperfield Park II': 'Copperfield Park', 'Copperfield Park III': 'Copperfield Park',\n",
        "            'Five West': 'Five West', 'Five West Phase I': 'Five West', 'Five West Phase II': 'Five West', 'Five West Phase III': 'Five West',\n",
        "            'Imperial Place': 'Imperial Place', 'Imperial Place I': 'Imperial Place', 'Imperial Place II': 'Imperial Place',\n",
        "            'Lakeview Green': 'Lakeview Green', 'Lakeview Green 1': 'Lakeview Green', 'Lakeview Green 2': 'Lakeview Green', 'Lakeview Green 3': 'Lakeview Green', 'Lakeview Green 4': 'Lakeview Green',\n",
        "            'Point Mckay': 'Point Mckay', 'Point Mckay West': 'Point Mckay', 'Point Mckay East': 'Point Mckay', 'Point Mckay Phase I': 'Point Mckay', 'Point Mckay Phase II': 'Point Mckay', 'Point Mckay Phase III': 'Point Mckay',\n",
        "            'KeyNote': 'KeyNote', 'KeyNote 2': 'KeyNote',\n",
        "            'The Guardian': 'The Guardian', 'Guardian North': 'The Guardian', 'Guardian South': 'The Guardian',\n",
        "            'Evolution': 'Evolution', 'Evolution Fuse': 'Evolution', 'Evolution Pulse': 'Evolution',\n",
        "            'University City': 'University City', 'University City I': 'University City', 'University City II': 'University City', 'University City III': 'University City', 'University City IV': 'University City',\n",
        "            'The Mosaic In McKenzie Town': 'The Mosaic',\n",
        "            'The Pavilions Of Eau Claire': 'The Pavilions Of Eau Claire',\n",
        "        }\n",
        "        logger.log_info(f\"Loaded {len(complex_mapping):,} specific building-to-complex mappings.\", also_print=False)\n",
        "\n",
        "        # Case-insensitive mapping for safety\n",
        "        case_insensitive_mapping = {\n",
        "            k.lower(): v for k, v in complex_mapping.items()\n",
        "        }\n",
        "\n",
        "        logger.log_info(\"Applying complex mapping to create 'Condo_Complex' column...\", also_print=True)\n",
        "        complex_df['Condo_Complex'] = complex_df['Condo_Name'].apply(\n",
        "            lambda x: case_insensitive_mapping.get(\n",
        "                str(x).lower() if pd.notna(x) else '',\n",
        "                x if pd.notna(x) else np.nan # Default: complex is same as building name\n",
        "            )\n",
        "        )\n",
        "        logger.log_info(\"'Condo_Complex' column created.\", also_print=False)\n",
        "\n",
        "        # --- (Your entire analysis and reporting logic from this cell goes here) ---\n",
        "\n",
        "        logger.log_info(\"\\nAnalyzing complex groupings...\", also_print=True)\n",
        "        multi_building_complexes_details = {}\n",
        "        valid_complexes_data = complex_df[complex_df['Condo_Complex'].notna()]\n",
        "\n",
        "        if not valid_complexes_data.empty:\n",
        "            for complex_name in sorted(valid_complexes_data['Condo_Complex'].unique()):\n",
        "                complex_mask = valid_complexes_data['Condo_Complex'] == complex_name\n",
        "                buildings_in_complex = valid_complexes_data[complex_mask]['Condo_Name'].unique()\n",
        "\n",
        "                if len(buildings_in_complex) > 1:\n",
        "                    multi_building_complexes_details[complex_name] = buildings_in_complex\n",
        "\n",
        "        # ... (rest of your analysis and print statements) ...\n",
        "\n",
        "        total_unique_complexes = complex_df['Condo_Complex'].nunique(dropna=True)\n",
        "        total_unique_buildings = complex_df['Condo_Name'].nunique(dropna=True)\n",
        "        num_multi_building_complexes = len(multi_building_complexes_details)\n",
        "\n",
        "        print(\"\\n--- Condo Complex Grouping Summary ---\")\n",
        "        summary_stats_data = [\n",
        "            [\"Total Unique Building Names (pre-complex grouping)\", f\"{total_unique_buildings:,}\"],\n",
        "            [\"Total Unique Condo Complexes (post-grouping)\", f\"{total_unique_complexes:,}\"],\n",
        "            [\"Complexes with Multiple Buildings\", f\"{num_multi_building_complexes:,}\"]\n",
        "        ]\n",
        "        print(tabulate(summary_stats_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nSummary Statistics (detailed):\\n{tabulate(summary_stats_data, headers=['Metric', 'Value'], tablefmt='plain')}\")\n",
        "\n",
        "        logger.log_info(\"Condo Complex grouping creation process completed.\", also_print=True)\n",
        "        return complex_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"PROCESS HALTED: Error in create_complex_column: {str(e)}\", also_print=True)\n",
        "        logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Condo Complex Grouping (Post-Automation)\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame):\n",
        "        raise NameError(\"Variable 'combined_data' not found.\")\n",
        "    if 'Condo_Name' not in combined_data.columns:\n",
        "        raise ValueError(\"Required column 'Condo_Name' not found.\")\n",
        "\n",
        "    # Update `combined_data` with the new 'Condo_Complex' column\n",
        "    combined_data = create_complex_column(combined_data)\n",
        "    logger.log_info(\"Updated 'combined_data' with new 'Condo_Complex' column.\", also_print=True)\n",
        "\n",
        "    print(\"\\n‚úÖ Condo complex grouping completed successfully! üéâ\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during condo complex grouping:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape}\")\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 21 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "WP9jXKEw9M4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDnc4ITOP_BT"
      },
      "source": [
        "# Training Data Filtering and Validation System\n",
        "## Overview\n",
        "This code block prepares data for machine learning by ensuring we have enough samples of each condo for reliable analysis. It's like ensuring we have enough examples of each condo type to make accurate predictions, similar to how a real estate agent needs multiple comparable properties for accurate valuations.\n",
        "\n",
        "## Technical Details\n",
        "### Input\n",
        "* DataFrame containing condo information\n",
        "* Required column: 'Condo_Name'\n",
        "* Configuration parameter: min_samples (default=10)\n",
        "* Current scale: Works with full MLS dataset\n",
        "\n",
        "### Output\n",
        "* filtered_df: Clean DataFrame containing only condos with sufficient samples\n",
        "* excluded_condos: Series containing information about excluded properties\n",
        "* Detailed statistics in log file\n",
        "\n",
        "### Key Operations\n",
        "1. Input Validation\n",
        "   * Verifies DataFrame format and required columns\n",
        "   * Validates min_samples parameter\n",
        "   * Handles edge cases safely\n",
        "\n",
        "2. Data Filtering\n",
        "   * Counts samples per condo\n",
        "   * Applies minimum sample threshold\n",
        "   * Creates clean filtered dataset\n",
        "\n",
        "3. Statistical Analysis\n",
        "   * Calculates retention rates\n",
        "   * Analyzes excluded data\n",
        "   * Provides detailed distribution reports\n",
        "\n",
        "## Code Review Findings\n",
        "1. Potential Improvements:\n",
        "   * Add validation for data quality (not just quantity)\n",
        "   * Consider adding a warning threshold for borderline cases\n",
        "   * Add option to export excluded condos list\n",
        "   * Consider adding data balance checks\n",
        "\n",
        "2. Safety Features Present:\n",
        "   * Input validation\n",
        "   * Error handling\n",
        "   * Data copying\n",
        "   * Detailed logging\n",
        "\n",
        "## Pipeline Implications\n",
        "This filtering step is crucial because:\n",
        "1. It affects machine learning model quality\n",
        "2. Impacts prediction reliability\n",
        "3. Influences which condos we can analyze\n",
        "\n",
        "## Next Steps\n",
        "After running this filter:\n",
        "1. Review excluded condos for patterns\n",
        "2. Consider adjusting min_samples threshold\n",
        "3. Analyze data balance\n",
        "4. Prepare filtered data for model training\n",
        "\n",
        "###### Console Output Guide\n",
        "Key information to watch for:\n",
        "```markdown\n",
        "* Initial data statistics\n",
        "* Filtering results\n",
        "* Retention percentages\n",
        "* Distribution of excluded condos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =================================================================================\n",
        "# TRAINING DATA FILTERING AND SAMPLE VALIDATION\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This code block prepares training data for machine learning by:\n",
        "1. Filtering out condos with insufficient samples (less than minimum threshold)\n",
        "2. Validating data quality and sample sizes\n",
        "3. Tracking excluded condos for analysis\n",
        "4. Providing detailed statistics about data retention\n",
        "\n",
        "Key features:\n",
        "- Uses minimum sample threshold (default: 10 samples)\n",
        "- Maintains data integrity with extensive validation\n",
        "- Provides detailed logging of filtering results\n",
        "- Analyzes distribution of excluded samples\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "import traceback\n",
        "\n",
        "def prepare_filtered_training_data(df: pd.DataFrame, min_samples: int = 10, top_n_excluded_dist: int = 10) -> tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Prepares training data by filtering out condos with insufficient samples for reliable analysis.\n",
        "    This helps ensure we have enough data points for each condo in our training set.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas DataFrame\n",
        "        The dataset containing condo information (now uses combined_data directly).\n",
        "        Must include 'Condo_Name' column.\n",
        "    min_samples : int\n",
        "        Minimum number of samples required for a condo to be included.\n",
        "        Condos with fewer samples will be excluded.\n",
        "    top_n_excluded_dist : int\n",
        "        Number of top sample counts to display in the excluded condos distribution table on console.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple:\n",
        "        filtered_df : pandas DataFrame\n",
        "            The filtered training dataset containing only condos with sufficient samples.\n",
        "        excluded_condos : pandas Series\n",
        "            Information about excluded condos and their sample counts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting training data filtering process...\", also_print=True)\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(f\"--- Training Data Filtering (Min Samples: {min_samples}) ---\")\n",
        "\n",
        "        if not isinstance(df, pd.DataFrame): # Changed from training_df to df\n",
        "            logger.log_error(\"Input 'df' must be a pandas DataFrame.\", also_print=True)\n",
        "            raise ValueError(\"df must be a pandas DataFrame\")\n",
        "\n",
        "        if 'Condo_Name' not in df.columns: # Changed from training_df to df\n",
        "            logger.log_error(\"DataFrame must contain 'Condo_Name' column for filtering.\", also_print=True)\n",
        "            raise ValueError(\"DataFrame must contain 'Condo_Name' column\")\n",
        "\n",
        "        cleaned_condo_names_series = df['Condo_Name'].astype(str).str.strip() # Changed from training_df to df\n",
        "\n",
        "        if not isinstance(min_samples, int) or min_samples < 1:\n",
        "            logger.log_error(\"min_samples must be a positive integer.\", also_print=True)\n",
        "            raise ValueError(\"min_samples must be a positive integer\")\n",
        "\n",
        "        condo_counts = cleaned_condo_names_series[cleaned_condo_names_series != 'nan'].value_counts()\n",
        "\n",
        "        print(\"\\n--- Initial Data Statistics ---\")\n",
        "        initial_stats_data = [\n",
        "            [\"Total Properties in Dataset\", f\"{len(df):,}\"], # Changed from training_df to df\n",
        "            [\"Total Unique Condo Names (non-NaN, non-'nan' string)\", f\"{len(condo_counts):,}\"]\n",
        "        ]\n",
        "        print(tabulate(initial_stats_data, headers=[\"Metric\", \"Count\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nInitial Data Statistics (detailed):\\n{tabulate(initial_stats_data, headers=['Metric', 'Count'], tablefmt='plain')}\")\n",
        "\n",
        "        valid_condos_for_filter = condo_counts[condo_counts >= min_samples].index\n",
        "\n",
        "        valid_records_mask = df['Condo_Name'].isin(valid_condos_for_filter) # Changed from training_df to df\n",
        "        filtered_df = df[valid_records_mask].copy() # Changed from training_df to df\n",
        "\n",
        "        excluded_condos = condo_counts[condo_counts < min_samples]\n",
        "\n",
        "        original_unique_condo_names = len(condo_counts)\n",
        "        remaining_unique_condo_names = len(valid_condos_for_filter)\n",
        "        excluded_unique_condo_names_count = len(excluded_condos)\n",
        "\n",
        "        original_total_samples = len(df) # Changed from training_df to df\n",
        "        remaining_total_samples = len(filtered_df)\n",
        "        excluded_total_samples = original_total_samples - remaining_total_samples\n",
        "\n",
        "        print(\"\\n--- Filtering Results (Min Samples: {}) ---\".format(min_samples))\n",
        "        filtering_results_data = [\n",
        "            [\"Category\", \"Original\", \"Retained\", \"Excluded\", \"% Retained\", \"% Excluded\"],\n",
        "            [\"Condo Names\", f\"{original_unique_condo_names:,}\", f\"{remaining_unique_condo_names:,}\", f\"{excluded_unique_condo_names_count:,}\",\n",
        "                f\"{(remaining_unique_condo_names/original_unique_condo_names*100):.1f}%\", f\"{(excluded_unique_condo_names_count/original_unique_condo_names*100):.1f}%\"],\n",
        "            [\"Total Samples\", f\"{original_total_samples:,}\", f\"{remaining_total_samples:,}\", f\"{excluded_total_samples:,}\",\n",
        "                f\"{(remaining_total_samples/original_total_samples*100):.1f}%\", f\"{(excluded_total_samples/original_total_samples*100):.1f}%\"]\n",
        "        ]\n",
        "        print(tabulate(filtering_results_data, headers=\"firstrow\", tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nFiltering Results (detailed):\\n{tabulate(filtering_results_data, headers='firstrow', tablefmt='plain')}\")\n",
        "\n",
        "        print(\"\\n--- Distribution of Excluded Condos by Sample Count ---\")\n",
        "        if not excluded_condos.empty:\n",
        "            excluded_dist_by_count = excluded_condos.value_counts().sort_index(ascending=False)\n",
        "\n",
        "            excluded_dist_table_data = []\n",
        "            for samples_per_condo, num_condos in excluded_dist_by_count.head(top_n_excluded_dist).items():\n",
        "                excluded_dist_table_data.append([f\"{samples_per_condo}\", f\"{num_condos:,}\"])\n",
        "\n",
        "            print(tabulate(excluded_dist_table_data, headers=[\"Samples per Condo\", \"Number of Condos\"], tablefmt=\"grid\"))\n",
        "            logger.log_info(f\"\\nDistribution of Excluded Condos (detailed):\\n{excluded_dist_by_count.to_string()}\")\n",
        "\n",
        "        else:\n",
        "            print(\"No condos were excluded based on the minimum sample threshold.\")\n",
        "            logger.log_info(\"No condos were excluded based on the minimum sample threshold.\")\n",
        "\n",
        "        logger.log_info(\"Training data filtering process completed.\", also_print=True)\n",
        "        return filtered_df, excluded_condos\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error in prepare_filtered_training_data: {str(e)}\", also_print=True)\n",
        "        logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "        raise\n",
        "\n",
        "# Main Execution Block\n",
        "try:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Training Data Filtering and Sample Validation\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty. Please run previous data preparation cells.\")\n",
        "\n",
        "    if 'Condo_Name' not in combined_data.columns:\n",
        "        logger.log_error(\"Required column 'Condo_Name' not found in 'combined_data' DataFrame. Cannot proceed.\", also_print=True)\n",
        "        raise ValueError(\"Required column 'Condo_Name' not found in 'combined_data' DataFrame. Cannot proceed with training data filtering.\")\n",
        "\n",
        "    MIN_SAMPLES = 10\n",
        "\n",
        "    # Update `combined_data` with the filtered DataFrame.\n",
        "    combined_data, excluded_condo_series_info = prepare_filtered_training_data(\n",
        "        df=combined_data,\n",
        "        min_samples=MIN_SAMPLES,\n",
        "        top_n_excluded_dist=10\n",
        "    )\n",
        "\n",
        "    logger.log_info(\"Updated 'combined_data' variable with filtered dataset.\", also_print=True)\n",
        "\n",
        "    print(\"\\n‚úÖ Training data filtering and sample validation completed successfully! üéâ\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during training data filtering:\", also_print=True)\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "rtKC6ISoHAo1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03GLkxR7QFIk"
      },
      "source": [
        "# Machine Learning Feature Preparation System\n",
        "## Overview\n",
        "This code block transforms raw condo data into a format suitable for machine learning by creating a feature matrix and preparing target variables. Think of it as converting our real estate data into a format that a machine learning model can understand and learn from.\n",
        "\n",
        "## Technical Details\n",
        "### Input\n",
        "* DataFrame containing standardized condo data\n",
        "* Required columns:\n",
        "  - 'Condo_Name' (target variable)\n",
        "  - 'Year_Built' (numerical feature)\n",
        "  - 'Latitude', 'Longitude' (location features)\n",
        "  - 'Postal_Code', 'Subdivision_Name', 'Condo_Complex' (categorical features)\n",
        "* Configuration: min_samples threshold (default=10)\n",
        "\n",
        "### Output\n",
        "* X: Feature matrix (dummy variables for model training)\n",
        "* y: Target variable series (Condo_Name predictions)\n",
        "* feature_info: Dictionary containing feature statistics and memory usage\n",
        "\n",
        "### Key Operations\n",
        "1. Data Validation and Filtering\n",
        "   * Verifies all required columns exist\n",
        "   * Filters out condos with insufficient samples\n",
        "   * Creates working copy to preserve original data\n",
        "\n",
        "2. Numerical Feature Processing\n",
        "   * Converts Year_Built to numeric format\n",
        "   * Handles missing values using median imputation\n",
        "   * Validates data ranges\n",
        "\n",
        "3. Location Feature Engineering\n",
        "   * Creates location bins for nearby properties\n",
        "   * Generates unique location identifiers\n",
        "   * Handles missing coordinate data\n",
        "\n",
        "4. Feature Matrix Creation\n",
        "   * Generates dummy variables for categorical features\n",
        "   * Uses sparse matrices to optimize memory\n",
        "   * Creates comprehensive feature information dictionary\n",
        "\n",
        "## Code Review Findings\n",
        "1. Potential Improvements:\n",
        "   * Consider adding feature scaling/normalization\n",
        "   * Implement feature selection for high-cardinality categories\n",
        "   * Add correlation analysis between features\n",
        "   * Consider adding cross-validation setup\n",
        "\n",
        "2. Memory Optimization Opportunities:\n",
        "   * Monitor memory usage for large datasets\n",
        "   * Consider chunking for very large datasets\n",
        "   * Implement feature selection to reduce dimensionality\n",
        "\n",
        "## Pipeline Implications\n",
        "This preparation step is crucial because:\n",
        "1. Feature quality directly impacts model performance\n",
        "2. Memory usage affects scalability\n",
        "3. Proper encoding ensures model understands the data\n",
        "\n",
        "## Next Steps\n",
        "After this preparation:\n",
        "1. Review feature importance\n",
        "2. Consider feature selection\n",
        "3. Evaluate memory usage\n",
        "4. Begin model training\n",
        "\n",
        "###### Console Output Guide\n",
        "Watch for these key messages:\n",
        "```markdown\n",
        "* Missing required columns\n",
        "* Sample size filtering results\n",
        "* Feature creation statistics\n",
        "* Memory usage warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =================================================================================\n",
        "# FEATURE ENGINEERING AND TRAINING MATRIX PREPARATION (REVISED)\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This code block prepares the final feature matrix for machine learning by:\n",
        "1. Creating and processing features from various condo characteristics.\n",
        "2. Converting categorical variables into dummy variables.\n",
        "3. Handling missing values and data type conversions.\n",
        "4. Creating location-based binning features directly on the main DataFrame.\n",
        "5. Preparing both feature matrix (X) and target variable (y).\n",
        "\n",
        "Data Processing Steps:\n",
        "1. Input validation and filtering (re-application of min_samples for robustness).\n",
        "2. Numerical feature processing (Year_Built).\n",
        "3. Location feature creation (coordinate binning) on combined_data.\n",
        "4. Categorical feature encoding (dummy variables) for the feature matrix X.\n",
        "5. Final matrix assembly and preparation of target variable y.\n",
        "\n",
        "Key Features:\n",
        "-   Uses sparse matrices to save memory for one-hot encoded features.\n",
        "-   Creates location bins for granular geographic features.\n",
        "-   Handles missing values appropriately for each feature type.\n",
        "-   Provides detailed logging of feature creation process.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "from sklearn.model_selection import train_test_split\n",
        "import traceback\n",
        "\n",
        "def prepare_training_data(df: pd.DataFrame, min_samples: int = 10) -> tuple[pd.DataFrame, pd.Series, dict]:\n",
        "    \"\"\"\n",
        "    Prepares data for training the condo name prediction model by creating\n",
        "    a feature matrix from various condo characteristics.\n",
        "\n",
        "    This function:\n",
        "    1. Filters out condos with insufficient samples.\n",
        "    2. Processes numerical features (Year_Built).\n",
        "    3. Creates location-based features (coordinate bins) directly on the input DataFrame.\n",
        "    4. Generates dummy variables (one-hot encoding) for categorical features.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df (pd.DataFrame): The master dataset containing standardized condo information (expected combined_data).\n",
        "                         Required columns are listed internally.\n",
        "    min_samples (int): Minimum number of samples required for a condo to be included\n",
        "                         in the training data. Condos with fewer samples will be excluded.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X (pd.DataFrame): Feature matrix with one-hot encoded dummy variables (sparse DataFrame).\n",
        "    y (pd.Series): Target variable (Condo_Name to predict).\n",
        "    feature_info (dict): Information about generated features, including counts and memory.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting training data preparation for ML...\", also_print=True)\n",
        "\n",
        "        # Step 1: Input Validation and Filtering (Re-apply min_samples filter for robustness)\n",
        "        # ---------------------------------------------------------------------------------\n",
        "        required_columns = [\n",
        "            'Condo_Name', 'Year_Built', 'Latitude', 'Longitude',\n",
        "            'Postal_Code', 'Subdivision_Name', 'Condo_Complex'\n",
        "        ]\n",
        "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            logger.log_error(f\"Missing required columns for feature engineering: {', '.join(missing_cols)}\", also_print=True)\n",
        "            raise ValueError(f\"Missing required columns for feature engineering: {', '.join(missing_cols)}\")\n",
        "\n",
        "        logger.log_info(\"Input validation complete. All required columns present.\", also_print=False)\n",
        "\n",
        "        print(\"\\n--- Step 1: Filtering for sufficient samples ---\")\n",
        "        condo_counts = df['Condo_Name'].value_counts()\n",
        "        valid_condos = condo_counts[condo_counts >= min_samples].index\n",
        "\n",
        "        filtering_summary_data = [\n",
        "            [\"Total unique condo names (initial)\", f\"{len(condo_counts):,}\"],\n",
        "            [\"Unique condos with >= {} samples\".format(min_samples), f\"{len(valid_condos):,}\"],\n",
        "            [\"Unique condos excluded\", f\"{len(condo_counts) - len(valid_condos):,}\"]\n",
        "        ]\n",
        "        print(tabulate(filtering_summary_data, headers=[\"Metric\", \"Count\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nFiltering Summary (detailed):\\n{tabulate(filtering_summary_data, headers='firstrow', tablefmt='plain')}\")\n",
        "\n",
        "        # Filter the DataFrame to include only records from valid condos for feature engineering\n",
        "        working_df = df[df['Condo_Name'].isin(valid_condos)].copy() # Use .copy() to ensure independent df\n",
        "        logger.log_info(f\"Working dataset size after filtering: {len(working_df):,} records.\", also_print=True)\n",
        "        if working_df.empty:\n",
        "            logger.log_error(\"No records remaining after filtering for minimum samples. Adjust `min_samples`.\", also_print=True)\n",
        "            raise ValueError(\"No records remaining after filtering for minimum samples. Adjust `min_samples`.\")\n",
        "\n",
        "        # Step 2: Process Numerical Features (Year_Built)\n",
        "        # ------------------------------------------------\n",
        "        print(\"\\n--- Step 2: Processing numerical features (Year_Built) ---\")\n",
        "\n",
        "        working_df['Year_Built'] = pd.to_numeric(working_df['Year_Built'], errors='coerce')\n",
        "        year_nan_count = working_df['Year_Built'].isna().sum()\n",
        "        if year_nan_count > 0:\n",
        "            year_median = working_df['Year_Built'].median()\n",
        "            working_df['Year_Built'] = working_df['Year_Built'].fillna(year_median).astype('int32')\n",
        "            logger.log_info(f\"Filled {year_nan_count:,} missing Year_Built values with median ({year_median:.0f}).\", also_print=True)\n",
        "        else:\n",
        "            working_df['Year_Built'] = working_df['Year_Built'].astype('int32')\n",
        "            logger.log_info(\"No missing Year_Built values found. Converting to int32.\", also_print=True)\n",
        "\n",
        "        logger.log_info(f\"Year_Built range after processing: {working_df['Year_Built'].min()} to {working_df['Year_Built'].max()}.\", also_print=True)\n",
        "\n",
        "\n",
        "        # Step 3: Process Location Features (Coordinate Binning)\n",
        "        # ----------------------------------------------------\n",
        "        print(\"\\n--- Step 3: Processing location features (Binning) ---\")\n",
        "\n",
        "        for coord_col in ['Latitude', 'Longitude']:\n",
        "            working_df[coord_col] = pd.to_numeric(working_df[coord_col], errors='coerce')\n",
        "            nan_count = working_df[coord_col].isna().sum()\n",
        "            if nan_count > 0:\n",
        "                median_coord = working_df[coord_col].median()\n",
        "                working_df[coord_col] = working_df[coord_col].fillna(median_coord)\n",
        "                logger.log_warning(f\"Filled {nan_count:,} missing {coord_col} values with median ({median_coord:.4f}) for binning.\", also_print=True)\n",
        "            else:\n",
        "                logger.log_info(f\"No missing {coord_col} values found.\", also_print=False)\n",
        "\n",
        "        ### REVISED CODE ###\n",
        "        # Create location bins by rounding coordinates to 2 decimal places (~1km precision) to reduce dimensionality.\n",
        "        working_df['lat_bin'] = working_df['Latitude'].round(2)\n",
        "        working_df['lon_bin'] = working_df['Longitude'].round(2)\n",
        "\n",
        "        # Create unique location identifier string\n",
        "        working_df['location_id'] = (\n",
        "            working_df['lat_bin'].astype(str) + '_' +\n",
        "            working_df['lon_bin'].astype(str)\n",
        "        )\n",
        "\n",
        "        logger.log_info(f\"Created {working_df['location_id'].nunique():,} unique location bins.\", also_print=True)\n",
        "\n",
        "        # Step 4: Create Feature Matrix (Mixed Numerical and Categorical)\n",
        "        # ---------------------------------------------------------------\n",
        "        print(\"\\n--- Step 4: Creating Feature Matrix ---\")\n",
        "\n",
        "        ### REVISED CODE ###\n",
        "        # Year_Built is now treated as a numerical feature and is not one-hot encoded.\n",
        "        feature_columns_to_encode = [\n",
        "            'Postal_Code',\n",
        "            'Subdivision_Name',\n",
        "            'location_id',       # Location ID from binning\n",
        "            'Condo_Complex'      # Condo Complex grouping\n",
        "        ]\n",
        "\n",
        "        # Initialize list to hold all our feature dataframes\n",
        "        feature_frames = []\n",
        "\n",
        "        feature_info = {\n",
        "            'columns': {},\n",
        "            'total_features': 0,\n",
        "            'memory_usage_mb': 0\n",
        "        }\n",
        "\n",
        "        # Manually add the numerical Year_Built feature first\n",
        "        print(\"Processing numerical feature: 'Year_Built'\")\n",
        "        # Ensure index is aligned for future concatenation\n",
        "        year_built_df = working_df[['Year_Built']].reset_index(drop=True)\n",
        "        feature_frames.append(year_built_df)\n",
        "        feature_info['columns']['Year_Built'] = {\n",
        "            'unique_values': working_df['Year_Built'].nunique(),\n",
        "            'features_created': 1, # It's just one numerical feature\n",
        "            'missing_values_handled': year_nan_count\n",
        "        }\n",
        "\n",
        "        # Now, process and one-hot encode the categorical features\n",
        "        for column in feature_columns_to_encode:\n",
        "            print(f\"Processing column for encoding: '{column}'\", end='\\r')\n",
        "            logger.log_info(f\"\\nProcessing column '{column}' for one-hot encoding...\", also_print=False)\n",
        "\n",
        "            missing_val_in_feature = working_df[column].isna().sum()\n",
        "            if missing_val_in_feature > 0:\n",
        "                logger.log_warning(f\"Missing values ({missing_val_in_feature:,}) in '{column}'. `get_dummies` will create a '{column}_nan' category.\", also_print=True)\n",
        "\n",
        "            dummies = pd.get_dummies(\n",
        "                working_df[column],\n",
        "                prefix=column,\n",
        "                sparse=True\n",
        "            )\n",
        "            # Ensure index alignment before adding to the list\n",
        "            feature_frames.append(dummies.reset_index(drop=True))\n",
        "\n",
        "            feature_info['columns'][column] = {\n",
        "                'unique_values': working_df[column].nunique(dropna=False),\n",
        "                'features_created': len(dummies.columns),\n",
        "                'missing_values_handled': missing_val_in_feature\n",
        "            }\n",
        "            logger.log_info(f\"Created {len(dummies.columns):,} features from '{column}'. Unique values: {working_df[column].nunique(dropna=False):,}.\", also_print=False)\n",
        "\n",
        "        # Combine all generated feature frames into the final feature matrix X\n",
        "        X = pd.concat(feature_frames, axis=1)\n",
        "\n",
        "        # Ensure all column names are strings to prevent errors in some ML libraries\n",
        "        X.columns = X.columns.astype(str)\n",
        "\n",
        "        # Create the target variable y (the Condo_Name to predict)\n",
        "        y = working_df['Condo_Name']\n",
        "\n",
        "        feature_info['total_features'] = X.shape[1]\n",
        "        feature_info['memory_usage_mb'] = X.memory_usage().sum() / (1024 * 1024)\n",
        "\n",
        "        print(\"\\n--- Final Feature Matrix (X) and Target (y) Statistics ---\")\n",
        "        final_stats_data = [\n",
        "            [\"Feature Matrix (X) Shape\", f\"{X.shape}\"],\n",
        "            [\"Number of Target Classes (y)\", f\"{y.nunique():,}\"],\n",
        "            [\"Total Features Created\", f\"{feature_info['total_features']:,}\"],\n",
        "            [\"Memory Usage (X)\", f\"{feature_info['memory_usage_mb']:.2f} MB\"]\n",
        "        ]\n",
        "        print(tabulate(final_stats_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nFinal Dataset Statistics (detailed):\\n{tabulate(final_stats_data, headers=['Metric', 'Value'], tablefmt='plain')}\")\n",
        "\n",
        "        logger.log_info(\"Training data feature engineering complete.\", also_print=True)\n",
        "        return X, y, feature_info\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"PROCESS HALTED: Error in prepare_training_data: {str(e)}\", also_print=True)\n",
        "        logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# Main Execution Block\n",
        "# =================================================================================\n",
        "try:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Feature Engineering and Training Matrix Preparation (REVISED)\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty. Please run previous data preparation cells.\")\n",
        "\n",
        "    required_overall_cols = [\n",
        "        'Condo_Name', 'Year_Built', 'Latitude', 'Longitude',\n",
        "        'Postal_Code', 'Subdivision_Name', 'Condo_Complex'\n",
        "    ]\n",
        "    for col in required_overall_cols:\n",
        "        if col not in combined_data.columns:\n",
        "            logger.log_error(f\"Missing required column in 'combined_data': '{col}'. Ensure previous cells are run correctly.\", also_print=True)\n",
        "            raise ValueError(f\"Missing required column in 'combined_data': '{col}'. Ensure previous cells are run correctly.\")\n",
        "\n",
        "\n",
        "    MIN_SAMPLES_FOR_FE = 10\n",
        "\n",
        "    # X, y are the final feature matrix and target.\n",
        "    X, y, feature_info = prepare_training_data(combined_data, min_samples=MIN_SAMPLES_FOR_FE)\n",
        "\n",
        "\n",
        "    print(\"\\n--- Detailed Feature Information ---\")\n",
        "    logger.log_info(\"Detailed feature breakdown (unique values, features created, missing values) is available in the log file.\", also_print=True)\n",
        "\n",
        "    detailed_feature_breakdown_data = []\n",
        "    # Sort feature_info to show Year_Built first\n",
        "    sorted_columns = ['Year_Built'] + [col for col in feature_info['columns'] if col != 'Year_Built']\n",
        "    for column in sorted_columns:\n",
        "        info = feature_info['columns'][column]\n",
        "        detailed_feature_breakdown_data.append([\n",
        "            column,\n",
        "            f\"{info['unique_values']:,}\",\n",
        "            f\"{info['features_created']:,}\",\n",
        "            f\"{info['missing_values_handled']:,}\" if info['missing_values_handled'] > 0 else \"0\"\n",
        "        ])\n",
        "    logger.log_info(f\"\\nDetailed Feature Breakdown per Column:\\n{tabulate(detailed_feature_breakdown_data, headers=['Feature Column', 'Unique Values', 'Features Created', 'Missing Handled'], tablefmt='plain')}\")\n",
        "\n",
        "\n",
        "    print(\"\\n‚úÖ Feature engineering and training matrix preparation completed successfully! üéâ\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during feature engineering:\", also_print=True)\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "Kaw_mhdrHItT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "-----\n",
        "\n",
        "### **CONDO NAME CLASSIFICATION MODEL TRAINING (Full Pipeline with CV & Tuning)**\n",
        "\n",
        "This code block is the heart of your machine learning pipeline for predicting `Condo_Name`. It takes your prepared features (`X`) and target variable (`y`) and trains a robust Random Forest classifier.\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "The primary goal of this block is to:\n",
        "\n",
        "1.  **Train a Machine Learning Model**: Specifically, a `RandomForestClassifier`, to accurately predict the `Condo_Name` based on the features you engineered.\n",
        "2.  **Evaluate Robustly**: Use **Stratified K-Fold Cross-Validation** to get a reliable estimate of the model's performance across different subsets of your data, ensuring the results aren't just \"lucky\" for one split.\n",
        "3.  **Optimize Performance**: Optionally run **Hyperparameter Tuning (Randomized Search)** to automatically find the best combination of model settings for your specific dataset, maximizing accuracy and generalization.\n",
        "4.  **Enable Reusability**: **Save the trained model and its optimal parameters** to your Google Drive, so you only have to run the computationally intensive tuning process *once*. For monthly updates, you can then load the pre-tuned parameters and just train the model quickly.\n",
        "5.  **Address Imbalance**: Use `class_weight='balanced'` in the Random Forest to improve performance on condo names with fewer historical sales.\n",
        "\n",
        "-----\n",
        "\n",
        "#### **How to Use This Block**\n",
        "\n",
        "1.  **Ensure Prerequisites**:\n",
        "\n",
        "      * Make sure all previous data loading, cleaning, and **Feature Engineering and Training Matrix Preparation** blocks have been run successfully. This block relies on the `X` (feature matrix) and `y` (target variable) DataFrames being present in your Colab environment.\n",
        "\n",
        "2.  **Configure the `CONFIG` Dictionary**:\n",
        "    The most important part is the `CONFIG` dictionary at the top of the code block. **Adjust these settings based on whether this is your initial run or a monthly update:**\n",
        "\n",
        "    ```python\n",
        "    CONFIG = {\n",
        "        \"RUN_HYPERPARAMETER_TUNING\": True,  # <--- SET THIS:\n",
        "                                            #      - True: For the FIRST, one-time run (VERY LONG!)\n",
        "                                            #      - False: For subsequent monthly runs (much faster)\n",
        "        \"SAVE_MODEL_AND_PARAMS\": True,      # Set to True to save the trained model and best parameters to Drive\n",
        "        \"MODEL_SAVE_DIR\": \"/content/drive/My Drive/Realtor/Models\", # **Verify this path exists in your Drive**\n",
        "        \"MODEL_FILENAME_PREFIX\": \"condo_name_classifier\", # Base name for your saved files\n",
        "\n",
        "        \"N_FOLDS_CV\": 5,                    # Number of folds for cross-validation (e.g., 5 or 10)\n",
        "        \"RANDOM_STATE\": 42,                 # For reproducibility\n",
        "        \"N_ITER_RANDOM_SEARCH\": 20,         # Number of parameter combinations to try during tuning.\n",
        "                                            # - For initial testing: 5-10 iterations.\n",
        "                                            # - For a thorough search: 50-100+ iterations (will be very long).\n",
        "        \"DEFAULT_MODEL_PARAMS\": {           # Parameters used if tuning is OFF and no saved params are found.\n",
        "            'n_estimators': 150,\n",
        "            'max_depth': 15,\n",
        "            'min_samples_leaf': 3,\n",
        "            'min_samples_split': 5,\n",
        "            'max_features': 'sqrt',\n",
        "            'criterion': 'gini', # Added 'criterion' for more complete default params matching possible tuning options\n",
        "            'class_weight': 'balanced',\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1,\n",
        "            'verbose': 0\n",
        "        }\n",
        "    }\n",
        "    ```\n",
        "\n",
        "      * **`RUN_HYPERPARAMETER_TUNING`**:\n",
        "          * Set to `True` for your **first, initial run**. This will trigger the lengthy parameter search.\n",
        "          * Set to `False` for **all subsequent monthly runs**. This will load the previously found best parameters and just train the model quickly.\n",
        "      * **`MODEL_SAVE_DIR`**: **Crucially, make sure this directory exists in your Google Drive.** The code will attempt to create it, but it's good to double-check.\n",
        "\n",
        "3.  **Run the Cell**:\n",
        "\n",
        "      * Simply execute the code cell.\n",
        "\n",
        "-----\n",
        "\n",
        "#### **Expected Output and Learning Points**\n",
        "\n",
        "You will see detailed output in your Colab console, guiding you through the process:\n",
        "\n",
        "  * **Dataset Information**: Summary of your `X` and `y` data.\n",
        "  * **Hyperparameter Tuning Results (If `RUN_HYPERPARAMETER_TUNING` is `True`)**:\n",
        "      * A `WARNING` about the long runtime.\n",
        "      * Progress updates from `RandomizedSearchCV`.\n",
        "      * A summary table showing the `Best F1-Macro Score (CV)`, `Tuning Duration`, and the `Best Parameters Found`.\n",
        "      * This is what you'll **copy and paste into your `DEFAULT_MODEL_PARAMS` or load from the saved JSON file** for future runs.\n",
        "  * **Cross-Validation Results**:\n",
        "      * A summary table showing the `Mean Score` and `Std Dev` for Accuracy, F1-Macro, and F1-Weighted across all `N_FOLDS_CV` folds. This is your most reliable estimate of how well your model generalizes.\n",
        "      * This step always runs, providing consistent performance monitoring.\n",
        "  * **Final Model Training**: Confirmation that the final model is being trained on your entire dataset using the chosen parameters.\n",
        "  * **Model Saving (If `SAVE_MODEL_AND_PARAMS` is `True`)**: Confirmation of where the model and parameters are saved in your Google Drive.\n",
        "\n",
        "**Key Learning Points:**\n",
        "\n",
        "  * **Reproducibility (`RANDOM_STATE`)**: Notice how `random_state` is consistently used throughout for splits and model initialization. This ensures that your results are reproducible.\n",
        "  * **Class Imbalance (`class_weight='balanced'`)**: This parameter is vital for multi-class classification where some categories have many more samples than others. It helps the model learn from less frequent condo names.\n",
        "  * **Cross-Validation Power**: By seeing average performance across multiple folds, you get a much more trustworthy picture of your model's real-world effectiveness than from a single train/test split.\n",
        "  * **Tuning for Performance**: Hyperparameter tuning systematically explores different model configurations to find the sweet spot for your data.\n",
        "  * **Model Persistence for Efficiency**: Once tuned, saving and loading the model and its parameters saves massive amounts of time on subsequent runs. You've effectively separated the \"optimization\" phase from the \"operational\" phase of your pipeline.\n",
        "\n",
        "This block is designed to give you both a highly performant model and a deep understanding of its training and evaluation process."
      ],
      "metadata": {
        "id": "UBl--WrOQv_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =================================================================================\n",
        "# CONDO NAME CLASSIFICATION MODEL TRAINING (Full Pipeline with CV & Tuning)\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This code block trains a machine learning model to predict condo names,\n",
        "incorporating cross-validation, optional hyperparameter tuning, and model persistence.\n",
        "\n",
        "Key features:\n",
        "1.  **Configurable Execution**: Toggle hyperparameter tuning and model saving.\n",
        "2.  **Cross-Validation (Stratified K-Fold)**: Provides robust and reliable\n",
        "    performance estimates by training and evaluating the model across multiple folds.\n",
        "3.  **Hyperparameter Tuning (Randomized Search - Manual Iteration)**: Searches form\n",
        "    optimal model parameters, providing granular progress updates and checkpointing\n",
        "    the best parameters found so far. Designed as a one-time intensive process.\n",
        "4.  **Model Persistence**: Saves the trained model and its best parameters to disk,\n",
        "    allowing for quick loading and reuse in subsequent runs without retraining.\n",
        "5.  **Class Imbalance Handling**: Uses 'balanced' class weighting for RandomForest.\n",
        "6.  **Detailed Logging and Output**: Provides comprehensive insights into\n",
        "    the training process, CV results, and tuning outcomes.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "import traceback\n",
        "import time\n",
        "import joblib # For saving and loading models/parameters\n",
        "import json   # For saving and loading parameters as JSON\n",
        "from collections import defaultdict # For collecting tuning results\n",
        "\n",
        "# Import scikit-learn components\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate # Removed RandomizedSearchCV import as we're implementing it manually\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, make_scorer\n",
        "from sklearn.inspection import permutation_importance # Optional: for feature importance after training\n",
        "\n",
        "# For randomized search parameter sampling\n",
        "from scipy.stats import randint, uniform # For continuous and integer distributions\n",
        "import random # For sampling from lists\n",
        "\n",
        "# =================================================================================\n",
        "# CONFIGURATION - ADJUST THESE PARAMETERS AS NEEDED\n",
        "# =================================================================================\n",
        "CONFIG = {\n",
        "    \"RUN_HYPERPARAMETER_TUNING\": True,  # Set to True for initial tuning run (LONG!)\n",
        "                                        # Set to False for subsequent monthly runs (uses saved params)\n",
        "    \"SAVE_MODEL_AND_PARAMS\": True,      # Set to True to save the trained model and best parameters\n",
        "    \"MODEL_SAVE_DIR\": \"/content/drive/My Drive/Realtor/Models\", # Directory to save model artifacts\n",
        "    \"MODEL_FILENAME_PREFIX\": \"condo_name_classifier\", # Prefix for model and params files\n",
        "\n",
        "    \"N_FOLDS_CV\": 5,                    # Number of folds for cross-validation\n",
        "    \"RANDOM_STATE\": 42,                 # Seed for reproducibility\n",
        "    \"N_ITER_RANDOM_SEARCH\": 20,         # Number of parameter settings that are sampled (for manual Randomized Search)\n",
        "                                        # HINT: Start with small number (e.g., 2-5) for quick test, then increase (e.g., 20-50) for thorough search.\n",
        "    \"DEFAULT_MODEL_PARAMS\": {           # Used if RUN_HYPERPARAMETER_TUNING is False AND no saved params are found\n",
        "        'n_estimators': 150,\n",
        "        'max_depth': 15,\n",
        "        'min_samples_leaf': 3,\n",
        "        'min_samples_split': 5,\n",
        "        'max_features': 'sqrt',\n",
        "        'criterion': 'gini',\n",
        "        'class_weight': 'balanced',\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'verbose': 0\n",
        "    },\n",
        "    \"TUNING_PARAM_DIST\": {              # Parameter distribution for manual Randomized Search\n",
        "        'n_estimators': [100, 150, 200, 250], # Sample from these specific values\n",
        "        'max_depth': [10, 20, 30, None],\n",
        "        'min_samples_leaf': [1, 2, 4, 8],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'max_features': ['sqrt', 'log2', 0.5, 0.7],\n",
        "        'criterion': ['gini', 'entropy']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Ensure model save directory exists\n",
        "import os\n",
        "os.makedirs(CONFIG[\"MODEL_SAVE_DIR\"], exist_ok=True)\n",
        "MODEL_SAVE_PATH = os.path.join(CONFIG[\"MODEL_SAVE_DIR\"], CONFIG[\"MODEL_FILENAME_PREFIX\"] + \"_model.joblib\")\n",
        "PARAMS_SAVE_PATH = os.path.join(CONFIG[\"MODEL_SAVE_DIR\"], CONFIG[\"MODEL_FILENAME_PREFIX\"] + \"_best_params.json\")\n",
        "TUNING_LOG_PATH = os.path.join(CONFIG[\"MODEL_SAVE_DIR\"], CONFIG[\"MODEL_FILENAME_PREFIX\"] + \"_tuning_log.json\") # New: for tuning progress\n",
        "\n",
        "# =================================================================================\n",
        "# Helper Functions for Model Persistence\n",
        "# =================================================================================\n",
        "\n",
        "def save_model_and_params(model, params, model_path, params_path):\n",
        "    \"\"\"Saves the trained model and its best parameters to disk.\"\"\"\n",
        "    try:\n",
        "        joblib.dump(model, model_path)\n",
        "        with open(params_path, 'w') as f:\n",
        "            json.dump(params, f)\n",
        "        logger.log_info(f\"Model saved to: {model_path}\", also_print=True)\n",
        "        logger.log_info(f\"Best parameters saved to: {params_path}\", also_print=True)\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error saving model or parameters: {e}\", also_print=True)\n",
        "        logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "\n",
        "def load_model_and_params(model_path, params_path):\n",
        "    \"\"\"Loads a trained model and its best parameters from disk.\"\"\"\n",
        "    try:\n",
        "        model = joblib.load(model_path)\n",
        "        with open(params_path, 'r') as f:\n",
        "            params = json.load(f)\n",
        "        logger.log_info(f\"Model loaded from: {model_path}\", also_print=True)\n",
        "        logger.log_info(f\"Parameters loaded from: {params_path}\", also_print=True)\n",
        "        return model, params\n",
        "    except FileNotFoundError:\n",
        "        logger.log_warning(f\"Model or parameters not found at {model_path} / {params_path}. Will use default/newly tuned parameters.\", also_print=True)\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"Error loading model or parameters: {e}\", also_print=True)\n",
        "        logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "        return None, None\n",
        "\n",
        "# =================================================================================\n",
        "# Main Training and Evaluation Function\n",
        "# =================================================================================\n",
        "\n",
        "def train_condo_classifier_full_pipeline(X: pd.DataFrame, y: pd.Series):\n",
        "    \"\"\"\n",
        "    Orchestrates the full model training pipeline including optional hyperparameter tuning,\n",
        "    cross-validation for robust evaluation, and persistence.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X (pd.DataFrame): Full feature matrix for training and evaluation.\n",
        "    y (pd.Series): Full target variable for training and evaluation.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple:\n",
        "        final_model (RandomForestClassifier): The final model trained on the full dataset.\n",
        "        best_params_used (dict): The hyperparameters used for the final model.\n",
        "        cv_results (dict): Dictionary containing cross-validation results.\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Starting Condo Name Classification Full Pipeline...\", also_print=True)\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"--- Condo Name Classification Model Training (Full Pipeline) ---\")\n",
        "\n",
        "    # Input Validation\n",
        "    if X.empty or y.empty:\n",
        "        logger.log_error(\"Input data (X or y) is empty. Cannot proceed.\", also_print=True)\n",
        "        raise ValueError(\"Input data (X or y) is empty. Cannot proceed.\")\n",
        "    if X.shape[1] == 0:\n",
        "        logger.log_error(\"Feature matrix (X) has 0 columns. Check feature engineering step.\", also_print=True)\n",
        "        raise ValueError(\"Feature matrix (X) has 0 columns. Check feature engineering step.\")\n",
        "    if y.nunique() < 2:\n",
        "        logger.log_error(f\"Target variable y has {y.nunique()} unique classes. Need at least 2 for classification.\", also_print=True)\n",
        "        raise ValueError(\"Insufficient number of target classes for classification.\")\n",
        "\n",
        "    print(\"\\n--- Dataset Information for Model Training ---\")\n",
        "    dataset_info_data = [\n",
        "        [\"Total Samples\", f\"{len(X):,}\"],\n",
        "        [\"Number of Features\", f\"{X.shape[1]:,}\"],\n",
        "        [\"Number of Classes\", f\"{y.nunique():,}\"]\n",
        "    ]\n",
        "    print(tabulate(dataset_info_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "    logger.log_info(f\"\\nDataset Information (detailed):\\n{tabulate(dataset_info_data, headers=['Metric', 'Value'], tablefmt='plain')}\")\n",
        "\n",
        "    best_params = CONFIG[\"DEFAULT_MODEL_PARAMS\"] # Start with defaults\n",
        "    best_score_overall = -np.inf # To track best score during tuning\n",
        "\n",
        "    # Define common CV strategy and scorer\n",
        "    cv_strategy = StratifiedKFold(n_splits=CONFIG[\"N_FOLDS_CV\"],\n",
        "                                  shuffle=True,\n",
        "                                  random_state=CONFIG[\"RANDOM_STATE\"])\n",
        "    scorer = make_scorer(f1_score, average='macro', zero_division=0)\n",
        "\n",
        "    # ===============================================================\n",
        "    # Step 1: Hyperparameter Tuning (Manual Randomized Search with Checkpointing)\n",
        "    # ===============================================================\n",
        "    if CONFIG[\"RUN_HYPERPARAMETER_TUNING\"]:\n",
        "        logger.log_info(\"\\n--- Step 1: Running Hyperparameter Tuning (Manual Randomized Search) ---\", also_print=True)\n",
        "        print(\"‚ö°Ô∏è WARNING: This process can be very time-consuming. Please be patient. ‚ö°Ô∏è\")\n",
        "        print(f\"Sampling {CONFIG['N_ITER_RANDOM_SEARCH']} candidates, each evaluated with {CONFIG['N_FOLDS_CV']} folds.\")\n",
        "\n",
        "        tuning_progress_log = [] # To store details of each candidate evaluated\n",
        "\n",
        "        # Seed random for parameter sampling to ensure reproducibility of search\n",
        "        random.seed(CONFIG[\"RANDOM_STATE\"])\n",
        "\n",
        "        tuning_start_time = time.time()\n",
        "        for i in range(CONFIG[\"N_ITER_RANDOM_SEARCH\"]):\n",
        "            candidate_start_time = time.time()\n",
        "\n",
        "            # Randomly sample parameters from the defined distributions\n",
        "            # Use random.choice for lists, randint/uniform for ranges if desired\n",
        "            candidate_params = {key: random.choice(value) if isinstance(value, list) else value.rvs(1)[0]\n",
        "                                for key, value in CONFIG[\"TUNING_PARAM_DIST\"].items()}\n",
        "            # Ensure random_state and n_jobs are passed to each model for consistency\n",
        "            candidate_params['random_state'] = CONFIG[\"RANDOM_STATE\"]\n",
        "            candidate_params['n_jobs'] = -1\n",
        "            candidate_params['class_weight'] = 'balanced'\n",
        "            candidate_params['verbose'] = 0 # Suppress verbose during individual candidate fit\n",
        "\n",
        "            logger.log_info(f\"\\n--- Candidate {i+1}/{CONFIG['N_ITER_RANDOM_SEARCH']} ---\", also_print=True)\n",
        "            logger.log_info(f\"Parameters: {json.dumps(candidate_params, indent=2)}\", also_print=True)\n",
        "\n",
        "            try:\n",
        "                # Train and evaluate this candidate using cross_validate\n",
        "                candidate_cv_results = cross_validate(\n",
        "                    estimator=RandomForestClassifier(**candidate_params),\n",
        "                    X=X,\n",
        "                    y=y,\n",
        "                    cv=cv_strategy,\n",
        "                    scoring=scorer,\n",
        "                    n_jobs=-1,\n",
        "                    return_train_score=False,\n",
        "                    verbose=0 # Suppress verbose for individual CV folds\n",
        "                )\n",
        "                candidate_score = candidate_cv_results['test_score'].mean() # Get mean test score (f1_macro)\n",
        "                candidate_std = candidate_cv_results['test_score'].std()\n",
        "\n",
        "                candidate_end_time = time.time()\n",
        "                candidate_duration = candidate_end_time - candidate_start_time\n",
        "\n",
        "                logger.log_info(f\"  CV Score (F1-Macro): {candidate_score:.4f} (Std: {candidate_std:.4f})\", also_print=True)\n",
        "                logger.log_info(f\"  Candidate evaluation completed in {candidate_duration:.2f} seconds.\", also_print=True)\n",
        "\n",
        "                tuning_progress_log.append({ # Log full details of this candidate's run\n",
        "                    \"candidate_num\": i + 1,\n",
        "                    \"params\": candidate_params,\n",
        "                    \"cv_score_mean\": candidate_score,\n",
        "                    \"cv_score_std\": candidate_std,\n",
        "                    \"cv_scores_all_folds\": candidate_cv_results['test_score'].tolist(),\n",
        "                    \"duration_seconds\": candidate_duration\n",
        "                })\n",
        "\n",
        "                # Update overall best\n",
        "                if candidate_score > best_score_overall:\n",
        "                    best_score_overall = candidate_score\n",
        "                    best_params = candidate_params\n",
        "                    logger.log_info(f\"  NEW BEST FOUND! Score: {best_score_overall:.4f}. Saving best parameters...\", also_print=True)\n",
        "                    # Checkpoint: Save best parameters immediately\n",
        "                    with open(PARAMS_SAVE_PATH, 'w') as f:\n",
        "                        json.dump(best_params, f, indent=2)\n",
        "                    logger.log_info(f\"  Best parameters checkpointed to: {PARAMS_SAVE_PATH}\", also_print=False)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.log_error(f\"Error evaluating candidate {i+1}: {e}. Skipping.\", also_print=True)\n",
        "                logger.log_info(f\"Traceback for candidate {i+1}:\\n{traceback.format_exc()}\", also_print=False)\n",
        "                # Log this candidate's failure but continue the search\n",
        "                tuning_progress_log.append({\n",
        "                    \"candidate_num\": i + 1,\n",
        "                    \"params\": candidate_params,\n",
        "                    \"status\": \"FAILED\",\n",
        "                    \"error\": str(e),\n",
        "                    \"traceback\": traceback.format_exc()\n",
        "                })\n",
        "\n",
        "            # Save tuning progress log periodically or after each candidate\n",
        "            with open(TUNING_LOG_PATH, 'w') as f:\n",
        "                json.dump(tuning_progress_log, f, indent=2)\n",
        "\n",
        "        tuning_end_time = time.time()\n",
        "\n",
        "        print(\"\\n--- Hyperparameter Tuning Results ---\")\n",
        "        tuning_results_data = [\n",
        "            [\"Best F1-Macro Score (Overall CV)\", f\"{best_score_overall:.4f}\"],\n",
        "            [\"Tuning Duration\", f\"{tuning_end_time - tuning_start_time:.2f} seconds ({ (tuning_end_time - tuning_start_time) / 3600:.2f} hours)\"],\n",
        "            [\"Best Parameters Found\", json.dumps(best_params, indent=2)]\n",
        "        ]\n",
        "        print(tabulate(tuning_results_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nHyperparameter Tuning Results (detailed):\\n{tabulate(tuning_results_data, headers=['Metric', 'Value'], tablefmt='plain')}\")\n",
        "        logger.log_info(f\"\\nBest Parameters Found:\\n{json.dumps(best_params, indent=2)}\", also_print=False)\n",
        "\n",
        "    else: # RUN_HYPERPARAMETER_TUNING is False\n",
        "        logger.log_info(\"\\n--- Step 1: Loading Pre-tuned Parameters ---\", also_print=True)\n",
        "        # Attempt to load saved best parameters\n",
        "        loaded_model, loaded_params = load_model_and_params(MODEL_SAVE_PATH, PARAMS_SAVE_PATH)\n",
        "        if loaded_params:\n",
        "            best_params = loaded_params\n",
        "            logger.log_info(f\"Using loaded parameters: {json.dumps(best_params, indent=2)}\", also_print=False)\n",
        "        else:\n",
        "            best_params = CONFIG[\"DEFAULT_MODEL_PARAMS\"]\n",
        "            logger.log_warning(f\"No pre-tuned parameters found. Using DEFAULT parameters: {json.dumps(best_params, indent=2)}\", also_print=True)\n",
        "\n",
        "    # ===============================================================\n",
        "    # Step 2: Cross-Validation for Robust Performance Evaluation (using best_params)\n",
        "    # ===============================================================\n",
        "    logger.log_info(f\"\\n--- Step 2: Running {CONFIG['N_FOLDS_CV']}-Fold Cross-Validation ---\", also_print=True)\n",
        "    print(f\"Using parameters for CV: {json.dumps(best_params, indent=2)}\")\n",
        "\n",
        "    cv_model = RandomForestClassifier(random_state=CONFIG[\"RANDOM_STATE\"],\n",
        "                                      n_jobs=-1,\n",
        "                                      verbose=0, # Suppress verbose during CV runs for cleaner output\n",
        "                                      **best_params)\n",
        "\n",
        "    scoring = {\n",
        "        'accuracy': make_scorer(accuracy_score),\n",
        "        'f1_macro': make_scorer(f1_score, average='macro', zero_division=0),\n",
        "        'f1_weighted': make_scorer(f1_score, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "    cv_start_time = time.time()\n",
        "    cv_results = cross_validate(\n",
        "        estimator=cv_model,\n",
        "        X=X,\n",
        "        y=y,\n",
        "        cv=StratifiedKFold(n_splits=CONFIG[\"N_FOLDS_CV\"], shuffle=True, random_state=CONFIG[\"RANDOM_STATE\"]),\n",
        "        scoring=scoring,\n",
        "        n_jobs=-1,\n",
        "        return_train_score=False, # We care about validation scores\n",
        "        verbose=1 # Show overall CV progress\n",
        "    )\n",
        "    cv_end_time = time.time()\n",
        "\n",
        "    cv_accuracy_mean = cv_results['test_accuracy'].mean()\n",
        "    cv_accuracy_std = cv_results['test_accuracy'].std()\n",
        "    cv_f1_macro_mean = cv_results['test_f1_macro'].mean()\n",
        "    cv_f1_macro_std = cv_results['test_f1_macro'].std()\n",
        "    cv_f1_weighted_mean = cv_results['test_f1_weighted'].mean()\n",
        "    cv_f1_weighted_std = cv_results['test_f1_weighted'].std()\n",
        "\n",
        "    print(f\"\\n--- {CONFIG['N_FOLDS_CV']}-Fold Cross-Validation Results ---\")\n",
        "    cv_summary_data = [\n",
        "        [\"Metric\", \"Mean Score\", \"Std Dev\"],\n",
        "        [\"Accuracy\", f\"{cv_accuracy_mean:.4f}\", f\"{cv_accuracy_std:.4f}\"],\n",
        "        [\"F1-Macro\", f\"{cv_f1_macro_mean:.4f}\", f\"{cv_f1_macro_std:.4f}\"],\n",
        "        [\"F1-Weighted\", f\"{cv_f1_weighted_mean:.4f}\", f\"{cv_f1_weighted_std:.4f}\"]\n",
        "    ]\n",
        "    print(tabulate(cv_summary_data, headers=\"firstrow\", tablefmt=\"grid\"))\n",
        "    logger.log_info(f\"\\nCross-Validation Results (detailed):\\n{tabulate(cv_summary_data, headers='firstrow', tablefmt='plain')}\")\n",
        "    logger.log_info(f\"Cross-validation completed in {cv_end_time - cv_start_time:.2f} seconds.\", also_print=True)\n",
        "\n",
        "\n",
        "    # ===============================================================\n",
        "    # Step 3: Train Final Model on Full Dataset\n",
        "    # ===============================================================\n",
        "    logger.log_info(\"\\n--- Step 3: Training Final Model on Full Dataset ---\", also_print=True)\n",
        "\n",
        "    final_model = RandomForestClassifier(random_state=CONFIG[\"RANDOM_STATE\"],\n",
        "                                         n_jobs=-1,\n",
        "                                         verbose=0,\n",
        "                                         **best_params)\n",
        "\n",
        "    final_train_start_time = time.time()\n",
        "    final_model.fit(X, y) # Train on the entire dataset\n",
        "    final_train_end_time = time.time()\n",
        "\n",
        "    # FIX: Corrected time calculation: end_time - start_time\n",
        "    logger.log_info(f\"Final model training on full dataset complete in {final_train_end_time - final_train_start_time:.2f} seconds.\", also_print=True)\n",
        "\n",
        "    # ===============================================================\n",
        "    # Step 4: Model Persistence (Conditional)\n",
        "    # ===============================================================\n",
        "    if CONFIG[\"SAVE_MODEL_AND_PARAMS\"]:\n",
        "        logger.log_info(\"\\n--- Step 4: Saving Model and Parameters ---\", also_print=True)\n",
        "        save_model_and_params(final_model, best_params, MODEL_SAVE_PATH, PARAMS_SAVE_PATH)\n",
        "    else:\n",
        "        logger.log_info(\"\\nSkipping model and parameter saving (SAVE_MODEL_AND_PARAMS is False).\", also_print=True)\n",
        "\n",
        "    logger.log_info(\"Condo Name Classification Full Pipeline completed.\", also_print=True)\n",
        "    return final_model, best_params, cv_results\n",
        "\n",
        "# =================================================================================\n",
        "# Main Execution Block (for CONDO NAME CLASSIFICATION MODEL TRAINING)\n",
        "# =================================================================================\n",
        "\n",
        "try:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Condo Name Classification Model Training (Full Pipeline)\")\n",
        "\n",
        "    if 'X' not in locals() or 'y' not in locals() or X.empty or y.empty:\n",
        "        raise NameError(\"Feature matrix (X) or target (y) not found or empty. Please run the Feature Engineering block first.\")\n",
        "\n",
        "    condo_model, optimal_params, cv_evaluation = train_condo_classifier_full_pipeline(X, y)\n",
        "\n",
        "    print(\"\\n‚úÖ Condo Name Classification Model Training (Full Pipeline) completed successfully! üéâ\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An unexpected error occurred during model training pipeline:\", also_print=True)\n",
        "    logger.log_error(f\"Type: {type(e).__name__}\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "Z8qTBW_BQklt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ChydioVrmJj"
      },
      "source": [
        "# Date Analysis and Standardization\n",
        "## Pipeline Status Check\n",
        "\n",
        "### Data Input Issue\n",
        "‚ùó **Important Pipeline Issue**: The code is using `cleaned_data` instead of our main pipeline variable `combined_data`. Let me show you how we should modify this code to maintain our data pipeline integrity.\n",
        "\n",
        "Here's the corrected version that maintains our pipeline consistency:\n",
        "\n",
        "```python\n",
        "# First analyze the dates in our main dataset\n",
        "date_analysis = analyze_date_columns(combined_data)\n",
        "\n",
        "# Convert dates and save back to our main pipeline variable\n",
        "combined_data = convert_dates_for_power_bi(combined_data)\n",
        "```\n",
        "\n",
        "## Understanding the Date Processing\n",
        "\n",
        "This code performs two crucial operations that work together like a sophisticated date management system. Let me explain why each part matters and how they work together.\n",
        "\n",
        "### The Analysis Phase\n",
        "The `analyze_date_columns` function works like a thorough date inspector. Imagine you're examining a collection of historical documents - you want to make sure all the dates make logical sense and follow a proper timeline. This function does exactly that for our real estate data by:\n",
        "\n",
        "1. **Checking Date Types and Formats**:\n",
        "   Just as we might find dates written in different styles (like \"January 1, 2024\" or \"01/01/2024\"), this function identifies how our dates are currently stored in the database.\n",
        "\n",
        "2. **Validating Date Sequences**:\n",
        "   In real estate transactions, events must happen in a logical order - you can't purchase a house before it's listed, and you can't close before purchasing. The function verifies these logical sequences.\n",
        "\n",
        "3. **Identifying Suspicious Dates**:\n",
        "   Just as we might question a document dated \"2025\" or \"1890\" in a modern real estate transaction, this function flags dates that seem unreasonable:\n",
        "   - Future dates that haven't happened yet\n",
        "   - Very old dates that predate modern real estate practices (before 1980)\n",
        "\n",
        "### The Conversion Phase\n",
        "The `convert_dates_for_power_bi` function acts like a date standardization expert. After we understand what issues exist in our dates, this function:\n",
        "\n",
        "1. **Creates Consistent Formats**:\n",
        "   Converts all dates to a standard datetime format that Power BI can understand and work with effectively.\n",
        "\n",
        "2. **Handles Errors Gracefully**:\n",
        "   If it encounters dates it can't convert, it marks them as null rather than failing or creating invalid data.\n",
        "\n",
        "3. **Maintains Data Quality**:\n",
        "   Keeps track of how many dates couldn't be converted, helping us understand our data quality.\n",
        "\n",
        "### Why This Matters for Our Pipeline\n",
        "This date standardization step is crucial because:\n",
        "- Proper date formats enable time-based analysis in Power BI\n",
        "- Consistent date formats prevent visualization errors\n",
        "- Validated date sequences ensure accurate transaction timeline analysis\n",
        "- Clean date data enables reliable trend analysis\n",
        "\n",
        "## Next Steps\n",
        "After this standardization, our `combined_data` will have properly formatted dates ready for:\n",
        "1. Timeline visualizations in Power BI\n",
        "2. Sales trend analysis\n",
        "3. Seasonal pattern identification\n",
        "4. Transaction duration calculations\n",
        "\n",
        "Would you like me to explain any part of this process in more detail? For instance, I could elaborate on how the date validation works or why certain date formats are better for Power BI."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 20: POWER BI COMPATIBILITY ANALYSIS\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This is a read-only analysis cell that serves as a \"pre-flight check\"\n",
        "before exporting data. It analyzes every column in the final DataFrame\n",
        "and flags potential issues for both Power BI (Power Query) and\n",
        "Streamlit (Parquet file) performance.\n",
        "\n",
        "It checks for:\n",
        "- High Nulls: Columns with a high percentage of missing data.\n",
        "- High Cardinality: Columns with too many unique values (e.g., ID columns).\n",
        "- Long Text: 'object' columns with long average text length.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "import traceback\n",
        "\n",
        "def analyze_columns_for_power_bi(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyzes DataFrame columns with a focus on Power BI compatibility.\n",
        "    Calculates nulls, cardinality, text complexity, and flags concerns.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting Power BI column analysis...\", also_print=True)\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"--- Power BI Column Compatibility Analysis ---\")\n",
        "\n",
        "        column_info = []\n",
        "        total_rows = len(df)\n",
        "        if total_rows == 0:\n",
        "            logger.log_info(\"DataFrame is empty. No columns to analyze.\", also_print=True)\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Analyze each column\n",
        "        for column in sorted(df.columns):\n",
        "            dtype = df[column].dtype\n",
        "            null_count = df[column].isna().sum()\n",
        "            null_percentage = (null_count / total_rows) * 100\n",
        "            unique_count = df[column].nunique()\n",
        "            cardinality_score = min((unique_count / total_rows) * 100, 100)\n",
        "\n",
        "            # [EXCELLENT LOGIC] Custom text complexity score\n",
        "            text_score = 0.0\n",
        "            if dtype == 'object':\n",
        "                try:\n",
        "                    avg_length = df[column].dropna().astype(str).str.len().mean()\n",
        "                    if pd.isna(avg_length): avg_length = 0\n",
        "\n",
        "                    if avg_length <= 50:\n",
        "                        text_score = (avg_length / 50) * 25\n",
        "                    elif avg_length <= 200:\n",
        "                        text_score = 25 + ((avg_length - 50) / 150) * 25\n",
        "                    elif avg_length <= 1000:\n",
        "                        text_score = 50 + ((avg_length - 200) / 800) * 25\n",
        "                    else:\n",
        "                        text_score = 75 + min((avg_length - 1000) / 1000 * 25, 25)\n",
        "                except (TypeError, AttributeError):\n",
        "                    text_score = 0.0\n",
        "\n",
        "            column_info.append({\n",
        "                'Column_Name': column,\n",
        "                'Data_Type': str(dtype),\n",
        "                'Null_Count': null_count,\n",
        "                'Null_Percentage': round(null_percentage, 2),\n",
        "                'Cardinality_Score': round(cardinality_score, 2),\n",
        "                'Text_Score': round(text_score, 2)\n",
        "            })\n",
        "\n",
        "        analysis_df = pd.DataFrame(column_info)\n",
        "\n",
        "        # Add a 'Concerns' flag\n",
        "        analysis_df['Concerns'] = analysis_df.apply(\n",
        "            lambda row: ' | '.join(filter(None, [\n",
        "                'High Cardinality' if row['Cardinality_Score'] > 80 else '',\n",
        "                'Long Text' if row['Text_Score'] > 50 else '',\n",
        "                'Many Nulls' if row['Null_Percentage'] > 20 else ''\n",
        "            ])) or 'None',\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Create a formatted display version for console\n",
        "        display_data_for_table = []\n",
        "        for index, row in analysis_df.iterrows():\n",
        "            display_data_for_table.append([\n",
        "                row['Column_Name'],\n",
        "                row['Data_Type'],\n",
        "                f\"{row['Null_Count']:,}\",\n",
        "                f\"{row['Null_Percentage']:.1f}%\",\n",
        "                f\"{row['Cardinality_Score']:.1f}\",\n",
        "                f\"{row['Text_Score']:.1f}\" if row['Text_Score'] > 0 else \"-\",\n",
        "                row['Concerns']\n",
        "            ])\n",
        "\n",
        "        display_data_for_table.sort(key=lambda x: x[0].lower())\n",
        "\n",
        "        logger.log_info(f\"\\nTotal columns analyzed: {len(analysis_df)}\", also_print=True)\n",
        "        concerns_count = (analysis_df['Concerns'] != 'None').sum()\n",
        "\n",
        "        if concerns_count > 0:\n",
        "            logger.log_info(\n",
        "                f\"Columns needing attention for Power BI/Parquet: {concerns_count} \"\n",
        "                f\"({(concerns_count/len(analysis_df))*100:.1f}%)\", also_print=True)\n",
        "            logger.log_info(\"\\nColumns with Concerns (detailed to log):\")\n",
        "            logger.log_info(analysis_df[analysis_df['Concerns'] != 'None'][['Column_Name', 'Concerns']].to_string(index=False))\n",
        "        else:\n",
        "            logger.log_info(\"No significant compatibility concerns identified.\", also_print=True)\n",
        "\n",
        "        print(\"\\n--- Power BI Column Analysis Results ---\")\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(display_data_for_table,\n",
        "                                  headers=[\"Column Name\", \"Data Type\", \"Null Count\", \"Null %\", \"Cardinality\", \"Text Score\", \"Concerns\"],\n",
        "                                  tablefmt=\"grid\"))\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        logger.log_info(f\"\\nFull Power BI Column Analysis Results (detailed):\\n{tabulate.tabulate(display_data_for_table, headers=['Column Name', 'Data Type', 'Null Count', 'Null %', 'Cardinality', 'Text Score', 'Concerns'], tablefmt='plain')}\")\n",
        "\n",
        "        logger.log_info(\"Power BI column analysis completed.\", also_print=True)\n",
        "        return analysis_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"PROCESS HALTED: Error in analyze_columns_for_power_bi: {str(e)}\", also_print=True)\n",
        "        logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 20: Power BI Compatibility Analysis ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Power BI Column Compatibility Analysis\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty.\")\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    # Run the analysis on the final `combined_data` DataFrame\n",
        "    power_bi_column_analysis_df = analyze_columns_for_power_bi(combined_data)\n",
        "\n",
        "    print(\"\\n‚úÖ Power BI column compatibility analysis completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during Power BI compatibility analysis:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    # [FIX] Added standard finally block\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape} (unchanged)\")\n",
        "\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 20 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "qzThXQWLSMj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 21: POWER BI EXPORT PREPARATION (CSV)\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This code block prepares the final cleaned and engineered data for export\n",
        "to a CSV file specifically for Power BI.\n",
        "\n",
        "It ensures data types are compatible with Power BI's CSV importer,\n",
        "which often prefers concrete types (like int64 with 0 for nulls) over\n",
        "nullable types. It removes temporary columns and saves the file to a\n",
        "consistent path for Power BI's automatic refresh.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "from pathlib import Path\n",
        "import traceback\n",
        "\n",
        "def prepare_and_export_for_power_bi(df: pd.DataFrame, output_folder: Path, columns_to_drop: list = None) -> tuple[pd.DataFrame, dict]:\n",
        "    \"\"\"\n",
        "    Prepares data for Power BI by:\n",
        "    1. Converting data types to simple, Power BI-friendly formats (e.g., int64 with 0 fill).\n",
        "    2. Removing specified temporary/unnecessary columns.\n",
        "    3. Exporting to a CSV file.\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Starting Power BI export preparation...\", also_print=True)\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"--- Power BI Export Preparation ---\")\n",
        "\n",
        "    df_export = df.copy()\n",
        "    issues_found = []\n",
        "\n",
        "    # [EXCELLENT LOGIC] This type map is specific to Power BI's CSV import.\n",
        "    # It correctly fills NaNs with 0 for int64, which is a common\n",
        "    # practice for this specific export target.\n",
        "    column_types = {\n",
        "        # Monetary\n",
        "        'Close_Price': 'float64', 'Original_List_Price': 'float64',\n",
        "        'Previous_List_Price': 'float64', 'Tax_Assessed_Value': 'float64',\n",
        "        'Condo_Fee': 'float64',\n",
        "\n",
        "        # Numeric (Note: fillna(0) for int64)\n",
        "        'RMS_Total': 'int64', 'Below_Grade_Finished_Area': 'float64',\n",
        "        'Lot_Size_SF': 'float64', 'Num_Garage_Spaces': 'int64',\n",
        "        'Days_On_Market': 'int64', 'Cumulative_Days_On_Market': 'int64',\n",
        "        'Bedrooms_Above_Grade': 'int64', 'Bedrooms_Below_Grade': 'int64',\n",
        "        'Year_Built': 'int64', # This will use the Int64 from the previous step\n",
        "\n",
        "        # Dates\n",
        "        'Close_Date': 'datetime64[ns]', 'Listing_Contract_Date': 'datetime64[ns]',\n",
        "        'Purchase_Contract_Date': 'datetime64[ns]',\n",
        "\n",
        "        # Categorical/String (object)\n",
        "        'Condo_Name': 'object', 'Condo_Complex': 'object', 'Postal_Code': 'object',\n",
        "        'Subdivision_Name': 'object', 'standardized_commission': 'object',\n",
        "        'standardized_parking_complete': 'object', 'Style': 'object',\n",
        "        'Property_Sub_Type': 'object', 'City': 'object', 'Basement_Finish': 'object',\n",
        "        'Is_Walkout': 'object', 'Suite_Separate_Entry': 'object',\n",
        "        'Garage_YN': 'object', 'Occupant_Type': 'object', 'Previous_Status': 'object',\n",
        "        'Structure_Type': 'object', 'Suite': 'object', 'Zoning': 'object',\n",
        "        'MLS_Num': 'object', 'LINC_Num': 'object', 'source_file': 'object'\n",
        "    }\n",
        "\n",
        "    logger.log_info(\"\\nVerifying and converting data types for Power BI CSV...\", also_print=True)\n",
        "\n",
        "    for column, expected_type in column_types.items():\n",
        "        if column in df_export.columns:\n",
        "            current_type = str(df_export[column].dtype)\n",
        "\n",
        "            # Special handling for Int64 -> int64\n",
        "            if current_type == 'Int64' and expected_type == 'int64':\n",
        "                current_type = 'int64' # Treat it as compatible if we fillna\n",
        "\n",
        "            if current_type != expected_type:\n",
        "                try:\n",
        "                    if expected_type.startswith('datetime'):\n",
        "                        df_export[column] = pd.to_datetime(df_export[column], errors='coerce')\n",
        "\n",
        "                    elif expected_type.startswith(('int', 'float')):\n",
        "                        if df_export[column].dtype == 'object':\n",
        "                            df_export[column] = (\n",
        "                                df_export[column].astype(str)\n",
        "                                .str.replace(r'[$,]', '', regex=True)\n",
        "                                .str.replace('‚àí', '-')\n",
        "                                .str.strip()\n",
        "                            )\n",
        "\n",
        "                        df_export[column] = pd.to_numeric(df_export[column], errors='coerce')\n",
        "\n",
        "                        if expected_type == 'int64':\n",
        "                            # This is the key PBI logic: fillna(0)\n",
        "                            df_export[column] = df_export[column].fillna(0).astype('int64')\n",
        "\n",
        "                    elif expected_type == 'object':\n",
        "                        df_export[column] = df_export[column].astype(str)\n",
        "\n",
        "                    logger.log_info(f\"Converted '{column}' from {current_type} to {expected_type}.\", also_print=False)\n",
        "\n",
        "                except Exception as e:\n",
        "                    issues_found.append(f\"Error converting '{column}' to {expected_type}: {str(e)}\")\n",
        "                    logger.log_error(f\"Failed to convert '{column}': {str(e)}\", also_print=True)\n",
        "        else:\n",
        "            logger.log_info(f\"WARNING: Expected column '{column}' not found for type check.\", also_print=True)\n",
        "\n",
        "    # --- Step 4: Remove specified columns ---\n",
        "    if columns_to_drop is None:\n",
        "        columns_to_drop = [\n",
        "            'standardized_parking', 'Year', 'Price_Per_SqFt', 'Original_Style',\n",
        "            'Commission', 'lat_bin', 'lon_bin', 'location_id', 'original_index',\n",
        "        ]\n",
        "\n",
        "    columns_dropped = []\n",
        "    columns_not_found = []\n",
        "\n",
        "    logger.log_info(\"\\nRemoving specified columns...\", also_print=True)\n",
        "    for column in columns_to_drop:\n",
        "        if column in df_export.columns:\n",
        "            df_export = df_export.drop(columns=column)\n",
        "            columns_dropped.append(column)\n",
        "            logger.log_info(f\"Dropped column: '{column}'.\", also_print=False)\n",
        "        else:\n",
        "            columns_not_found.append(column)\n",
        "            logger.log_info(f\"NOTE: Column to drop '{column}' not found.\", also_print=False)\n",
        "\n",
        "    # --- Step 5: Export to CSV ---\n",
        "    output_path = output_folder / 'power_bi_ready_data.csv'\n",
        "    logger.log_info(f\"\\nExporting data to CSV: {output_path}...\", also_print=True)\n",
        "\n",
        "    # [YOUR LOGIC] This is correct for Power BI\n",
        "    df_export.to_csv(output_path, index=False, encoding='utf-8')\n",
        "\n",
        "    if output_path.exists():\n",
        "        file_size_mb = output_path.stat().st_size / (1024 * 1024)\n",
        "        exported_rows = len(df_export)\n",
        "\n",
        "        summary = {\n",
        "            'original_columns_count': len(df.columns),\n",
        "            'final_columns_count': len(df_export.columns),\n",
        "            'columns_dropped': columns_dropped,\n",
        "            'columns_not_found_to_drop': columns_not_found,\n",
        "            'file_size_mb': file_size_mb,\n",
        "            'export_path': str(output_path),\n",
        "            'issues_found_during_conversion': issues_found,\n",
        "            'exported_row_count': exported_rows\n",
        "        }\n",
        "\n",
        "        print(\"\\n--- Export Results Summary ---\")\n",
        "        summary_table_data = [\n",
        "            [\"Export Path\", str(output_path)],\n",
        "            [\"File Size\", f\"{file_size_mb:.2f} MB\"],\n",
        "            [\"Exported Rows\", f\"{exported_rows:,}\"],\n",
        "            [\"Original Columns\", f\"{summary['original_columns_count']:,}\"],\n",
        "            [\"Final Columns\", f\"{summary['final_columns_count']:,}\"],\n",
        "            [\"Columns Dropped\", f\"{len(summary['columns_dropped']):,}\"]\n",
        "        ]\n",
        "        if summary['columns_dropped']:\n",
        "            summary_table_data.append([\"Dropped List\", \", \".join(summary['columns_dropped'])])\n",
        "        if summary['columns_not_found_to_drop']:\n",
        "            summary_table_data.append([\"Not Found (to drop)\", \", \".join(summary['columns_not_found_to_drop'])])\n",
        "\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(summary_table_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"\\nExport Results Summary (detailed):\\n{tabulate.tabulate(summary_table_data, headers=['Metric', 'Value'], tablefmt='plain')}\")\n",
        "\n",
        "        if issues_found:\n",
        "            print(\"\\n‚ö†Ô∏è WARNING: Issues found during type conversions. See log for details. ‚ö†Ô∏è\")\n",
        "            logger.log_error(\"\\nIssues found during type conversions (detailed to log):\", also_print=True)\n",
        "            for issue in issues_found:\n",
        "                logger.log_error(f\"- {issue}\", also_print=True)\n",
        "        else:\n",
        "            logger.log_info(\"\\nNo issues found during type conversions.\", also_print=True)\n",
        "\n",
        "        return df_export, summary\n",
        "    else:\n",
        "        logger.log_error(f\"Export file was not created successfully at {output_path}.\", also_print=True)\n",
        "        raise FileNotFoundError(f\"Export file was not created successfully at {output_path}.\")\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 21: Power BI Export Preparation ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Final Data Preparation and Export for Power BI\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found, is not a DataFrame, or is empty.\")\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    if 'folder_path' not in locals() or not isinstance(folder_path, Path):\n",
        "        raise NameError(\"Variable 'folder_path' (from first cell) not found or is not a Path object.\")\n",
        "\n",
        "    # [GOOD LOGIC] This makes the export path consistent\n",
        "    power_bi_output_folder = folder_path.parent\n",
        "    logger.log_info(f\"Targeting Power BI export folder: {power_bi_output_folder}\", also_print=True)\n",
        "\n",
        "    export_df, export_summary = prepare_and_export_for_power_bi(\n",
        "        combined_data,\n",
        "        power_bi_output_folder\n",
        "    )\n",
        "\n",
        "    if not export_summary['issues_found_during_conversion']:\n",
        "        print(\"\\n‚úÖ Power BI export completed successfully! Your file is ready for refresh. üéâ\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è Power BI export completed WITH ISSUES. Please review the output and log. ‚ö†Ô∏è\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during Power BI export preparation:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise # Re-raise to halt execution\n",
        "finally:\n",
        "    # [FIX] Added standard finally block\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape} (unchanged)\")\n",
        "\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 21 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "QYXzvSlkShI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 22: PARQUET/STREAMLIT PRE-FLIGHT ANALYSIS\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This is a read-only analysis cell to check the final DataFrame and\n",
        "provide specific recommendations for optimizing data types before\n",
        "exporting to Parquet for Streamlit.\n",
        "\n",
        "This analysis identifies columns that can be converted to:\n",
        "- 'Int64' (Nullable Integer): For integer-like columns that contain NaNs.\n",
        "- 'category': For text columns with low cardinality (few unique values).\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate  # <-- [FIX] Standardized import\n",
        "import traceback\n",
        "\n",
        "def analyze_columns_for_parquet(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyzes DataFrame columns and provides specific recommendations\n",
        "    for Parquet/Streamlit optimization.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.log_info(\"Starting Parquet/Streamlit column analysis...\", also_print=True)\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"--- Parquet/Streamlit Pre-flight Analysis ---\")\n",
        "\n",
        "        column_info = []\n",
        "        total_rows = len(df)\n",
        "        if total_rows == 0:\n",
        "            logger.log_info(\"DataFrame is empty. No columns to analyze.\", also_print=True)\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Analyze each column\n",
        "        for column in sorted(df.columns):\n",
        "            dtype = str(df[column].dtype)\n",
        "            null_count = df[column].isna().sum()\n",
        "            null_pct = (null_count / total_rows) * 100\n",
        "            unique_count = df[column].nunique()\n",
        "            cardinality_pct = (unique_count / total_rows) * 100\n",
        "\n",
        "            recommendation = \"\"\n",
        "\n",
        "            # --- Recommendation Logic ---\n",
        "            if \"datetime\" in dtype:\n",
        "                recommendation = \"‚úÖ Keep as datetime\"\n",
        "            elif dtype == \"Int64\":\n",
        "                recommendation = \"‚úÖ Already Int64\"\n",
        "            elif dtype == \"category\":\n",
        "                recommendation = \"‚úÖ Already category\"\n",
        "            elif dtype in [\"float64\", \"int64\"]:\n",
        "                # Check if a float column is all integers (or NaN)\n",
        "                is_int_like = False\n",
        "                if dtype == \"float64\" and null_count != total_rows:\n",
        "                    try:\n",
        "                        is_int_like = (df[column].dropna() % 1 == 0).all()\n",
        "                    except TypeError:\n",
        "                        is_int_like = False # Fails on non-numeric\n",
        "\n",
        "                if is_int_like or dtype == \"int64\":\n",
        "                    if null_count > 0:\n",
        "                        recommendation = \"‚û°Ô∏è Convert to 'Int64' (nullable)\"\n",
        "                    else:\n",
        "                        recommendation = \"Keep as 'int64'\"\n",
        "                else:\n",
        "                    recommendation = \"Keep as 'float64'\"\n",
        "\n",
        "            elif dtype == \"object\":\n",
        "                if unique_count == total_rows:\n",
        "                    recommendation = \"‚ö†Ô∏è High Cardinality (ID Column?)\"\n",
        "                elif cardinality_pct < 1.0: # Heuristic: <1% unique values\n",
        "                    recommendation = \"‚û°Ô∏è Convert to 'category'\"\n",
        "                else:\n",
        "                    recommendation = \"Keep as 'object'\"\n",
        "\n",
        "            else:\n",
        "                recommendation = f\"Review ({dtype})\"\n",
        "\n",
        "            column_info.append({\n",
        "                'Column_Name': column,\n",
        "                'Data_Type': dtype,\n",
        "                'Nulls': f\"{null_count:,} ({null_pct:.1f}%)\",\n",
        "                'Cardinality': f\"{unique_count:,}\",\n",
        "                'Recommendation': recommendation\n",
        "            })\n",
        "\n",
        "        analysis_df = pd.DataFrame(column_info)\n",
        "\n",
        "        print(\"\\n--- Parquet/Streamlit Optimization Recommendations ---\")\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        print(tabulate.tabulate(analysis_df,\n",
        "                                  headers=[\"Column Name\", \"Data Type\", \"Nulls\", \"Cardinality\", \"Recommendation\"],\n",
        "                                  tablefmt=\"grid\"))\n",
        "        # [FIX] Using tabulate.tabulate()\n",
        "        logger.log_info(f\"\\nFull Parquet Analysis (detailed):\\n{tabulate.tabulate(analysis_df, headers='keys', tablefmt='plain')}\")\n",
        "\n",
        "        logger.log_info(\"Parquet/Streamlit analysis complete.\", also_print=True)\n",
        "        return analysis_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log_error(f\"PROCESS HALTED: Error in analyze_columns_for_parquet: {str(e)}\", also_print=True)\n",
        "        logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "        raise\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 22: Parquet/Streamlit Analysis ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Running Parquet/Streamlit Pre-flight Analysis\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found.\")\n",
        "    print(f\"DataFrame shape at start: {combined_data.shape}\")\n",
        "\n",
        "    # Run the analysis\n",
        "    parquet_analysis_df = analyze_columns_for_parquet(combined_data)\n",
        "\n",
        "    print(\"\\n‚úÖ Parquet/Streamlit analysis completed successfully! üéâ\")\n",
        "    print(\"Review the 'Recommendation' column. The next cell will perform these conversions.\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during Parquet analysis:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape} (unchanged)\")\n",
        "\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 22 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "Ou7LOHbLS6l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELL 23: STREAMLIT/PARQUET PREPARATION & EXPORT (Corrected)\n",
        "# =================================================================================\n",
        "\"\"\"\n",
        "This is the final \"action\" cell. Based on the analysis from Cell 22,\n",
        "this script performs the recommended data type conversions to create\n",
        "a highly optimized DataFrame for Parquet and Streamlit.\n",
        "\"\"\"\n",
        "\n",
        "# --- Re-importing for cell modularity ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate\n",
        "from pathlib import Path\n",
        "import traceback\n",
        "\n",
        "def prepare_for_streamlit(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies final, optimal data types to the DataFrame for\n",
        "    Parquet export and Streamlit performance.\n",
        "    \"\"\"\n",
        "    logger.log_info(\"Applying final data types for Streamlit/Parquet...\", also_print=True)\n",
        "    df_prep = df.copy()\n",
        "\n",
        "    # --- 1. Define Target Data Types (Based on Cell 22 Analysis) ---\n",
        "    int_cols = [\n",
        "        'Year_Built', 'Bedrooms_Above_Grade', 'Bedrooms_Below_Grade',\n",
        "        'Total_Baths', 'Full_Baths', 'Days_On_Market', 'Cumulative_Days_On_Market'\n",
        "    ]\n",
        "    if 'Num_Garage_Spaces' in df_prep.columns:\n",
        "        df_prep['Num_Garage_Spaces'] = pd.to_numeric(df_prep['Num_Garage_Spaces'], errors='coerce')\n",
        "        int_cols.append('Num_Garage_Spaces')\n",
        "\n",
        "    cat_cols = [\n",
        "        'City', 'Previous_Status', 'Structure_Type', 'Style',\n",
        "        'Property_Sub_Type', 'Suite', 'Basement_Finish', 'Is_Walkout',\n",
        "        'Suite_Separate_Entry', 'Garage_YN', 'standardized_parking_complete',\n",
        "        'standardized_commission', 'Occupant_Type', 'Zoning',\n",
        "        'Condo_Type', 'Condo_Name'\n",
        "        # [FIX] Removed 'Garage_Sp' from this list\n",
        "    ]\n",
        "\n",
        "    float_cols = [\n",
        "        'Close_Price', 'Original_List_Price', 'Previous_List_Price',\n",
        "        'Tax_Assessed_Value', 'Condo_Fee', 'RMS_Total',\n",
        "        'Below_Grade_Finished_Area', 'Lot_Size_SF', 'Latitude', 'Longitude'\n",
        "    ]\n",
        "\n",
        "    # --- 2. Apply Type Conversions ---\n",
        "    logger.log_info(\"Converting columns to 'Int64' (nullable integer)...\", also_print=False)\n",
        "    for col in int_cols:\n",
        "        if col in df_prep.columns:\n",
        "            try:\n",
        "                df_prep[col] = pd.to_numeric(df_prep[col], errors='coerce').astype('Int64')\n",
        "            except Exception as e:\n",
        "                logger.log_error(f\"Failed to convert '{col}' to Int64: {e}\", also_print=True)\n",
        "\n",
        "    logger.log_info(\"Converting columns to 'category' for efficiency...\", also_print=False)\n",
        "    for col in cat_cols:\n",
        "        if col in df_prep.columns:\n",
        "            try:\n",
        "                df_prep[col] = df_prep[col].astype('category')\n",
        "            except Exception as e:\n",
        "                logger.log_error(f\"Failed to convert '{col}' to category: {e}\", also_print=True)\n",
        "\n",
        "    logger.log_info(\"Converting columns to 'float64' for precision...\", also_print=False)\n",
        "    for col in float_cols:\n",
        "        if col in df_prep.columns:\n",
        "            try:\n",
        "                df_prep[col] = pd.to_numeric(df_prep[col], errors='coerce').astype('float64')\n",
        "            except Exception as e:\n",
        "                logger.log_error(f\"Failed to convert '{col}' to float64: {e}\", also_print=True)\n",
        "\n",
        "    # [FIX] Add specific, robust handling for the problematic 'Garage_Sp' column\n",
        "    logger.log_info(\"Converting 'Garage_Sp' to 'string' (nullable string)...\", also_print=False)\n",
        "    if 'Garage_Sp' in df_prep.columns:\n",
        "        try:\n",
        "            # Use modern pd.StringDtype() for a nullable string column\n",
        "            # This is the most robust way to handle this messy column for Parquet\n",
        "            df_prep['Garage_Sp'] = df_prep['Garage_Sp'].astype(pd.StringDtype())\n",
        "        except Exception as e:\n",
        "            logger.log_error(f\"Failed to convert 'Garage_Sp' to string: {e}\", also_print=True)\n",
        "\n",
        "    logger.log_info(\"Final type conversion complete.\", also_print=True)\n",
        "    return df_prep\n",
        "\n",
        "# =================================================================================\n",
        "# EXECUTION BLOCK\n",
        "# =================================================================================\n",
        "try:\n",
        "    logger.log_info(\"--- Starting CELL 23: Streamlit/Parquet Export ---\", also_print=True)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Preparing & Exporting `combined_data` to Parquet\")\n",
        "\n",
        "    if 'combined_data' not in locals() or not isinstance(combined_data, pd.DataFrame) or combined_data.empty:\n",
        "        raise NameError(\"Variable 'combined_data' not found.\")\n",
        "\n",
        "    if 'power_bi_output_folder' in locals() and isinstance(power_bi_output_folder, Path):\n",
        "        output_folder = power_bi_output_folder\n",
        "    elif 'folder_path' in locals() and isinstance(folder_path, Path):\n",
        "        output_folder = folder_path.parent\n",
        "    else:\n",
        "        raise NameError(\"Output folder path not found. Run previous cells.\")\n",
        "\n",
        "    parquet_output_path = output_folder / 'streamlit_ready_data.parquet'\n",
        "\n",
        "    # --- Memory Analysis ---\n",
        "    mem_before_mb = combined_data.memory_usage(deep=True).sum() / (1024 * 1024)\n",
        "    logger.log_info(f\"Memory usage *before* optimization: {mem_before_mb:.2f} MB\", also_print=True)\n",
        "\n",
        "    # --- Run Preparation ---\n",
        "    df_to_export = prepare_for_streamlit(combined_data)\n",
        "\n",
        "    mem_after_mb = df_to_export.memory_usage(deep=True).sum() / (1024 * 1024)\n",
        "    logger.log_info(f\"Memory usage *after* optimization: {mem_after_mb:.2f} MB\", also_print=True)\n",
        "\n",
        "    print(\"\\n--- Data Type Optimization Summary ---\")\n",
        "    mem_summary_data = [\n",
        "        [\"Memory Before\", f\"{mem_before_mb:.2f} MB\"],\n",
        "        [\"Memory After\", f\"{mem_after_mb:.2f} MB\"],\n",
        "        [\"Reduction\", f\"{(mem_before_mb - mem_after_mb):.2f} MB ({(1 - mem_after_mb/mem_before_mb):.1%})\"]\n",
        "    ]\n",
        "    print(tabulate.tabulate(mem_summary_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "\n",
        "    # --- Drop Final Unnecessary Columns ---\n",
        "    columns_to_drop = [\n",
        "        'standardized_parking', 'Year', 'Price_Per_SqFt', 'Original_Style',\n",
        "        'Commission', 'lat_bin', 'lon_bin', 'location_id', 'original_index',\n",
        "    ]\n",
        "\n",
        "    final_cols_to_drop = [col for col in columns_to_drop if col in df_to_export.columns]\n",
        "    if final_cols_to_drop:\n",
        "        df_to_export = df_to_export.drop(columns=final_cols_to_drop)\n",
        "        logger.log_info(f\"Dropped final temp columns: {', '.join(final_cols_to_drop)}\", also_print=True)\n",
        "\n",
        "    # --- Export to Parquet ---\n",
        "    logger.log_info(f\"\\nExporting data to Parquet: {parquet_output_path}...\", also_print=True)\n",
        "    df_to_export.to_parquet(parquet_output_path, index=False, compression='snappy')\n",
        "\n",
        "    if parquet_output_path.exists():\n",
        "        file_size_mb = parquet_output_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"\\n--- Parquet Export Summary ---\")\n",
        "        export_summary_data = [\n",
        "            [\"Export Path\", str(parquet_output_path)],\n",
        "            [\"File Size\", f\"{file_size_mb:.2f} MB\"],\n",
        "            [\"Rows Exported\", f\"{len(df_to_export):,}\"],\n",
        "            [\"Columns Exported\", f\"{len(df_to_export.columns):,}\"]\n",
        "        ]\n",
        "        print(tabulate.tabulate(export_summary_data, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
        "        logger.log_info(f\"Parquet export successful. File size: {file_size_mb:.2f} MB.\", also_print=True)\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Parquet file was not created successfully at {parquet_output_path}.\")\n",
        "\n",
        "    print(\"\\n‚úÖ Streamlit/Parquet export completed successfully! üéâ\")\n",
        "\n",
        "except (NameError, ValueError) as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: {e}\", also_print=True)\n",
        "except Exception as e:\n",
        "    logger.log_error(f\"\\nPROCESS HALTED: An error occurred during Parquet export:\", also_print=True)\n",
        "    logger.log_error(f\"Details: {str(e)}\", also_print=True)\n",
        "    logger.log_info(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "    raise\n",
        "finally:\n",
        "    if 'combined_data' in locals() and isinstance(combined_data, pd.DataFrame):\n",
        "        print(f\"DataFrame shape at end: {combined_data.shape} (unchanged)\")\n",
        "\n",
        "    check_memory_usage()\n",
        "    logger.display_log_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CELL 23 COMPLETE.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "HjHS1iW7UOAe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoVVE4n/Nv/yET1RSGkxiB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}